[
 {
  "topic": "Week 4 Lectures",
  "content": "Scanning in PostgreSQL\n",
  "key_words": [
   "week",
   "4",
   "lecture"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Scanning in other File Structures \n",
  "content": "Above examples are for heap files\n simple, unordered, maybe indexed, no hashing\n Other access file structures in PostgreSQL:\n btree, hash, gist, gin\n each implements:\n startscan, getnext, endscan\n insert, delete\n other file-specific operators\n Implementing Relational Operations\n",
  "key_words": [
   "scanning",
   "in",
   "other",
   "file",
   "structure"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Implementing Relational Operators \n",
  "content": "So far, have considered ...\n scanning   (e.g. select * from R)\n With file structures ...\n heap file ... tuples added to any page which has space\n sorted file ... tuples arranged in file in key order\n hash file ... tuples placed in pages using hash function\n Now ...\n sorting   (e.g. select * from R order by x)\n projection   (e.g. select x,y from R)\n selection   (e.g. select * from R where Cond)\n and\n indexes ... search trees based on pages/keys\n signatures ... bit-strings which \"summarize\" tuples\n",
  "key_words": [
   "implementing",
   "relational",
   "operator"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Implementing Relational Operators \n",
  "content": "File/query Parameters ...\n r tuples of size R,   b pages of size B,   c tuples per page\n Rel.k attribute in where clause,   bq answer pages for query q\n bOv overflow pages,   average overflow chain length Ov\n",
  "key_words": [
   "implementing",
   "relational",
   "operator"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Reminder on Cost Analyses \n",
  "content": "When showing the cost of operations, don't include Tr and Tw:\n for queries, simply count number of pages read\n for updates, use nr and nw to distinguish reads/writes\n When comparing two methods for same query\n ignore the cost of writing the result (same for both)\n In counting reads and writes, assume minimal buffering\n each request_page() causes a read\n each release_page() causes a write (if page is dirty)\n Sorting\n",
  "key_words": [
   "reminder",
   "on",
   "cost",
   "analysis"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "The Sort Operation \n",
  "content": "Sorting is explicit in queries only in the order by clause\n select * from Students order by name;\n Sorting is used internally in other operations:\n eliminating duplicate tuples for projection\n ordering files to enhance select efficiency\n implementing various styles of join\n forming tuple groups in group by\n Sort methods such as quicksort are designed for in-memory data.\n For large data on disks, use external sorts such as merge sort.\n",
  "key_words": [
   "the",
   "sort",
   "operation"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Two-way Merge Sort \n",
  "content": "Example:\n",
  "key_words": [
   "two-way",
   "merge",
   "sort"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Two-way Merge Sort \n",
  "content": "Requires three in-memory buffers: Assumption: cost of merge on two buffers \u2245 0.\n",
  "key_words": [
   "two-way",
   "merge",
   "sort"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Comparison for Sorting \n",
  "content": "Above assumes that we have a function to compare tuples.\n Needs to understand ordering on different data types.\n Need a function tupCompare(r1,r2,f) (cf. C's strcmp)\n int tupCompare(r1,r2,f)\n {\n    if (r1.f < r2.f) return -1;\n    if (r1.f > r2.f) return 1;\n    return 0;\n }\n Assume < and > are overloaded for all attribute types.\n",
  "key_words": [
   "comparison",
   "for",
   "sorting"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Comparison for Sorting \n",
  "content": "In reality, need to sort on multiple attributes and ASC/DESC, e.g.\n -- example multi-attribute sort\n select * from Students\n order by age desc, year_enrolled\n Sketch of multi-attribute sorting function\n int tupCompare(r1,r2,criteria)\n {\n    foreach (f,ord) in criteria {\n       if (ord == ASC) {\n          if (r1.f < r2.f) return -1;\n          if (r1.f > r2.f) return 1;\n       }\n       else {\n          if (r1.f > r2.f) return -1;\n          if (r1.f < r2.f) return 1;\n       }\n    }\n    return 0;\n }\n",
  "key_words": [
   "comparison",
   "for",
   "sorting"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Cost of Two-way Merge Sort \n",
  "content": "For a file containing b data pages:\n require ceil(log2b) passes to sort,\n each pass requires b page reads, b page writes\n Gives total cost:   2.b.ceil(log2b)\n Example: Relation with r=105 and c=50   \u21d2   b=2000 pages.\n Number of passes for sort:   ceil(log22000)  =  11\n Reads/writes entire file 11 times!    Can we do better?\n",
  "key_words": [
   "cost",
   "of",
   "two-way",
   "merge",
   "sort"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "n-Way Merge Sort \n",
  "content": "Merge passes use:   B memory buffers,   n input buffers,   B-n output buffers\n Typically, consider only one output buffer, i.e. B = n + 1\n",
  "key_words": [
   "n-way",
   "merge",
   "sort"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... n-Way Merge Sort \n",
  "content": "Method:\n // Produce B-page-long runs\n for each group of B pages in Rel {\n     read pages into memory buffers\n     sort group in memory\n     write pages out to Temp\n }\n // Merge runs until everything sorted\n // n-way merge, where n=B-1\n numberOfRuns = \u2308b/B\u2309\n while (numberOfRuns > 1) {\n     for each group of n runs in Temp {\n         merge into a single run via input buffers\n         write run to newTemp via output buffer\n     }\n     numberOfRuns = \u2308numberOfRuns/n\u2309\n     Temp = newTemp // swap input/output files\n }\n",
  "key_words": [
   "n-way",
   "merge",
   "sort"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... n-Way Merge Sort \n",
  "content": "Method for merging n runs (n input buffers, 1 output buffer):\n for i = 1..n {\n    read first page of run[i] into a buffer[i]\n    set current tuple cur[i] to first tuple in buffer[i]\n }\n while (more than 1 run still has tuples) {\n    i = find buffer with smallest current tuple\n    if (output buffer full) { write it and clear it}\n    copy current tuple in buffer[i] to output buffer\n    advance to next tuple in buffer[i]\n    if (no more tuples in buffer[i]) {\n       if (no more pages in run feeding buffer[i])\n          mark run as complete\n       else {\n          read next page of run into buffer[i]\n          set current tuple in buffer[i] as first tuple\n }  }  }\n copy tuples in non-empty buffer to output\n",
  "key_words": [
   "n-way",
   "merge",
   "sort"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Cost of n-Way Merge Sort \n",
  "content": "Consider file where b = 4096, B = 16 total buffers:\n pass 0 produces 256 \u00d7 16-page sorted runs\n pass 1 produces 18 \u00d7 240-page sorted runs\n pass 2 produces 2 \u00d7 3600-page sorted run\n pass 3 produces 1 \u00d7 4096-page sorted run\n (cf. two-way merge sort which needs 11 passes)\n For b data pages and n=15 input buffers (15-way merge)\n first pass: read/writes b pages, gives b0 = \u2308b/B\u2309 runs\n then need \u2308lognb0\u2309 passes until sorted\n each pass reads and writes b pages (2.b )\n Cost = 2.b.(1 + \u2308lognb0\u2309),   where b0 = \u2308b/B\u2309\n",
  "key_words": [
   "cost",
   "of",
   "n-way",
   "merge",
   "sort"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Sorting in PostgreSQL \n",
  "content": "Sort uses a polyphase merge-sort (from Knuth):\n backend/utils/sort/tuplesort.c\n Tuples are mapped to SortTuple structs for sorting:\n containing pointer to tuple and sort key\n no need to reference actual Tuples during sort\n unless multiple attributes used in sort\n If all data fits into memory, sort using qsort().\n If memory fills while reading, form \"runs\" and do disk-based sort.\n",
  "key_words": [
   "sorting",
   "in",
   "postgresql"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Sorting in PostgreSQL \n",
  "content": "Disk-based sort has phases:\n divide input into sorted runs using HeapSort\n merge using N buffers, one output buffer\n N = as many buffers as workMem allows\n Described in terms of \"tapes\" (\"tape\" \u2245 sorted run)\n Implementation of \"tapes\": backend/utils/sort/logtape.c\n",
  "key_words": [
   "sorting",
   "in",
   "postgresql"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Sorting in PostgreSQL \n",
  "content": "Sorting is generic and comparison operators are defined in catalog:\n // gets pointer to function via pg_operator\n SelectSortFunction(Oid sortOperator,\n                    bool nulls_first,\n                    Oid *sortFunction,\n                    int *sortFlags);\n // returns negative, zero, positive\n ApplySortFunction(FmgrInfo *sortFunction,\n                   int sortFlags,\n                   Datum datum1, bool isNull1,\n                   Datum datum2, bool isNull2);\n Flags indicate: ascending/descending, nulls-first/last.\n ApplySortFunction() is PostgreSQL's version of tupCompare()\n Implementing Projection\n",
  "key_words": [
   "sorting",
   "in",
   "postgresql"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "The Projection Operation \n",
  "content": "Consider the query:\n select distinct name,age from Employee;\n If the Employee relation has four tuples such as:\n (94002, John, Sales, Manager,   32)\n (95212, Jane, Admin, Manager,   39)\n (96341, John, Admin, Secretary, 32)\n (91234, Jane, Admin, Secretary, 21)\n then the result of the projection is:\n (Jane, 21)   (Jane, 39)   (John, 32)\n Note that duplicate tuples (e.g. (John,32)) are eliminated.\n",
  "key_words": [
   "the",
   "projection",
   "operation"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... The Projection Operation \n",
  "content": "The projection operation needs to:\n 1. scan the entire relation as input\n already seen how to do scanning\n 2. remove unwanted attributes in output tuples\n implementation depends on tuple internal structure\n essentially, make a new tuple with fewer attributes and where the values may be computed from existing attributes\n 3. eliminate any duplicates produced   (if distinct)\n two approaches: sorting or hashing\n",
  "key_words": [
   "the",
   "projection",
   "operation"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Sort-based Projection \n",
  "content": "Requires a temporary file/relation (Temp)\n for each tuple T in Rel {\n     T' = mkTuple([attrs],T)\n     write T' to Temp\n }\n sort Temp on [attrs]\n for each tuple T in Temp {\n     if (T == Prev) continue\n     write T to Result\n     Prev = T\n }\n",
  "key_words": [
   "sort-based",
   "projection"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Cost of Sort-based Projection \n",
  "content": "The costs involved are (assuming n+1 buffers for sort):\n scanning original relation Rel:   bR   (with cR)\n writing Temp relation:   bT     (smaller tuples, cT > cR, sorted)\n sorting Temp relation:   2.bT.ceil(lognb0) where b0 = ceil(bT/(n+1))\n scanning Temp, removing duplicates:   bT\n writing the result relation:   bOut     (maybe less tuples)\n Cost = sum of above = bR + bT + 2.bT.ceil(lognb0) + bT + bOut\n",
  "key_words": [
   "cost",
   "of",
   "sort-based",
   "projection"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Hash-based Projection \n",
  "content": "Partitioning phase:\n",
  "key_words": [
   "hash-based",
   "projection"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Hash-based Projection \n",
  "content": "Duplicate elimination phase:\n",
  "key_words": [
   "hash-based",
   "projection"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Hash-based Projection \n",
  "content": "Algorithm for both phases:\n for each tuple T in relation Rel {\n     T' = mkTuple([attrs],T)\n     H = h1(T', n)\n     B = buffer for partition[H]\n     if (B full) write and clear B\n     insert T' into B\n }\n for each partition P in 0..n-1 {\n     for each tuple T in partition P {\n         H = h2(T, n)\n         B = buffer for hash value H\n         if (T not in B) insert T into B\n         // assumes B never gets full\n     }\n     write and clear all buffers\n }\n",
  "key_words": [
   "hash-based",
   "projection"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Cost of Hash-based Projection \n",
  "content": "The total cost is the sum of the following:\n scanning original relation R:   bR\n writing partitions:   bP   (bR vs bP ?)\n re-reading partitions:   bP\n writing the result relation:   bOut\n Cost = bR + 2bP + bOut\n To ensure that n is larger than the largest partition ...\n use hash functions (h1,h2) with uniform spread\n allocate at least sqrt(bR)+1 buffers\n if insufficient buffers, significant re-reading overhead\n",
  "key_words": [
   "cost",
   "of",
   "hash-based",
   "projection"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Projection on Primary Key \n",
  "content": "No duplicates, so the above approaches are not required.\n Method:\n bR = nPages(Rel)\n for i in 0 .. bR-1 {\n    P = read page i\n    for j in 0 .. nTuples(P) {\n       T = getTuple(P,j)\n       T' = mkTuple(pk, T)\n       if (outBuf is full) write and clear\n       append T' to outBuf\n    }\n }\n if (nTuples(outBuf) > 0) write\n",
  "key_words": [
   "projection",
   "on",
   "primary",
   "key"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Index-only Projection \n",
  "content": "Can do projection without accessing data file iff ...\n relation is indexed on (A1,A2,...An)    (indexes described later)\n projected attributes are a prefix of (A1,A2,...An)\n Basic idea:\n scan through index file (which is already sorted on attributes)\n duplicates are already adjacent in index, so easy to skip\n Cost analysis ...\n index has bi pages   (where bi \u226a bR)\n Cost = bi reads + bOut writes\n",
  "key_words": [
   "index-only",
   "projection"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Comparison of Projection Methods \n",
  "content": "Difficult to compare, since they make different assumptions:\n index-only: needs an appropriate index\n hash-based: needs buffers and good hash functions\n sort-based: needs only buffers \u21d2 use as default\n Best case scenario for each (assuming n+1 in-memory buffers):\n index-only:   bi + bOut   \u226a   bR + bOut\n hash-based:   bR + 2.bP + bOut\n sort-based:   bR + bT + 2.bT.ceil(lognb0) + bT + bOut\n We normally omit bOut, since each method produces the same result\n",
  "key_words": [
   "comparison",
   "of",
   "projection",
   "method"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Projection in PostgreSQL \n",
  "content": "Code for projection forms part of execution iterators:\n backend/executor/execQual.c\n Functions involved with projection:\n ExecProject(projInfo,...) ... extracts/stores projected data\n ExecTargetList(...) ... makes new tuple from old tuple + projection info\n ExecStoreTuple(newTuple,...) ... save tuple in output slot\n Implementing Selection\n",
  "key_words": [
   "projection",
   "in",
   "postgresql"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Varieties of Selection \n",
  "content": "Selection:   select * from R where C\n filters a subset of tuples from one relation R\n based on a condition C on the attribute values\n We consider three distinct styles of selection:\n 1-d (one dimensional)   (condition uses only 1 attribute)\n n-d (multi-dimensional)   (condition uses >1 attribute)\n similarity   (approximate matching, with ranking)\n Each style has several possible file-structures/techniques.\n",
  "key_words": [
   "variety",
   "of",
   "selection"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Varieties of Selection \n",
  "content": "We can view a relation as defining a tuple space\n assume relation R with attributes a1,...,an\n attribute domains of R specify a n-dimensional space\n each tuple (v1,v2,...,vn) \u2208 R is a point in that space\n queries specify values/ranges on N\u22651 dimensions\n a query defines a point/line/plane/region of the n-d space\n results are tuples lying at/on/in that point/line/plane/region\n E.g. if N=n, we are checking existence of a tuple (at a point)\n",
  "key_words": [
   "variety",
   "of",
   "selection"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Varieties of Selection \n",
  "content": "One-dimensional selection queries = condition on single attribute.\n one: select * from R where k = val\n where k is a unique attribute and val is a constant\n pmr: select * from R where k = val\n where k is non-unique and val is a constant\n range: select * from R where k \u2265 lo and k \u2264 hi\n where k is any attribute and lo and hi are constants\n either lo or hi may be omitted for open-ended range\n",
  "key_words": [
   "variety",
   "of",
   "selection"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Implementing Select Efficiently \n",
  "content": "Two basic approaches:\n physical arrangement of tuples\n sorting   (search strategy)\n hashing   (static, dynamic, n-dimensional)\n additional indexing information\n index files   (primary, secondary, trees)\n signatures   (superimposed, disjoint)\n Our analyses assume: 1 input buffer available for each relation.\n If more buffers are available, most methods benefit.\n",
  "key_words": [
   "implementing",
   "select",
   "efficiently"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Recap on Implementing Selection \n",
  "content": "Selection =  select * from  R  where  C\n yields a subset of R tuples satisfying condition C\n a very important (frequent) operation in relational databases\n Types of selection determined by type of condition\n one:  select * from  R  where id =  k\n pmr:  select * from  R  where age=65  (1-d)\n          select * from  R  where age=65 and gender='m'  (n-d)\n rng:   select * from  R  where age\u226518 and age\u226421  (1-d)\n          select * from  R  where age between 18 and 21  (n-d)\n                                              and height between 160 and 190 note: rng = range\n",
  "key_words": [
   "recap",
   "on",
   "implementing",
   "selection"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Recap on Implementing Selection \n",
  "content": "Strategies for implementing selection efficiently\n arrangement of tuples in file  (e.g. sorting, hashing)\n auxiliary data structures  (e.g. indexes, signatures)\n Interested in cost for select, delete, update, and insert\n for select, simply count number of pages read nr\n for others, use nr and nw to distinguish reads/writes\n Typical file structure has\n b main data pages, bOv overflow pages, c tuples per page\n auxiliary files with e.g. oversized values, index entries\n Heap Files\n Note: this is not \"heap\" as in the top-to-bottom ordered tree. It means simply an unordered collection of tuples in a file.\n",
  "key_words": [
   "recap",
   "on",
   "implementing",
   "selection"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Selection in Heaps \n",
  "content": "For all selection queries, the only possible strategy is:\n // select * from R where C\n f = openFile(fileName(\"R\"),READ);\n for (p = 0; p < nPages(f); p++) {\n     buf = readPage(f, p);\n     for (i = 0; i < nTuples(buf); i++) {\n         tup = getTuple(buf,i);\n         if (tup satisfies C)\n             add tup to result set\n     }\n }\n i.e. linear scan through file searching for matching tuples\n",
  "key_words": [
   "selection",
   "in",
   "heap"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Selection in Heaps \n",
  "content": "The heap is scanned from the first to the last page:\n Costrange  =  Costpmr  =  b\n If we know that only one tuple matches the query (one query), a simple optimisation is to stop the scan once that tuple is found.\n Costone :      Best = 1      Average = b/2      Worst = b\n",
  "key_words": [
   "selection",
   "in",
   "heap"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Insertion in Heaps \n",
  "content": "Insertion: new tuple is appended to file (in last page).\n f = openFile(fileName(\"R\"),READ|WRITE);\n b = nPages(f)-1;\n buf = readPage(f, b);  // request page\n if (isFull(buf)) // all slots used\n     { b++; clear(buf); }  // add a new page\n if (tooLarge(newTup,buf)) // not enough space for tuple\n     { deal with oversize tuple }\n insertTuple(newTup, buf);\n writePage(f, b, buf);  // mark page as dirty & release\n Costinsert  =  1r + 1w\n Plus possible extra writes for oversize tuples, e.g. PostgreSQL's TOAST files\n",
  "key_words": [
   "insertion",
   "in",
   "heap"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Insertion in Heaps \n",
  "content": "Alternative strategy:\n find any page from R with enough space\n preferably a page already loaded into memory buffer\n PostgreSQL's strategy:\n use last updated page of R in buffer pool\n otherwise, search buffer pool for page with enough space\n assisted by free space map (FSM) associated with each table\n for details: backend/access/heap/{heapam.c,hio.c}\n",
  "key_words": [
   "insertion",
   "in",
   "heap"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Insertion in Heaps \n",
  "content": "PostgreSQL's tuple insertion:\n heap_insert(Relation relation,    // relation desc\n             HeapTuple newtup,     // new tuple data\n             CommandId cid, ...)   // SQL statement\n finds page which has enough free space for newtup\n ensures page loaded into buffer pool and locked\n copies tuple data into page buffer, sets xmin, etc.\n marks buffer as dirty\n writes details of insertion into transaction log\n returns OID of new tuple if relation has OIDs\n",
  "key_words": [
   "insertion",
   "in",
   "heap"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Deletion in Heaps \n",
  "content": "SQL:  delete from R  where Condition\n Implementation of deletion:\n f = openFile(fileName(\"R\"),READ|WRITE);\n for (p = 0; p < nPages(f); p++) {\n     buf = readPage(f, p);\n     ndels = 0;\n     for (i = 0; i < nTuples(buf); i++) {\n         tup = getTuple(buf,i);\n         if (tup satisfies Condition)\n             { ndels++; deleteTuple(buf,i); }\n     }\n     if (ndels > 0) writePage(f, p, buf);\n     if (ndels > 0 && unique) break;\n }\n If buffers,   read = request,   write = mark-as-dirty\n",
  "key_words": [
   "deletion",
   "in",
   "heap"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Exercise 6: Cost of Deletion in Heaps \n",
  "content": "Consider the following queries ...\n delete from Employees where id = 12345  -- one\n delete from Employees where dept = 'Marketing'  -- pmr\n delete from Employees where 40 <= age and age < 50  -- range\n Show how each will be executed and estimate the cost, assuming:\n b = 100,  bq2 = 3,  bq3 = 20\n State any other assumptions.\n Generalise the cost models for each query type.\n",
  "key_words": [
   "exercise",
   "6",
   "cost",
   "of",
   "deletion",
   "in",
   "heap"
  ],
  "question": null,
  "intent_tag": "exercise"
 },
 {
  "topic": "... Deletion in Heaps \n",
  "content": "PostgreSQL tuple deletion:\n heap_delete(Relation relation,    // relation desc\n             ItemPointer tid, ..., // tupleID\n             CommandId cid, ...)   // SQL statement\n gets page containing tuple into buffer pool and locks it\n sets flags, commandID and xmax in tuple; dirties buffer\n writes indication of deletion to transaction log\n Vacuuming eventually compacts space in each page.\n",
  "key_words": [
   "deletion",
   "in",
   "heap"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Updates in Heaps \n",
  "content": "SQL:  update R  set F = val  where Condition\n Analysis for updates is similar to that for deletion\n scan all pages\n replace any updated tuples   (within each page)\n write affected pages to disk\n Costupdate  =  br + bqw\n Complication: new version of tuple larger than old version  (too big for page)\n Solution:   delete, re-organise free space, then insert\n",
  "key_words": [
   "update",
   "in",
   "heap"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Updates in Heaps \n",
  "content": "PostgreSQL tuple update:\n heap_update(Relation relation,     // relation desc\n             ItemPointer otid,      // old tupleID\n             HeapTuple newtup, ..., // new tuple data\n             CommandId cid, ...)    // SQL statement\n essentially does delete(otid), then insert(newtup)\n also, sets old tuple's ctid field to reference new tuple\n can also update-in-place if no referencing transactions\n",
  "key_words": [
   "update",
   "in",
   "heap"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Heaps in PostgreSQL \n",
  "content": "PostgreSQL stores all table data in heap files (by default).\n Typically there are also associated index files.\n If a file is more useful in some other form:\n PostgreSQL may make a transformed copy during query execution\n programmer can set it via   create index...using hash\n Heap file implementation:   src/backend/access/heap\n",
  "key_words": [
   "heap",
   "in",
   "postgresql"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Heaps in PostgreSQL \n",
  "content": "PostgreSQL \"heap file\" may use multiple physical files\n files are named after the OID of the corresponding table\n first data file is called simply OID\n if size exceeds 1GB, create a fork called OID.1\n add more forks as data size grows (one fork for each 1GB)\n other files:\n free space map (OID_fsm), visibility map (OID_vm)\n optionally, TOAST file (if table has varlen attributes)\n for details: Chapter 55 in PostgreSQL documentation\n Sorted Files\n",
  "key_words": [
   "heap",
   "in",
   "postgresql"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Sorted Files \n",
  "content": "Records stored in file in order of some field k (the sort key).\n Makes searching more efficient; makes insertion less efficient\n E.g. assume c = 4\n",
  "key_words": [
   "sorted",
   "file"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Sorted Files \n",
  "content": "In order to mitigate insertion costs, use overflow blocks.\n Total number of overflow blocks = bov.\n Average overflow chain length = Ov = bov / b.\n Bucket = data page + its overflow page(s)\n",
  "key_words": [
   "sorted",
   "file"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Selection in Sorted Files \n",
  "content": "For one queries on sort key, use binary search.\n // select * from R where k = val  (sorted on R.k)\n lo = 0; hi = b-1\n while (lo <= hi) {\n     mid = (lo+hi) / 2;  // int division with truncation\n     (tup,loVal,hiVal) = searchBucket(f,mid,k,val);\n     if (tup != NULL) return tup;\n     else if (val < loVal) hi = mid - 1;\n     else if (val > hiVal) lo = mid + 1;\n     else return NOT_FOUND;\n }\n return NOT_FOUND;\n where  f is file for relation,  mid,lo,hi are page indexes,             k is a field/attr,  val,loVal,hiVal are values for k\n",
  "key_words": [
   "selection",
   "in",
   "sorted",
   "file"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Selection in Sorted Files \n",
  "content": "Search a page and its overflow chain for a key value\n searchBucket(f,p,k,val)\n {\n     buf = getPage(f,p);\n     (tup,min,max) = searchPage(buf,k,val,+INF,-INF)\n     if (tup != NULL) return(tup,min,max);\n     ovf = openOvFile(f);\n     ovp = ovflow(buf);\n     while (tup == NULL && ovp != NO_PAGE) {\n         buf = getPage(ovf,ovp);\n         (tup,min,max) = searchPage(buf,k,val,min,max)\n         ovp = ovflow(buf);\n     }         return (tup,min,max);\n }\n Assumes each page contains index of next page in Ov chain\n",
  "key_words": [
   "selection",
   "in",
   "sorted",
   "file"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Selection in Sorted Files \n",
  "content": "Search within a page for key; also find min/max key values\n searchPage(buf,k,val,min,max)\n {\n     res = NULL;\n     for (i = 0; i < nTuples(buf); i++) {\n         tup = getTuple(buf,i);\n         if (tup.k == val) res = tup;\n         if (tup.k < min) min = tup.k;\n         if (tup.k > max) max = tup.k;\n     }\n     return (res,min,max);\n }\n",
  "key_words": [
   "selection",
   "in",
   "sorted",
   "file"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Selection in Sorted Files \n",
  "content": "The above method treats each bucket like a single large page.\n Cases:\n best: find tuple in first data page we read\n worst: full binary search, and not found\n examine log2b data pages\n plus examine all of their overflow pages\n average: examine some data pages + their overflow pages\n Costone :     Best = 1      Worst = log2 b + bov\n Average case cost analysis relies on assumptions  (e.g. data distribution)\n",
  "key_words": [
   "selection",
   "in",
   "sorted",
   "file"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Selection in Sorted Files \n",
  "content": "For pmr query, on non-unique attribute k, where file is sorted on k\n tuples containing k may span several pages\n E.g. select * from R where k = 2\n Begin by locating a page p containing k=val   (as for one query).\n Scan backwards and forwards from p to find matches.\n Thus, Costpmr  =  Costone + (bq-1).(1+Ov)\n",
  "key_words": [
   "selection",
   "in",
   "sorted",
   "file"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Selection in Sorted Files \n",
  "content": "For range queries on unique sort key (e.g. primary key):\n use binary search to find lower bound\n read sequentially until reach upper bound\n E.g. select * from R where k >= 5 and k <= 13\n Costrange  =  Costone + (bq-1).(1+Ov)\n",
  "key_words": [
   "selection",
   "in",
   "sorted",
   "file"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Selection in Sorted Files \n",
  "content": "For range queries on non-unique sort key, similar method to pmr.\n binary search to find lower bound\n then go backwards to start of run\n then go forwards to last occurence of upper-bound\n E.g. select * from R where k >= 2 and k <= 6\n Costrange = Costone + (bq-1).(1+Ov)\n",
  "key_words": [
   "selection",
   "in",
   "sorted",
   "file"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Selection in Sorted Files \n",
  "content": "So far, have assumed query condition involves sort key k.\n If condition contains attribute j, not the sort key\n file is unlikely to be sorted by j as well\n sortedness gives no searching benefits\n Costone,   Costrange,   Costpmr   as for heap files\n",
  "key_words": [
   "selection",
   "in",
   "sorted",
   "file"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Updates to Sorted Files \n",
  "content": "Insertion approach:\n find appropriate page for tuple (via binary search)\n if page not full, insert into page\n otherwise, insert into next overflow block with space\n Thus, Costinsert  =  Costone + \u03b4w         (where \u03b4w = 1 or 2)\n Deletion strategy:\n find matching tuple(s)\n mark them as deleted\n Cost depends on selectivity of selection condition\n Thus, Costdelete  =  Costselect + bqw\n Hashed Files\n",
  "key_words": [
   "update",
   "to",
   "sorted",
   "file"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Hashing \n",
  "content": "use key value to compute page address of tuple.\n e.g. tuple with key = v is stored in page i\n Requires: hash function h(v) that maps KeyDomain \u2192 [0..b-1].\n hashing converts key value (any type) into integer value\n integer value is then mapped to page index\n note: can view integer value as a bit-string\n",
  "key_words": [
   "basic",
   "idea",
   "of",
   "hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Hashing \n",
  "content": "PostgreSQL hash function (simplified):\n uint32 hash_any(unsigned char *k, register int keylen)\n {\n    register uint32 a, b, c, len;\n    /* Set up the internal state */\n   len = keylen;  a = b = 0x9e3779b9;  c = 3923095;\n   /* handle most of the key */\n   while (len >= 12) {\n       a += (k[0] + (k[1]<<8) + (k[2]<<16) + (k[3]<<24));\n       b += (k[4] + (k[5]<<8) + (k[6]<<16) + (k[7]<<24));\n       c += (k[8] + (k[9]<<8) + (k[10]<<16) + (k[11]<<24));\n       mix(a, b, c);   k += 12; len -= 12;\n    }\n",
  "key_words": [
   "implementing",
   "hashing",
   "code",
   "in",
   "postgresql"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Hashing \n",
  "content": "hash_any() gives hash value as 32-bit quantity (uint32).\n Two ways to map raw hash value into a page address:\n if b = 2k, bitwise AND with k low-order bits set to one\n uint32 hashToPageNum(uint32 hval) {\n     uint32 mask = 0xFFFFFFFF;\n     return (hval & (mask >> (32-k)));\n }\n otherwise, use mod to produce value in range 0..b-1\n uint32 hashToPageNum(uint32 hval) {\n     return (hval % b);\n }\n",
  "key_words": [
   "implementing",
   "hashing",
   "code",
   "in",
   "postgresql"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Hashing Performance \n",
  "content": "Aims:\n distribute tuples evenly amongst buckets\n have most buckets nearly full   (attempt to minimise wasted space)\n Note: if data distribution not uniform, address distribution can't be uniform.\n Best case: every bucket contains same number of tuples.\n Worst case: every tuple hashes to same bucket.\n Average case: some buckets have more tuples than others.\n Use overflow pages to handle \"overfull\" buckets  (cf. sorted files)\n All tuples in each bucket must have same hash value.\n",
  "key_words": [
   "hashing",
   "performance"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Hashing Performance \n",
  "content": "Two important measures for hash files:\n load factor:   L  =  r / bc\n average overflow chain length:   Ov  =  bov / b\n Three cases for distribution of tuples in a hashed file:\n Case L Ov\n Best \u2245 1 0\n Worst >> 1 **\n Average < 1 0<Ov<1\n (** performance is same as Heap File)\n To achieve average case, aim for   0.75  \u2264  L  \u2264  0.9.\n",
  "key_words": [
   "hashing",
   "performance"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Selection with Hashing \n",
  "content": "Best performance occurs for one queries on hash key field.\n Basic strategy:\n compute page address via hash function hash(val)\n fetch that page and look for matching tuple\n possibly fetch additional pages from overflow chain\n Best Costone  =  1    (find in data page)\n Average Costone  =  1+Ov/2    (scan half of ovflow chain)\n Worst Costone  =  1+max(OvLen)    (find in last page of ovflow chain)\n",
  "key_words": [
   "selection",
   "with",
   "hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Selection with Hashing \n",
  "content": "Select via hashing on unique key k (one)\n // select * from R where k = val\n f = openFile(relName(\"R\"),READ);\n p = hash(val) % nPages(f);\n buf = getPage(f, p)\n for (i = 0; i < nTuples(buf); i++) {\n     tup = getTuple(buf,i);\n     if (tup.k == val) return tup;\n }\n ovp = ovflow(buf);\n while (ovp != NO_PAGE) {\n     buf = getPage(ovf,ovp);\n     for (i = 0; i < nTuples(Buf); i++) {\n         tup = getTuple(buf,i);\n         if (tup.k == val) return tup;\n }   }\n",
  "key_words": [
   "selection",
   "with",
   "hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Selection with Hashing \n",
  "content": "Select via hashing on non-unique hash key k (pmr)\n // select * from R where k = val\n f = openFile(relName(\"R\"),READ);\n p = hash(val) % nPages(f);\n buf = getPage(f, p)\n for (i = 0; i < nTuples(buf); i++) {\n     tup = getTuple(buf,i);\n     if (tup.k == val) append tup to results\n }\n ovp = ovflow(buf);\n while (ovp != NO_PAGE) {\n     buf = getPage(ovf,ovp);\n     for (i = 0; i < nTuples(Buf); i++) {\n         tup = getTuple(buf,i);\n         if (tup.k == val) append tup to results\n }   }\n Costpmr  =  1 + Ov\n",
  "key_words": [
   "selection",
   "with",
   "hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Selection with Hashing \n",
  "content": "Hashing does not help with range queries** ...\n Costrange = b + bov\n Selection on attribute j which is not hash key ...\n Costone,   Costrange,   Costpmr  =  b + bov\n ** unless the hash function is order-preserving (and most aren't)\n",
  "key_words": [
   "selection",
   "with",
   "hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Insertion with Hashing \n",
  "content": "Insertion uses similar process to one queries.\n // insert tuple t with key=val into rel R\n // f = data file ... ovf = ovflow file\n p = hash(val) % nPages(R)\n P = getPage(f,p)\n if (tup fits in page P)     { insert t into P; return }\n for each overflow page Q of P {\n     if (tup fits in page Q)\n         { insert t into Q; return }\n }\n add new overflow page Q\n link Q to previous overflow page\n insert t into Q\n Costinsert:    Best: 1r + 1w    Worst: 1+max(OvLen))r + 2w\n",
  "key_words": [
   "insertion",
   "with",
   "hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Deletion with Hashing \n",
  "content": "Similar performance to select:\n // delete from R where k = val\n // f = data file ... ovf = ovflow file\n p = hash(val) % nPages(R)\n buf = getPage(f,p)\n ndel = delTuples(buf,k,val)\n if (ndel > 0) putPage(f,buf,p)\n p = ovFlow(buf)\n while (p != NO_PAGE) {\n     buf = getPage(ovf,p)\n     ndel = delTuples(buf,k,val)\n     if (ndel > 0) putPage(ovf,buf,p)\n     p = ovFlow(buf)\n }\n Extra cost over select is cost of writing back modified blocks.\n Method works for both unique and non-unique hash keys.\n",
  "key_words": [
   "deletion",
   "with",
   "hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Problem with Hashing... \n",
  "content": "So far, discussion of hashing has assumed a fixed file size (fixed b).\n What size file to use?\n the size we need right now   (performance degrades as file overflows)\n the maximum size we might ever need   (signifcant waste of space)\n Change file size \u21d2 change hash function \u21d2 rebuild file\n Methods for hashing with dynamic files:\n extendible hashing, dynamic hashing   (need a directory, no overflows)\n linear hashing   (expands file \"sytematically\", no directory, has overflows)\n",
  "key_words": [
   "problem",
   "with",
   "hashing..."
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Problem with Hashing... \n",
  "content": "All flexible hashing methods ...\n treat hash as 32-bit bit-string\n adjust hashing by using more/less bits\n Start with hash function to convert value to bit-string:\n uint32 hash(unsigned char *val)\n Require a function to extract d bits from bit-string:\n unit32 bits(int d, uint32 val)\n Use result of bits() as page address.\n",
  "key_words": [
   "problem",
   "with",
   "hashing..."
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Exercise 10: Bit Manipulation \n",
  "content": "1. Write a function to display uint32 values as 01010110...\n char *showBits(uint32 val, char *buf);\n Analogous to gets()   (assumes supplied buffer large enough)\n 2. Write a function to extract the d bits of a uint32\n uint32 bits(int d, uint32 val);\n If d > 0, gives low-order bits; if d < 0, gives high-order bits\n",
  "key_words": [
   "exercise",
   "10",
   "bit",
   "manipulation"
  ],
  "question": null,
  "intent_tag": "exercise"
 },
 {
  "topic": "... Problem with Hashing... \n",
  "content": "Important concept for flexible hashing: splitting\n consider one page (all tuples have same hash value)\n recompute page numbers by considering one extra bit\n if current page is 101, new pages have hashes 0101 and 1101\n some tuples stay in page 0101 (was 101)\n some tuples move to page 1101 (new page)\n also, rehash any tuples in overflow pages of page 101\n Result: expandable data file, never requiring a complete file rebuild\n",
  "key_words": [
   "problem",
   "with",
   "hashing..."
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Problem with Hashing... \n",
  "content": "Example of splitting:\n Tuples only show key value; assume h(val) = val\n",
  "key_words": [
   "problem",
   "with",
   "hashing..."
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Linear Hashing \n",
  "content": "File organisation:\n file of primary data blocks\n file of overflow data blocks\n a register called the split pointer (sp)\n Uses systematic method of growing data file ...\n hash function \"adapts\" to changing address range\n systematic splitting controls length of overflow chains\n Advantage: does not require auxiliary storage for a directory\n Disadvantage: requires overflow pages   (splits don't occur on full pages)\n",
  "key_words": [
   "linear",
   "hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Linear Hashing \n",
  "content": "File grows linearly (one block at a time, at regular intervals).\n Has \"phases\" of expansion; over each phase, b doubles.\n",
  "key_words": [
   "linear",
   "hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Selection with Lin.Hashing \n",
  "content": "If b=2d, the file behaves exactly like standard hashing.\n Use d bits of hash to compute block address.\n // select * from R where k = val\n h = hash(val);\n P = bits(d,h);  // lower-order bits\n for each tuple t in page P\n          and its overflow pages {\n     if (t.k == val) return t;\n }\n Average Costone  =  1+Ov\n",
  "key_words": [
   "selection",
   "with",
   "lin.hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Selection with Lin.Hashing \n",
  "content": "If b != 2d, treat different parts of the file differently.\n Parts A and C are treated as if part of a file of size 2d+1.\n Part B is treated as if part of a file of size 2d.\n Part D does not yet exist (tuples in B may move into it).\n",
  "key_words": [
   "selection",
   "with",
   "lin.hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Selection with Lin.Hashing \n",
  "content": "Modified search algorithm:\n // select * from R where k = val\n h = hash(val);\n P = bits(d,h);\n if (P < sp) { P = bits(d+1,h); }\n for each tuple t in page P\n          and its overflow blocks {\n     if (t.k == val) return R;\n }\n",
  "key_words": [
   "selection",
   "with",
   "lin.hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "File Expansion with Lin.Hashing \n",
  "content": "",
  "key_words": [
   "file",
   "expansion",
   "with",
   "lin.hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Insertion with Lin.Hashing \n",
  "content": "Abstract view:\n P = bits(d,hash(val));\n if (P < sp) P = bits(d+1,hash(val));\n // bucket P = page P + its overflow pages\n for each page Q in bucket P {\n     if (space in Q) {\n         insert tuple into Q\n         break\n     }\n }\n if (no insertion) {\n     add new ovflow page to bucket P\n     insert tuple into new page\n }\n if (need to split) {\n     partition tuples from bucket sp\n           into buckets sp and sp+2^d\n     sp++;\n     if (sp == 2^d) { d++; sp = 0; }\n }\n",
  "key_words": [
   "insertion",
   "with",
   "lin.hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Splitting \n",
  "content": " Two approaches to triggering a split:\n split every time a tuple is inserted into full block\n split when load factor reaches threshold (every k inserts)\n Note: always split block sp, even if not full/\"current\"\n Systematic splitting like this ...\n eventually reduces length of every overflow chain\n helps to maintain short average overflow chain length\n",
  "key_words": [
   "splitting"
  ],
  "question": [
   "How to decide that we \"need to split\"?"
  ],
  "intent_tag": "unknow"
 },
 {
  "topic": "... Splitting \n",
  "content": "Splitting process for block sp=01:\n",
  "key_words": [
   "splitting"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Splitting \n",
  "content": "Detailed splitting algorithm:\n // partitions tuples between two buckets\n newp = sp + 2^d; oldp = sp;\n buf = getPage(f,sp);\n clear(oldBuf); clear(newBuf);\n for (i = 0; i < nTuples(buf); i++) {\n     tup = getTuple(buf,i);\n     p = bits(d+1,hash(tup.k));\n     if (p == newp)         addTuple(newBuf,tup);\n     else\n         addTuple(oldBuf,tup);\n }\n p = ovflow(buf);  oldOv = newOv = 0;\n while (p != NO_PAGE) {\n     ovbuf = getPage(ovf,p);\n     for (i = 0; i < nTuples(ovbuf); i++) {\n         tup = getTuple(buf,i);\n         p = bits(d+1,hash(tup.k));\n         if (p == newp) {\n             if (isFull(newBuf)) {\n                 nextp = nextFree(ovf);\n                 ovflow(newBuf) = nextp;\n                 outf = newOv ? f : ovf;\n                 writePage(outf, newp, newBuf);\n                 newOv++; newp = nextp; clear(newBuf);\n             }\n             addTuple(newBuf, tup);\n         }\n         else {\n             if (isFull(oldBuf)) {\n                 nextp = nextFree(ovf);\n                 ovflow(oldBuf) = nextp;\n                 outf = oldOv ? f : ovf;\n                 writePage(outf, oldp, oldBuf);\n                 oldOv++; oldp = nextp; clear(oldBuf);\n             }\n             addTuple(oldBuf, tup);\n         }\n     }\n     addToFreeList(ovf,p);\n     p = ovflow(buf);\n }\n sp++;\n if (sp == 2^d) { d++; sp = 0; }\n",
  "key_words": [
   "splitting"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Insertion Cost \n",
  "content": "If no split required, cost same as for standard hashing:\n Costinsert:   Best: 1r + 1w,   Avg: (1+Ov)r + 1w,   Worst: (1+max(Ov))r + 2w\n If split occurs, incur Costinsert  plus cost of splitting:\n read block sp   (plus all of its overflow blocks)\n write block sp   (and its new overflow blocks)\n write block sp+2d   (and its new overflow blocks)\n On average, Costsplit  =  (1+Ov)r + (2+Ov)w\n",
  "key_words": [
   "insertion",
   "cost"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Deletion with Lin.Hashing \n",
  "content": "Deletion is similar to ordinary static hash file.\n But might wish to contract file when enough tuples removed.\n Rationale: r shrinks, b stays large \u21d2 wasted space.\n Method: remove last bucket in data file (contracts linearly).\n Involves a coalesce procedure which is an inverse split.\n",
  "key_words": [
   "deletion",
   "with",
   "lin.hashing"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "Hash Files in PostgreSQL \n",
  "content": "PostgreSQL uses linear hashing on tables which have been:\n create index Ix on R using hash (k);\n Hash file implementation: backend/access/hash\n hashfunc.c ... a family of hash functions\n hashinsert.c ... insert, with overflows\n hashpage.c ... utilities + splitting\n hashsearch.c ... iterator for hash files\n Based on \"A New Hashing Package for Unix\", Margo Seltzer, Winter Usenix 1991\n",
  "key_words": [
   "hash",
   "file",
   "in",
   "postgresql"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Hash Files in PostgreSQL \n",
  "content": "PostgreSQL uses slightly different file organisation ...\n has a single file containing main and overflow pages\n has groups of main pages of size 2n\n in between groups, arbitrary number of overflow pages\n maintains collection of \"split pointers\" in header page\n each split pointer indicates start of main page group\n If overflow pages become empty, add to free list and re-use.\n",
  "key_words": [
   "hash",
   "file",
   "in",
   "postgresql"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Hash Files in PostgreSQL \n",
  "content": "PostgreSQL hash file structure:",
  "key_words": [
   "hash",
   "file",
   "in",
   "postgresql"
  ],
  "question": null,
  "intent_tag": "unknow"
 },
 {
  "topic": "... Hash Files in PostgreSQL \n",
  "content": "Converting bucket # to page address:\n // which page is primary page of bucket\n uint bucket_to_page(headerp, B) {\n    uint *splits = headerp->hashm_spares;\n    uint chunk, base, offset, lg2(uint);\n    chunk = (B<2) ? 0 : lg2(B+1)-1;\n    base = splits[chunk];\n    offset = (B<2) ? B : B-(1<chunk);\n    return (base + offset);\n }\n // returns ceil(log_2(n))\n int lg2(uint n) {\n  int i, v;\n  for (i = 0, v = 1; v < n; v <= 1) i++;\n  return i;\n }\n Produced: 16 Aug 2018\n",
  "key_words": [
   "hash",
   "file",
   "in",
   "postgresql"
  ],
  "question": null,
  "intent_tag": "unknow"
 }
]