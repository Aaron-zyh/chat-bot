{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week1completed\n",
      "week2completed\n",
      "week3completed\n",
      "week4completed\n",
      "week6completed\n",
      "week7completed\n",
      "week8completed\n",
      "week9completed\n",
      "week10completed\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week1completed\n",
      "week2completed\n",
      "week3completed\n",
      "week4completed\n",
      "week6completed\n",
      "week7completed\n",
      "week8completed\n",
      "week9completed\n",
      "week10completed\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "data  5  was not finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "data  5  was not finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n",
      "header was finished.\n",
      "spiliter was finished.\n",
      "dataset was finished.\n"
     ]
    }
   ],
   "source": [
    "### Writen by Zengwei Wang\n",
    "### date: Apr 10, 2019\n",
    "### WARNING: default .pdf data resources should be under 'data' directory\n",
    "\n",
    "from tika import parser\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "data_path = './data/'\n",
    "\n",
    "#reading .txt file by lines\n",
    "def preprocess(content):\n",
    "    #Page 1 of 15file:///Users/jas/srvr/apps/cs9315/18s2/lectures/week10/notes.html\\n\n",
    "    content = [ele for ele in content if not bool(re.match(r\"Page.*file://.*notes.html\\n\",ele))]\n",
    "    #4/10/18, 3)16 pmWeek 10 Lectures\\n\n",
    "    content = [ele for ele in content if not bool(re.match(r\".*[a-z]mWeek.*\",ele))]\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        if bool(re.match(r\".*[0-9]*/[0-9]*\\n.*\",content[i])):\n",
    "            head_page = content[:i]\n",
    "            header_stop = i\n",
    "            break\n",
    "    \n",
    "    #get the content of first page\n",
    "    header = [ele for ele in head_page if ele != '\\n']\n",
    "    header = {'topic':'header\\n','content':' '.join(head_page).strip('\\n ')+'\\n'}\n",
    "    print('header was finished.')\n",
    "    #print(header['content'])\n",
    "    dataset = [header]\n",
    "    \n",
    "    \n",
    "    #get the rest content of lecture note\n",
    "    spiliter = []\n",
    "    for i in range(len(content)):\n",
    "        if bool(re.match(r\".*[0-9]*/[0-9]*\\n.*\",content[i])):\n",
    "            spiliter.append(i)\n",
    "    print('spiliter was finished.')\n",
    "    for i in range(len(spiliter)):\n",
    "        try:\n",
    "            topic = content[spiliter[i]]\n",
    "            comment = ' '.join(content[spiliter[i]+1:spiliter[i+1]])\n",
    "            dataset.append({'topic': topic, 'content': comment})\n",
    "        except:\n",
    "            dataset.append({'topic': content[spiliter[i]], \\\n",
    "                            'content': ' '.join(content[spiliter[i]+1:])})\n",
    "    print('dataset was finished.')\n",
    "    return dataset\n",
    "\n",
    "def get_text_files():\n",
    "    #convert .pdf files to .txt files\n",
    "    for i in range(1,11):\n",
    "        try:\n",
    "            if i < 10:\n",
    "                week = 'week0'\n",
    "            else:\n",
    "                week = 'week'\n",
    "            if not os.path.exists('./data_text/'):\n",
    "                os.makedirs('./data_text/')\n",
    "            files_adr =  data_path + week + str(i)+'.pdf'\n",
    "            output_adr = './data_text/'+ week + str(i)+'.txt'\n",
    "\n",
    "            raw = parser.from_file(files_adr)\n",
    "            #print(raw['content'])\n",
    "            with open(output_adr,'w') as f:\n",
    "                f.write(raw['content'])\n",
    "            print('week'+ str(i)+'completed')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def get_json_files():\n",
    "    for i in range(1,11):\n",
    "        try:\n",
    "            if i < 10:\n",
    "                week = 'week0'\n",
    "            else:\n",
    "                week = 'week'\n",
    "            file_adr = './data_text/'+ week + str(i)+'.txt'\n",
    "\n",
    "            with open(file_adr,'r') as f:\n",
    "                content = f.readlines()\n",
    "            # preprocessing funtion\n",
    "            content = preprocess(content)\n",
    "            if not os.path.exists('./data_preprocessed/'):\n",
    "                os.makedirs('./data_preprocessed/')\n",
    "            file_adr = './data_preprocessed/'+ week + str(i)+'.json' \n",
    "\n",
    "            with open(file_adr, 'w') as f:\n",
    "                f.write('[\\n')\n",
    "                for tp in content:\n",
    "                    f.write(json.dumps(tp, indent=1))\n",
    "                    if tp is not content[-1]:\n",
    "                        f.write(',\\n')\n",
    "                    else:\n",
    "                        f.write('\\n')\n",
    "    #                 f.write(tp['topic'])\n",
    "    #                 f.write('='*30 + '\\n')\n",
    "    #                 f.write(tp['content'])\n",
    "    #                 f.write('='*31 + '\\n')\n",
    "                f.write(']\\n')\n",
    "\n",
    "        except:\n",
    "            print('data ', i ,' was not finished.')\n",
    "            \n",
    "def readable_text():\n",
    "    for i in range(1,11):\n",
    "        try:\n",
    "            if i < 10:\n",
    "                week = 'week0'\n",
    "            else:\n",
    "                week = 'week'\n",
    "            file_adr = './data_text/'+ week + str(i)+'.txt'\n",
    "\n",
    "            with open(file_adr,'r') as f:\n",
    "                content = f.readlines()\n",
    "            # preprocessing funtion\n",
    "            content = preprocess(content)\n",
    "            if not os.path.exists('./readable/'):\n",
    "                os.makedirs('./readable/')\n",
    "            file_adr = './readable/'+ week + str(i)+'.text' \n",
    "\n",
    "            with open(file_adr, 'w') as f:\n",
    "                for tp in content:\n",
    "                    f.write(tp['topic'])\n",
    "                    f.write('='*30 + '\\n')\n",
    "                    f.write(tp['content'])\n",
    "                    f.write('='*31 + '\\n')\n",
    "\n",
    "        except:\n",
    "            print('data ', i ,' was not finished.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #generate .txt files\n",
    "    get_text_files()\n",
    "    get_json_files()\n",
    "    readable_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['}\\n', ']\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('../messages.json','r') as f:\n",
    "    data = f.readlines()\n",
    "print(data[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"topic\": \"header\\n\",\n",
      "    \"content\": \"Week 10 Lectures\\n \\n \\n \\n \\n Week 10 Lectures\\n \\n Beyond RDBMSs\\n\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(tp, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
