<!DOCTYPE html><html><head><meta charset='utf-8'> <title>C:\Users\admin\Desktop\新建文件夹 (3)\CourseNote(1).html</title> </head><body><div><h3 >字段1</h3><p>COMP9315 Introduction


COMP9315 DBMS Implementation1/142



( Data structures and algorithms inside relational DBMSs )








Lecturer:   John Shepherd

Web Site:   http://www.cse.unsw.edu.au/~cs9315/

(If WebCMS unavailable, use http://www.cse.unsw.edu.au/~cs9315/18s2/)


Lecturer2/142


Name:John Shepherd
Office:K17-410 (turn right from lift)
Phone:9385 6494
Email:jas@cse.unsw.edu.au
Consults:Mon 2-3, Wed 11-12   (in K17-410)
Research:
Information Extraction/Integration 
Information Retrieval/Web Search 
e-Learning Technologies 
Multimedia Databases 
Query Processing




Course Admin3/142


Name:Jashank Jeremy
Email:cs9315@cse.unsw.edu.au 
(email goes to both Jashank and me)

Reasons:
Enrolment problems 
Special consideration 
Detailed assignment questions 
Technical issues




What this Course is NOT4/142

Official course title: Database Systems Implementation

More accurate: Implementation of Database Engines

This is a course about

 the internal workings of database management systems
 their data structures, algorithms, techniques 

It is not a course about

 how to be a database administrator, or
 how to build (advanced) database applications



Course Goals5/142

Introduce you to:

 the architecture of relational database management systems
 algorithms and data structures for data-intensive computing
 representation of relational database objects
 implementation of relational operators  (sel,proj,join)
 techniques for processing SQL queries
 techniques for managing concurrent transactions
 concepts in distributed and non-relational databases

Develop skills in:

 analysing the performance of data-intensive algorithms
 the use of C to implement data-intensive algorithms



... Course Goals6/142

A major course goal is to give you significant exposure to:

 the inner workings of a complete RDBMS (PostgreSQL)
 a large software system (and accompanying apparatus)

Concepts will also be illustrated via their PostgreSQL implementation.

PostgreSQL is a good vehicle for this purpose, because:

 open-source, unlike Oracle, SQL-server, etc.
 good-quality, consistent code base, unlike MySQL, etc.



... Course Goals7/142

At the end of this course you should understand:

 internal structure and functioning of relational DBMSs
 how SQL queries are translated/optimised/evaluated
 techniques for implementing transactions and reliable storage

At the end of this course you should be able to:

 analyse the cost/tradeoffs of relational operation implementations
 select appropriate tools to solve data-intensive computing problems
 administer (install/tune/manage) a PostgreSQL installation
 (maybe) contribute modifications to the PostgreSQL project
 (maybe) build a relational database management system "from scratch"



Pre-requisites8/142

We assume that you are already familiar with

 the C language and programming in C (or C++) 
	  (e.g. completed an intro programming course in C)
 developing applications on RDBMSs 
	  (SQL, relational algebra   e.g. an intro DB course)
 basic ideas about file organisation and file manipulation 
	  (e.g. Unix open, close, lseek, read, write, flock)
 sorting algorithms, data structures for searching 
	  (quick/merge sort, binary trees, hash tables   e.g. a data structures course)

If you don't know this material well, you will struggle to pass ...


... Pre-requisites9/142

How you might meet the pre-reqs ...





... via courses at CSE.


Syllabus Overview10/142


 Relational DBMS Architecture

 including details of the PostgreSQL architecture
	(case study)

 Storage Management

 disks, file organisation, buffer pool management

 Relation Alegbra Operations

 implementation of selection, projection, join, sort, ...

 Query Processing

 mapping SQL to query plans, query plan optimisation

 Transaction Processing

 concurrency, locking, crash recovery

 Parallel and Distributed Databases

 moving beyond one data store

 Object Data, Document Data, Graph Data

 beyond relations and tuples





Textbooks11/142

No official text book; several are suitable ...


 Garcia-Molina, Ullman, Widom 
    "Database Systems: The Complete Book"
 Ramakrishnan, Gehrke 
    "Database Systems Management"
 Silberschatz, Korth, Sudarshan 
    "Database System Concepts"
 Kifer, Bernstein, Lewis 
    "Database Systems: An algorithmic-oriented approach"
 Elmasri, Navathe 
    "Database Systems: Models, languages, design ..."

but not all cover all topics in detail


Teaching/Learning12/142

Stuff that's available for you:

 Textbooks: describe syllabus in lots of detail
 Notes: describe syllabus topics in some detail
 Lectures: summarise Notes and contain examples/exercises
 Lecture videos: for review  (or if you miss a lecture, or are in WEB stream)
 Readings: research papers on selected topics

The onus is on you to use all of this material.

Note: Lecture slides, exercises and videos will be available only after the lecture.


... Teaching/Learning13/142

Things that you need to do:

 Exercises: tutorial-like questions
 Prac work: lab-class-like exercises
 Assignments: large/important practical exercises
 On-line quizzes: for self-assessment

Dependencies:

 Exercises → Exam (theory part)
 Prac work → Assignments → Exam (prac part)



... Teaching/Learning14/142

Scheduled classes?

 there are no tute classes or lab classes
 lectures are thus more important than usual

What to do if you have problems understanding stuff?

 ask a question in/after the lecture
 come to a consultation   (M, T, W, F)
 ask about it on the MessageBoard   (under WebCMS)
 send me email   (essential for detailed assignment questions)

Debugging is most easily done in person.


... Teaching/Learning15/142

The course web site site is where you can:

 find out the latest course news/announcements
 collect the course notes, and view lecture slides/videos
 get all of the information about theory/prac exercises
 get your questions answered (via the MessageBoard)

URL: http://www.cse.unsw.edu.au/~cs9315/

If WebCMS is ever down, most material is accessible via:

http://www.cse.unsw.edu.au/~cs9315/18s2/index.php


Prac Work16/142

Prac Work requires you to compile PostgreSQL from source code

 instructions explain how to do this on Linux at CSE
 also works easily on Linux and Mac OSX at home
 PostgreSQL docs describe how to compile for Windows

Make sure you do the first Prac Exercise when it becomes available.

Sort out any problems ASAP (preferably at a consultation).

You can do prac work in groups, if you wish.


Assignments17/142

Schedule of assignment work:



Ass
Description
Due
Marks


1
Storage Management
Week 5
11%


2
Query Processing
Week 11
14%



Assignments will be carried out in pairs (see WebCMS)

Choose your own online tools to share code (e.g. git,DropBox).

Ultimately, submission is via CSE's give system.

Will spend some time in lectures reviewing assignments.

Assignments will require up-front code-reading (see Pracs).


... Assignments18/142

Don't leave assignments to the last minute; you can submit early.

As a "carrot", bonus marks are available for early submissions.

As a "stick", marks deducted (from max) for late submissions.






... Assignments19/142

More comments on assignments ...

You are responsible for managing your own time, and for committing
enough time to complete your work in this course.

"Work pressure" is not an acceptable excuse for late assignments.

Plagiarism will be checked for and punished.

Slacking off and letting your partner do the work is unhelpful.

The exam will contain questions related to the assignment work.


Quizzes20/142

Over the course of the semester ...

 six online quizzes
 taken in your own time (but there are deadlines)
 each quiz is worth a small number of marks

Quizzes are primarily a review tool to check progress.

But they contribute 10% of your overall mark for the course.


Exam21/142

There will be a three-hour exam in the November exam period.

Exam is held in CSE Labs, but is mainly a written (typed) Exam.

The Course Notes (only) will be available in the exam.

Things that we can't reasonably test in the exam:

 writing large C programs, running major experiments, drawing diagrams

Everything else is potentially examinable.

The exam will be a mixture of descriptive questions,
quantitative analysis, and small programming exercises.

The exam contributes 65% of the overall mark for this course.


Supplementary Assessment Policy22/142

Everyone gets exactly one chance to pass the Exam.

If you attend the Exam

 I assume that you are fit/healthy enough to take it
 you won't get a 2nd chance, even with a medical certificate

If you're sick just before or on the day of the Exam

 do not attend the Exam
 get documentation to support your claim



... Supplementary Assessment Policy23/142

All Special Consideration requests:

 must document how you were affected
 must be submitted to Student Central   (useful to email me as well)

Supplementary Exams are in mid-December (near Xmas)

 UNSW requires you to be available at that time, if needed
 if granted a Supp and don't attend, your exam mark = 0


Excuses like "I have already bought a plane ticket home" are not acceptable.



Passing this Course24/142

There is only one way to pass this course:

 learn the material

 by listening in lectures and reading the Notes
 by doing the exercises and prac work 

 perform in the assignments/exam

You are assessed based on your demonstrated competence.

Pleading for a pass on compassionate grounds won't work.


Assessment Summary25/142

Your final mark/grade will be computed according to the following:
ass1   = mark for assignment 1      (out of 10)
ass2   = mark for assignment 2      (out of 15)
quiz   = mark for on-line quizzes   (out of 15)
exam   = mark for final exam        (out of 60)
okExam = exam > 24/60           (after scaling)

mark   = ass1 + ass2 + quiz + exam
grade  = HD|DN|CR|PS,  if mark ≥ 50 && okExam
       = FL,           if mark < 50 && okExam
       = UF,           if !okExam



Reading Material26/142

All of these textbooks have relevant material (to varying depths):

 Elmasri, Navathe, 
	Fundamentals of Database Systems (6th ed),
	Addison-Wesley, 2010.
		  (General DB book, conventional approach)
 Garcia-Molina, Ullman, Widom, 
	Database Systems: The Complete Book (2nd ed),
	Prentice Hall, 2009.
		  (General DB book, slightly more formal view)
 Ramakrishan, Gehrke, 
	Database Management Systems (3rd ed),
	McGraw-Hill, 2002.
		  (General DB book, emphasises "systems" aspects)
 Silberschatz, Korth, Sudarshan, 
	Database System Concepts (6th ed),
	McGraw-Hill, 2010.
		  (General DB book, conventional approach)
 Kifer, Bernstein, Lewis, 
	Database Systems: Application-Oriented Approach (2nd ed)
	Addison-Wesley, 2006.
		  (General DB book, application and transaction focus)



... Reading Material27/142

Useful material can also be found in:

 Hellerstein, Stonebraker, Hamilton,
	Architecture of a Database System,
	Foundations and Trends in Database Systems, 141-259, 2007.
	(Overview of modern DBMS architectures)
 Graefe,
	Query Evaluation Techniques for Large Databases,
	ACM Computing Surveys, 25(2), 73-170, 1993.
	(Overview of advanced query processing)
 Gaede and Gunther,
	Multidimensional Access Methods,
	ACM Computing Surveys, 30(2), 170-231, 1998.
	(Overview of indexing techniques)

These (and others) are available via the course web site.


PostgreSQL28/142

In this course, we will be using PostgreSQL v10.4   (compulsory)

PostgreSQL tarball is available for copying from the course web site. 
(Don't waste your IP quota downloading it from www.postgresql.org)

Install/modify your own PostgreSQL server at CSE (from source code).

(This is not the same setup as for COMP3311; COMP3311 used a shared binary)

Working on a home PC is simple if you run Linux or Mac OSX.

Alternatively, use putty to connect to CSE from home.

However, you can do it all on Windows (see Chap.16 PostgreSQL manual)


... PostgreSQL29/142

PostgreSQL is a large software system:

 > 1000 source code files in the core engine/clients
 > 500,000 lines of C code in the core

You won't be required to understand all of it :-)

You will need to learn to navigate this code effectively.

Will discuss relevant parts in lectures to help with this.


... PostgreSQL30/142

Some comments on PostgreSQL books:

 they tend to be expensive and short-lived
 many just provide the manual, plus a bit extra
 generally, anything published by O'Reilly is useful
 does the book describe the right version?
	(PostgreSQL 10)

There's no need to buy a PostgreSQL reference book ...


Relational Database Revision



Relational Databases32/142

Relational databases build on relational theory

 all data is modelled as relations (tables) and tuples
 constraints define consistency of data
 normalisation theory validates data designs
 relational algebra describes manipulation of data

Relational theory provides the foundation, plus ...

 40 years of technology development
 standardised declarative language (SQL)

leading to modern relational DBMSs.


... Relational Databases33/142

We begin by looking at relational databases top-down

 starting from SQL
 through relational operations
 to file structures and storage devices

For the remainder of the semster, we work bottom-up.


Database Management Systems34/142

Relational DBMSs provide critical infrastructure for modern computing

 enterprises commit mission-critical data to DBMSs (e.g. CBA)
 many major Web-sites are based on them (e.g. Wikipedia)
 even embedded in other software (e.g. web browsers)

Nowadays, some very large web sites are developing their
own large-scale distributed solutions

 Google (GFS), Amazon (SimpleDB),  ...

But even they run relational DBMSs in their "back-office".


DBMS History35/142


1960sFiles, Hierachical and network databases
1970Relational data model (Ted Codd)
1975First RDBMS and SQL (IBM Almaden)
1979First version of Oracle
1980sRefinement of technology, distributed systems, new data types (objects, Prolog)
1990sObject-relational DBMSs, OLAP, data mining, data warehousing, multimedia data, SQL standards
2000sDatabases for XML, bioinformatics, telecomms, SQL3
2000salso, very-large, distributed, relaxed-consistency storage



DBMS Functionality36/142

DBMSs provide a variety of functionalities:

 storing/modifying data and meta-data ~>(data defintions)
 constraint definition/storage/maintenance/checking
 declarative manipulation of data (via SQL)
 extensibility via views, triggers, procedures
 query re-writing (rules), optimisation (indexes)
 transaction processing, concurrency/recovery
 etc. etc. etc.

Common feature of all relational DBMSs: relational model, SQL.


DBMS for Data Definition37/142

Critical function of DBMS: defining relational data   (DDL sub-language)

Relational data: relations/tables, tuples, values, types, constraints.

E.g.
create domain WAMvalue float
   check (value between 0.0 and 100.0);

create table Students (
   id          integer,  -- e.g. 3123456
   familyName  text,     -- e.g. 'Smith'
   givenName   text,     -- e.g. 'John'
   birthDate   date,     -- e.g. '1-Mar-1984'
   wam         WAMvalue, -- e.g. 85.4
   primary key (id)
);

Executing the above adds meta-data to the database.

DBMSs typically store meta-data as special tables (catalog).


... DBMS for Data Definition38/142

Input: DDL statements





Result: meta-data in catalog is modified


... DBMS for Data Definition39/142

Specifying constraints is an important aspect
of data definition:

 attribute (column) constraints
 tuple constraints
 relation (table) constraints
 referential integrity constraints

Examples:
create table Employee (
   id      integer primary key,
   name    varchar(40),
   salary  real,
   age     integer check (age > 15),
   worksIn integer
              references Department(id),
   constraint PayOk check (salary > age*1000)
);



DBMS for Data Modification40/142

Critical function of DBMS: manipulating data   (DML sub-language)

 insert new tuples into tables
 delete existing tuples from tables
 update values within existing tuples

E.g.
insert into Enrolments(student,course,mark)
values (3312345, 5542, 75);

update Enrolments set mark = 77
where  student = 3354321 and course = 5542;

delete Enrolments where student = 331122333;



... DBMS for Data Modification41/142

Input: DML statements





Result: tuples are added, removed or modified


... DBMS for Data Modification42/142

Most DBMSs also provide bulk download/upload mechanisms:

 dump gives text copy of data/schema
 load reads data/schema info into DB

For PostgreSQL:

 pg_dump application dumps database
 copy SQL command loads entire tables



DBMS as Query Evaluator43/142

Most common function of relational DBMSs

 read an SQL query
 return a table giving result of query

E.g.
select s.id, c.code, e.mark
from   Students s, Courses c, Enrolments e
where  s.id = e.student and e.course = c.id;



... DBMS as Query Evaluator44/142

Input: SQL query





Output: table (displayed as text)


DBMS Architecture45/142

The aim of this course is to

 look inside the DBMS box
 discover the various mechanisms it uses
 understand and analyse their performance

Why should we care? (apart from passing the exam)

Practical reason:

 if we understand how query processor works 
	⇒ we can do a better job of writing efficient queries

Educational reason:

 DBMSs contain interesting data structures + algorithms
 these may be useful outside the (relational) DBMS context



... DBMS Architecture46/142

Fundamental tenets of DBMS architecture:

 data is stored permanently on large slow devices
 data is processed in small fast memory

Implications:

 data structures should minimise storage utilisation
 algorithms should minimise memory/disk data transfers

In the past, DBMSs attempted to solve this by completely
controlling disks themselves.

Modern DBMSs interact with storage via the O/S file-system.


... DBMS Architecture47/142

Implementation of DBMS operations is complicated by

 potentially multiple concurrent accesses to data structures 
	(not just data tables, but indexes, buffers, catalogues, ...)
 transactional requirements (atomicity, rollback, ...)
 requirement for high reliability of raw data (recovery)

Locking helps with concurrency, but may degrade performance.

In practice, may need new "concurrency-tolerant" data structures.

Transactions/reliability require some form of logging.


... DBMS Architecture48/142

Path of a query through a typical DBMS:





... DBMS Architecture49/142

Accoring to Silberschatz/Korth/Sudarshan (SKS) ...






... DBMS Architecture50/142

Accoring to Elmasri/Navathe (EN) ...






... DBMS Architecture51/142

According to Ramakrishnan/Gerhke (RG) ...






... DBMS Architecture52/142



Query optimiser
translates queries into efficient sequence of relational ops


Query executor
controls execution of sequence of relational ops


Access methods
basis for implementation of relational operations


Buffer manager
manages data transfer between disk and main memory


Storage manager
manages allocation of disk space and data structures


Concurrency manager
controls concurrent access to database


Recovery manager
ensures consistent database state after system failures


Integrity manager
verifies integrity constraints and user privileges




Database Engine Operations53/142

DB engine = "relational algebra virtual machine":




 selection (σ) 
 projection (π) 
 join (⋈) 


 union (∪) 
 intersection (∩) 
 difference (-) 


 sort 
 group 
 aggregate 




For each of these operations:

 various data structures and algorithms are available
 DBMSs may provide only one, or may provide a choice



... Database Engine Operations54/142

Different implementations of Selection:

 a hash-structured file is good for queries like:
select * from Students where id = 3312345;

where id is the hashing attribute
 a B-tree file is good for queries like:
select * from Employees where age > 55;




Relational Algebra55/142

Relational algebra (RA) can be viewed as ...

 mathematical system for manipulating relations, or
 data manipulation language (DML) for the relational model

Relational algebra consists of:

 operands: relations, or variables representing relations
 operators that map relations to relations
 rules for combining operands/operators into expressions
 rules for evaluating such expressions

RA can be viewed as the "machine language" for RDBMSs


... Relational Algebra56/142

Select, project, join provide a powerful set of operations for
constructing relations and extracting relevant data from them.





Adding set operations and renaming makes RA complete.


Notation57/142

Standard treatments of relational algebra use Greek symbols.

We use the following notation (because it is easier to reproduce):


  Operation
  StandardNotation
  OurNotation


  Selection
  σexpr(Rel)
  Sel[expr](Rel)


  Projection
  πA,B,C(Rel)
  Proj[A,B,C](Rel)


  Join
  Rel1 ⋈expr Rel2
  Rel1  Join[expr]  Rel2 


  Rename
  schemaRel
  Rename[schema](Rel) 



For other operations (e.g. set operations) we adopt the standard notation.



... Notation58/142

We define the semantics of RA operations using

 regular "conditional set" expressions   e.g. { x | condition }
 tuple notations:

 t[ab]   (extracts attributes a and b from tuple t)
 (x,y,z)   (enumerated tuples; specify attribute values)

 quantifiers, set operations, boolean operators



... Notation59/142

All RA operators return a result relation (no DB updates).

For convenience, we can name a result and use it later.

E.g.
Temp = R op1 S op2 T
Res  = Temp op3 Z
-- which is equivalent to
Res  = (R op1 S op2 T) op3 Z


Each "intermediate result" has a well-defined schema.



Sample Relations60/142

Example database #1 to demonstrate RA operators:





... Sample Relations61/142

Example database #2 to demonstrate RA operators:





Selection62/142

Selection returns a subset of the tuples
in a relation r that satisfy a specified condition C.


σC(r)   =   Sel[C](r)   =   { t  |  t ∈ r ∧ C(t) },     where r(R)


C is a boolean expression on attributes in R.

Result size:   |σC(r)| ≤ |r|

Result schema:   same as the schema of r   (i.e. R)

Computational view:
result = {}
for each tuple t in relation r
    if (C(t)) { result = result ∪ {t} }



... Selection63/142

Example selections:





... Selection64/142

Example queries:

 Find details about the Perryridge branch?

 Sel [branchName=Perryridge] (Branch)

 Which accounts are overdrawn?

 Sel [balance<0] (Account)

 Which Round Hill accounts are overdrawn?

 Sel [branchName=Round Hill ∧ balance<0] (Account)




Projection65/142

Projection returns a set of tuples containing
a subset of the attributes in the original relation.


πX(r)   =   Proj[X](r)   =   { t[X]  |  t ∈ r },     where r(R)


X specifies a subset of the attributes of R.

Note that removing key attributes can produce duplicates.

In RA, duplicates are removed from the result set.

(In many RDBMS's, duplicates are retained   (i.e. they use bag, not set, semantics))

Result size:   |πX(r)| ≤ |r|
   
Result schema:   R'(X)

Computational view:
result = {}
for each tuple t in relation r
    result = result ∪ {t[X]}



... Projection66/142

Example projections:





... Projection67/142

Example queries:

 What branches are there?

 Proj [branchName] (Branch)

 Which branches actually hold accounts?

 Proj [branchName] (Account)

 What are the names and addresses of all customers?

 Proj [name,address] (Customer)

 Generate a list of all the account numbers

 Proj [accountNo] (Account)    or
 Proj [account] (Depositor)    (if we assume every account has a depositor)




Union68/142

Union combines two compatible relations into
a single relation via set union of sets of tuples.


r1 ∪ r2   =   { t  |  t ∈ r1 ∨ t ∈ r2 },     where r1(R), r2(R)


Compatibility = both relations have the same schema

Result size:   |r1 ∪ r2|   ≤  
|r1| + |r2|
   
Result schema: R

Computational view:
result = r1
for each tuple t in relation r2
    result = result ∪ {t}



... Union69/142

Example queries:

 Which suburbs have either customers or branches?

 Proj[address](Customer) ∪ Proj[address](Branch)

 Which branches have either customers or accounts?

 Proj[homeBranch](Customer) ∪ Proj[branchName](Account)


The union operator is symmetric i.e.   R ∪ S   =   S ∪ R.


Intersection70/142

Intersection combines two compatible
relations into a single relation via set intersection of sets of tuples.


r1 ∩ r2   =   { t  |  t ∈ r1 ∧ t ∈ r2 },     where r1(R), r2(R)


Uses same notion of relation compatibility as union.

Result size:   |r1 ∪ r2|   ≤  
min(|r1|,|r2|)
   
Result schema: R

Computational view:
result = {}
for each tuple t in relation r1
    if (t ∈ r2) { result = result ∪ {t} }



... Intersection71/142

Example queries:

 Which suburbs have both customers and branches?

 Proj[address](Customer) ∩ Proj[address](Branch)

 Which branches have both customers and accounts?

 Proj[homeBranch](Customer) ∩ Proj[branchName](Account)


The intersection operator is symmetric i.e.   R ∩ S   =   S ∩ R.


Difference72/142

Difference finds the set of tuples that exist in
one relation but do not occur in a second compatible relation.


r1 - r2   =   { t  |  t ∈ r1 ∧ ¬ t ∈ r2 },     where r1(R), r2(R)


Uses same notion of relation compatibility as union.

Note:
tuples in r2 but not r1 do not appear in the result

i.e. set difference != complement of set intersection

Computational view:
result = {}
for each tuple t in relation r1
    if (!(t ∈ r2)) { result = result ∪ {t} }



... Difference73/142

Example difference:



s1 = Sel [B = 1] (r1) 
s2 = Sel [C = x] (r1) 
s1 - s2 
s2 - s1 


... Difference74/142

Example queries:

 Which customers have no accounts?

 AllCusts = Proj[customerNo](Customer)
CustsWithAccts = Proj[customer](Depositor)
Result = AllCusts - CustsWithAccts

 Which branches have no customers?

 AllBranches = Proj[branchName](Branch)
BranchesWithCusts = Proj[homeBranch](Customer)
Result = AllBranches - BranchesWithCusts




Natural Join75/142

Natural join is a specialised product:

 containing only pairs that match on their common attributes
 with one of each pair of common attributes eliminated

Consider relation schemas R(ABC..JKLM),  S(KLMN..XYZ).

The natural join of relations r(R) and s(S) is defined as:


r ⋈ s   =   r Join s   =   
	{ (t1[ABC..J] : t2[K..XYZ])  | 
	  t1 ∈ r ∧ t2 ∈ s ∧ match
	}

where    match   =  
		t1[K] = t2[K] ∧
		t1[L] = t2[L] ∧
		t1[M] = t2[M]


Computational view:
result = {}
for each tuple t1 in relation r
   for each tuple t2 in relation s
      if (matches(t1,t2))
         result = result ∪ {combine(t1,t2)}



... Natural Join76/142


Natural join can also be defined in terms of other relational algebra operations:


r Join s   =    Proj[R ∪ S] ( Sel[match] ( r × s) )


We assume that the union on attributes eliminates duplicates.

If we wish to join relations, where the common attributes have
different names, we rename the attributes first.

E.g. R(ABC) and S(DEF) can be joined by


R Join Rename[S(DCF)](S)


Note: |r ⋈ s| ≪ |r × s|, so join not implemented via product.


... Natural Join77/142

Example natural join:





... Natural Join78/142

Example queries:

 Who is the owner of account A101?

 Proj[name](Sel[account=A101](Customer ⋈ Depositor))

 Which accounts are held in branches in Horseneck?

 tmp1 = Sel[address=Horseneck](Account ⋈ Branch) 
	res   = Proj[accountNo](tmp1))

 Which customers hold accounts at a Brooklyn branch?

	tmp1 = Account ⋈ Branch ⋈ Customer ⋈ Depositor 
	res   = Proj[name](Sel[address=Brooklyn](tmp1))




Theta Join79/142

The theta join is a specialised product
containing only pairs that match on a supplied condition C.


r ⋈C s   =   { (t1 : t2)  |  t1 ∈ r ∧ t2 ∈ s ∧ C(t1 : t2) }, 
where r(R),s(S)


Examples:   (r1 Join[B>E] r2) ... (r1 Join[E<D ∧ C=G] r2)

Can be defined in terms of other RA operations:


r ⋈C s   =   r Join[C] s   =    Sel[C] ( r × s )


Unlike natural join, "duplicate" attributes are not removed.

Note that   r ⋈true s   =   r × s.


... Theta Join80/142

Example theta join:





... Theta Join81/142

Comparison between join operations:

 theta join allows arbitrary tests in the condition 
	(and leaves all attributes from the original relations in the result)
 equijoin has only equality tests in the condition 
	(and leaves all attributes from the original relations in the result)
 natural join has only equality tests on common attributes 
	(and removes one of each pair of matching attributes)


Equijoin is a specialised theta join; natural join is like theta join followed by projection.



Outer Join82/142

r Join s eliminates all s tuples that do not match some r tuple.

Sometimes, we wish to keep this information, so outer join

 includes all tuples from each relation in the result
 for pairs of matching tuples, concatenate attributes as for standard join
 for tuples that have no match, assign null to "unmatched" attributes



... Outer Join83/142

Example outer join:




Contrast this to the result for theta-join presented earlier.


... Outer Join84/142

There are three variations of outer join R OuterJoin S:

 left outer join (LeftOuterJoin) includes all tuples from R
 right outer join (RightOuterJoin) includes all tuples from S
 full outer join (OuterJoin) includes all tuples from R and S

Which one to use depends on the application e.g.

If we want to know about all Branches, regardless of whether they
have Customers as their homeBranch:


Branches LeftOuterJoin[branchName=homeBranch] Customer



... Outer Join85/142

Computational description of r(R) LeftOuterJoin s(S):
result = {}
for each tuple t1 in relation r
   nmatches = 0
   for each tuple t2 in relation s
      if (matches(t1,t2))
         result = result ∪ {combine(t1,t2)}
         nmatches++
   if (nmatches == 0)
      result = result ∪
                 {combine(t1,Snull)}

where Snull is a tuple from S with all atributes set to NULL.


Aggregation86/142

Two types of aggregation are common in database queries:

 accumulating summary values for data in tables

 typical operations Sum, Average, Count
 many operations work on a single column 
	(e.g. Sum[assets](Branch))

 grouping sets of tuples with common values 

 GroupBy[A1...An](R)
 typically we group using only a single attribute




Generalised Projection87/142

In standard projection, we select values of specified attributes.

In generalised projection we perform some computation on the
attribute value before placing it in the result tuple.

Examples:

 Display branch assets in Aus$ rather than US$.

 Proj [branchname,address,assets*0.75] (Branch)

 Display employee records using age rather than birthday.

 Proj [id,name,(today-birthdate)/365,salary] (Employee)




PostgreSQL



PostgreSQL89/142

PostgreSQL is a full-featured open-source (O)RDBMS.

 provides a relational engine with:

 efficient implementation of relational operations
 very good transaction processing (concurrent access)
 good backup/recovery (from application/system failure)
 novel query optimisation (genetic algorithm-based)
 replication, JSON, extensible indexing, etc. etc.

 already supports several non-standard data types
 allows users to define their own data types
 supports most of the SQL3 standard



Brief History of PostgreSQL90/142



1977-1985

Ingres (Stonebraker) 
research prototype 
 → Relational Technologies 
 → bought by Computer Associates



1986-1994

Postgres (Stonebraker) 
research prototype 
 → Illustra → bought by Informix



1994-1995

Postgres95 (Chen, Yu) 
added SQL, spawned PostgreSQL



1996-...
PostgreSQL (Momjian,Lane,...) 

	open-source DBMS with Oracle-level functionality,
	platform for experiments with new DBMS implementation ideas






PostgreSQL Online91/142

Web site:
www.postgresql.org

Key developers: Bruce Momjian, Tom Lane, Marc Fournier, ...

Full list of developers:
www.postgresql.org/developer/bios

Local copy of source code:

/home/cs9315/web/18s2/postgresql/src.tar.bz2

Documentation is available via WebCMS menu.


User View of PostgreSQL92/142

Users interact via SQL in a client process, e.g.

$ psql webcms
psql (10.4)
Type "help" for help.
webcms2=# select * from calendar;
 id | course |   evdate   |      event
----+--------+------------+---------------------------
  1 |      4 | 2001-08-09 | Project Proposals due
 10 |      3 | 2001-08-01 | Tute/Lab Enrolments Close
 12 |      3 | 2001-09-07 | Assignment #1 Due (10pm)
 ...

or

$dbconn = pg_connect("dbname=webcms");
$result = pg_query($dbconn,"select * from calendar");
while ($tuple = pg_fetch_array($result))
   { ... $tuple["event"] ... }



PostgreSQL Functionality93/142

PostgreSQL systems deal with various kinds of entities:

 users ... who can use the system, what they can do
 groups ... groups of users, for role-based privileges
 databases ... collections of schemas/tables/views/...
 namespaces ... to uniquely identify objects (schema.table.attr)
 tables ... collection of tuples (standard relational notion)
 views ... "virtual" tables (can be made updatable)
 functions ... operations on values from/in tables
 triggers ... operations invoked in response to events
 operators ... functions with infix syntax
 aggregates ... operations over whole table columns
 types ... user-defined data types (with own operations)
 rules ... for query rewriting (used e.g. to implement views)
 access methods ... efficient access to tuples in tables



... PostgreSQL Functionality94/142

PostgreSQL's dialect of SQL is mostly standard
	(but with extensions).

Differences visible at the user-level:

 attributes containing arrays of atomic values
 table type inheritance, table-valued functions, ...

Differences at the implementation level:

 referential integrity checking is accomplished via triggers
 views are implemented via query re-writing rules

Example:

create view myview as select * from mytab;
-- is implemented as
create type as myview (same fields as mytab);
create rule myview as on select to myview
            do instead select * from mytab;



... PostgreSQL Functionality95/142

PostgreSQL stored procedures differ from SQL standard:

 only provides functions, not procedures
	(but functions can return void)
 allows function overloading 
	(same function name, diff argument types)
 defined at different "lexical level" to SQL
 provides own PL/SQL-like language for functions

Example:

create or replace function
    barsIn(suburb text) returns setof Bars
as $$ 
declare
    r record;
begin
    for r in
        select * from Bars where location = suburb
    loop
       return next r;
    end loop;
end;
$$ language plpgsql;
used as e.g.
select * from barsIn('Randwick');



... PostgreSQL Functionality96/142

Concurrency is handled via multi-version concurrency control (MVCC)

 multiple "versions" of the database exist together
 a transaction sees the version that was valid at its start-time
 readers don't block writers; writers don't block readers
 this significantly reduces the need for locking

Disadvantages of this approach:

 extra storage for old versions of tuples   (vacuum fixes this)



... PostgreSQL Functionality97/142

Allows transactions to specify a consistency level for concurrency

 read-committed (allows some inconsistency),
	serializable (no inconsistency)
 default isolation level is read-committed
	  (⇒ potential portability issues)

Explicit locking is also available.

 different varieties: share/exclusive, row/table
 deadlock detection via time-out

Access methods need to implement their own concurrency control.


... PostgreSQL Functionality98/142

PostgreSQL has a well-defined and open extensibility model:

 stored procedures are held in database as strings


 allows a variety of languages to be used
 language interpreters can be integrated into PostgreSQL engine


 new data types, operators, aggregates, indexes can be added


 typically requires code written in C, following defined API
 for new data types, need to write input/output functions, ...
 for new indexes, need to implement file structures





... PostgreSQL Functionality99/142

Because of its extensibility, PostgreSQL has extra data types:

 built-in:   geometric (line,point,...), network address (macaddr,...)
 contributed:   complex number, ISBN/ISSN, encrypted password, ...

Also has a wider-than-usual range of access methods:

 B-trees, linear hashing, R-trees, GiST/GIN indexes
 full-text indexing

And provides a range of replication services.


Installing PostgreSQL100/142

PostgreSQL is available via the COMP9315 web site.

Provided as tarball and zip in ~cs9315/web/18s2/postgresql/

Brief summary of installation:
$ configure --prefix=~/your/pgsql/directory
$ make
$ make install
$ source ~/your/environment/file
   # set up environment variables
$ initdb
   # set up postgresql configuration
$ pg_ctl start
   # do some work with PostgreSQL databases
$ pg_ctl stop



... Installing PostgreSQL101/142

Simplified version of running the server:
$ source ~/your/environment/file
   # set up environment variables
$ pgs setup
   # runs initdb and fixes configuration
$ pgs start
   # do some work with PostgreSQL databases
$ pgs stop
   # stops server
$ pgs cleanup
   # removes all data files

pgs is a shell script in /home/cs9315/bin


PostgreSQL Configuration102/142

PostgreSQL configuration parameters (some important ones):

 PGHOME = directory where PostgreSQL resides 
	Same as value given in configure --prefix=$PGHOME 
	(typical value: /usr/local/pgsql)
 PGBIN  = directory where client applications reside 
	(typical value: $PGHOME/bin)
 PGDATA = directory where data files reside 
	(typical value: $PGHOME/data)
 PGHOST = host where server is running (if using TCP/IP)
 PGPORT = port address for server   (default: 5432)

Note: if not using TCP/IP, PGHOST holds name of directory
where Unix socket files reside.


... PostgreSQL Configuration103/142

A typical envrionment setup for COMP9315:
# Set up environment for running PostgreSQL
# Must be "source"d from sh, bash, ksh, ...

PGHOME=/home/jas/srvr/pgsql
export PGDATA=$PGHOME/data
export PGHOST=$PGDATA
export PGPORT=5432
export PATH=$PGHOME/bin:$PATH
export PGDATA PGHOST PATH

alias p0="$D/bin/pg_ctl stop"
alias p1="$D/bin/pg_ctl -l $PGDATA/log start"



... PostgreSQL Configuration104/142

Other configuration files live in $PGDATA.

postgresql.conf: server configuration

 must change unix_socket_directory to match $PGHOST
 may change max_connections to e.g. 10
 assignments may require other changes
 changes typically need server re-start to take effect

pg_hba.conf: authorisation/user access

 which users can access which database from which hosts
 ignore, since we do everything as PostgreSQL super-user



Using PostgreSQL for Assignments105/142

You will need to modify then re-start the server:
# edit source code to make changes
$ pg_ctl stop
$ make
$ make install
# restore postgresql configuration
$ pg_ctl start
# run tests and analyse results

Assumes no changes that affect storage structures.

I.e. existing databases will continue to work ok.


... Using PostgreSQL for Assignments106/142

If you change storage structures ...

 old database will not work with the new server
 need to dump, re-run initdb, then restore

# edit source code to make changes
$ pg_dump testdb > testdb.dump
$ make
$ pg_ctl stop
$ rm -fr /your/pgsql/directory/data
$ make install
$ initdb
# restore postgresql configuration
$ pg_ctl start
$ createdb testdb
$ psql testdb -f testdb.dump
# run tests and analyse results

Need to save a copy of postgresql.conf before re-installing.


PostgreSQL Architecture107/142

Client/server architecture:






Note: nowadays the postmaster process is also called postgres.



... PostgreSQL Architecture108/142

Notes:

 exactly one postmaster; many clients; many servers
 each client has its own server process
 client/server communication via TCP/IP or Unix sockets
 uses PostgreSQL-specific frontend/backend protocol
 client/server separation good for security/reliability
 client/server connection overhead is significant 
	(generally solved by client-side pooling of persistent conenctions)



... PostgreSQL Architecture109/142

Memory/storage architecture:






... PostgreSQL Architecture110/142

Notes:

 all servers access database files via buffer pool 
	(thus, all servers get a consistent view of data ... essential!)
 Unix kernel provides additional buffering (useful?)
 use of shared memory limits distribution/scalability 
	(all server processes must run on the same machine)
 shared tables are "global" system catalog tables 
	(hold user/group/database info for entire PostgreSQL installation)



... PostgreSQL Architecture111/142

File-system architecture:






... PostgreSQL Architecture112/142

Interesting files in $PGDATA:


PG_VERSION
which server version made this directory 


pg_hba.conf
who can access which databases from where


postgresql.conf
server parameters   (e.g. max connections)


postmaster.opts
how was current postmaster invoked


postmaster.pid
process id of current postmaster




PostgreSQL Source Code



PostgreSQL Source Code114/142

Top-level of PostgreSQL distribution contains:


 README,INSTALL:   overview and installation instructions
 config*:   scripts to build localised Makefiles
 Makefile:   top-level script to control system build
 src:   sub-directories containing system source code
 doc:   FAQs and documentation (in various formats)
 contrib:   source code for contributed extensions



... PostgreSQL Source Code115/142

How to get started understanding the workings of PostgreSQL:

 become familiar with the user-level interface
	 (psql, pg_dump, pg_ctl, etc.)
 start with the *.h files, then move to *.c files 
	(note that: *.c files live under src/backend/*, *.h files live under src/include)
 start globally, then work one subsystem-at-a-time

Some helpful information is available via:

 PostgreSQL link on web site
 Readings link on web site (but old docs)



... PostgreSQL Source Code116/142

The source code directory (src) contains:

 include:   *.h files with global definitions (constants, types, ...)
 backend:   code for PostgreSQL database engine (server)
 bin:   code for clients (e.g. psql, pg_ctl, pg_dump, ...)
 pl:   stored procedure language interpreters (e.g. plpgsql)
 interfaces   code for low-level C interfaces (e.g. libpq)


along with Makefiles to build system and other directories not relevant for us

Code for backend (DBMS engine)

 1300 files (800.c,500.h,4.y,5.l),   106 lines of code



... PostgreSQL Source Code117/142

We introduce the code

 by following the execution of a query
 tracing which functions are involved
 pointing out the files containing these functions

PostgreSQL manual has detailed description of internals:

 Section VII, Chapters 46,47,53-57
 Ch.46 is an overview; a good place to start
 other chapters discuss specific components

See also "How PostgreSQL Processes a Query"

 src/tools/backend/index.html



Life-cycle of a PostgreSQL query118/142

How a PostgreSQL query is executed:

 SQL query string is produced in client
 client establishes connection to PostgreSQL
 dedicated server process attached to client
 SQL query string sent to server process
 server parses/plans/optimises query
 server executes query to produce result tuples
 tuples are transmitted back to client
 client disconnects from server



... Life-cycle of a PostgreSQL query119/142






PostgreSQL server120/142

PostgresMain(int argc, char *argv[], ...)

 defined in src/backend/tcop/postgres.c
 PostgreSQL server (postgres) main loop
 performs much setting up/initialisation
 reads and executes requests from client
 using the frontend/backend protocol (Ch.46)
 on Q request, evaluates supplied query
 on X request, exits the server process



... PostgreSQL server121/142

As well as handling SQL queries, PostgresqlMain also

 handles "utility" commands e.g. CREATE TABLE

 most utility commands modify catalog   (e.g. CREATE X)
 other commands affect server   (e.g. vacuum)

 handles COPY command

 special COPY mode; context is one table
 reads line-by-line, treats each line as tuple
 inserts tuples into table; at end, checks constraints




... PostgreSQL server122/142

exec_simple_query(const char *query_string)

 defined in src/backend/tcop/postgres.c
 entry point for evaluating SQL queries
 assumes query_string is one or more SQL statements
 performs much setting up/initialisation
 parses the SQL string (into one or more parse trees)
 for each parsed query ...

 perform any rule-based rewriting
 produces an evaluation plan (optimisation)
 execute the plan, sending tuples to client




... PostgreSQL server123/142

Functions involved in exec_simple_query(...)

parsetree_list = pg_parse_query(query_string);
foreach(parsetree, parsetree_list) {
  querytree_list = pg_analyze_and_rewrite(parsetree, ...);
  plantree_list = pg_plan_queries(querytree_list, ...);
  portal = CreatePortal(...); // query execution env
  PortalDefineQuery(portal, ..., plantree_list, ...);
  receiver = CreateDestReceiver(dest); // client
  PortalRun(portal, ..., receiver, ...);
  ...
}

(For the time being, to simplify description, many details omitted)


PostgreSQL Data Types124/142

Data types defined in *.h files under src/include/

Two important data types: Node and List

 Node provides generic structure for nodes

 defined in src/include/nodes/nodes.h
 specific node types defined in src/include/nodes/*.h
 functions on nodes defined in src/backend/nodes/*.c

 List provides generic singly-linked list

 defined in src/include/nodes/pg_list.h
 functions on lists defined in src/backend/nodes/list.c




... PostgreSQL Data Types125/142

Generic Node = type tag + specific node struct

Node structures are defined for

 parse trees, plan trees, execution trees, ...

Other structures defined in PostgreSQL include:

 data objects: tuples, indexes, relations
 storage: buffers, pages, shared memory, files
 catalog, replication, locks, transactions, ...

See under src/include/ for full details.


Query Parser126/142

pg_parse_query(char *sqlStatements)

 defined in src/backend/tcop/postgres.c
 pg_parse_query() invokes raw_parser(char *str)

 defined in src/parser/parser.c

 raw_parser() invokes base_yyparse()

 defined in src/parser/gram.y

 parser written using bison and flex
 returns list of parse trees, one for each SQL statement



... Query Parser127/142






Query Rewriter128/142

pg_analyze_and_rewrite(Node *parsetree, ...)

 defined in src/backend/tcop/postgres.c
 converts parsed queries into form suitable for planning
 input is a parse tree (output of raw_parser())
 returns a list of Query nodes
 takes place in two stages

 parse_analyze() maps parse tree to Query

 defined in src/backend/parser/analyze.c

 pg_rewrite_query() applies rules (incl. expands views)

 defined in src/backed/tcop/postgres.c





... Query Rewriter129/142

Raw parse tree from parser is a syntax tree.

parse_analyze() performs semantic analysis

 performs type checking
 checks that tables and columns exist (catalog look-up)
 resolves column references 
(which table does each unqualified column name belong to)

Checking yields data about objects used in query.

Object data is added to parse tree data to make Query


... Query Rewriter130/142

Each query is represented by a Query structure

 defined in src/include/nodes/parsenodes.h
 holds all components of the SQL query, including

 required columns as list of TargetEntrys
 referenced tables as list of RangeTblEntrys
 where clause as node in FromExpr struct
 sorting requirements as list of SortGroupClauses

 queries may be nested, so forms a tree structure



... Query Rewriter131/142






Query Planner132/142

pg_plan_queries(querytree_list, ...)

 defined in src/backend/tcop/postgres.c
 converts analyzed queries into executable statements
 input is a list of Query nodes (from rewriter)
 returns a list of PlannedStmt nodes, one for each Query
 uses pg_plan_query() to plan each Query

 defined in src/backend/tcop/postgres.c

 which uses planner() to actually do the planning

 defined in optimizer/plan/planner.c




... Query Planner133/142

Each executable query is represented by a PlannedStmt node

 defined in src/include/nodes/plannodes.h
 contains information for exection of query, e.g.

 which relations are involved, targets for select into, etc.

 most important component is a tree of Plan nodes

Each Plan node represents one relational operation

 types: SeqScan, IndexScan, HashJoin, Sort, ...
 each Plan node also contains cost estimates for operation



... Query Planner134/142






... Query Planner135/142

PlannedStmt *planner(Query *parse, ...)

 defined in optimizer/plan/planner.c
 subquery_planner() performs standard transformations

 e.g. push selection and projection down the tree

 then invokes a cost-based optimiser:

 choose possible plan (execution order for operations)
 choose physical operations for this plan
 estimate cost of this plan (using DB statistics)
 do this for sufficient cases and pick cheapest




... Query Planner136/142

PostgreSQL has two methods of generating execution orders

 src/optimizer/path/

 generates all possible execution orders
 only suitable for small joins (up to around 15 tables)

 src/optimizer/geqo/

 generates only some execution orders
 uses genetic algorithms to chose "likely" candidates
 works effectively for large joins


See src/backend/optimizer/README for full details


Query Executor137/142

Queries run in a Portal environment containing

 the planned statement(s) (trees of Plan nodes)
 run-time versions of Plan nodes (under QueryDesc)
 description of result tuples (under TupleDesc)
 overall state of scan through result tuples (e.g. atStart)
 other context information (transaction, memory, ...)

Portal defined in src/include/utils/portal.h

PortalRun() functions also require

 destination for query results (e.g. connection to client)
 scan direction (forward or backward)



... Query Executor138/142

How query evaluation is invoked:

 exec_simple_query() creates Portal structure
 then inserts PlannedStmt into portal
 then sets up CommandDest to receive results
 then invokes PortalRun(portal,...,dest,...)
 PortalRun...() invokes ProcessQuery(plan,...)
 ProcessQuery() makes QueryDesc from plan
 then invokes ExecutorRun(qdesc,...)
 ExecutorRun() invokes ExecutePlan() to generate result



... Query Executor139/142

Run-time data structures: QueryDesc, EState, PlanState

QueryDesc structure holds run-time state of query

 defined in src/include/nodes/execnodes.h
 contains a tree of PlanState nodes
 mirrors structure of Plan tree, plus run-time info

EState structure holds working state for executor

 defined in src/include/nodes/execnodes.h
 contains copious state information, e.g.

 expression evaluation context, scan direction, tuple count




... Query Executor140/142

PlanState nodes contain state for one physical operation

 link to corresponding Plan node
 link to global EState structure
 inputs to this operation (lefttree, righttree)
 outputs from this operation (targetList, ps_projInfo)
 conditions to be satisfied by matching tuples (qual)

Plus fields for specific operations in PlanState subclasses.

PlanStates implement iterators: init()...next()...end()


... Query Executor141/142

ExecutePlan(eState, planState, ...)

 defined in backend/executor/execMain.c
 processes a PlanState tree to completion
 implements a demand-pull pipeline mechanism on tree
 asks top-level node for next tuple
 if top-level node needs tuple from child node, ask child
 recurse to operation on base table which can supply node
 pass tuple back to parent; parent continues processing



... Query Executor142/142

ExecProcNode(PlanState *node)

 defined in backend/executor/execProcnode.c
 invoked by ExecutePlan to drive demand-pull from tree
 handles processing of one PlanState operation
 uses switch on PlanState subclass type to choose 
	specialised function to handle current node (e.g. SeqScan)
 loop to find tuple satisfying qual condition
 format result tuple using ps_projInfo
 return tuple to caller


Produced: 24 Jul 2018</p><h3 >字段2</h3><p>COMP9315: Storage: Devices, Files, Pages, Tuples, Buffers, Catalogs


Storage Management



Storage Management2/234

Aims of storage management in DBMS:

 provide view of data as collection of pages/tuples
 map from database objects (e.g. relations) to disk files
 manage transfer of data to/from disk storage
 use buffers to minimise disk/memory transfers
 interpret loaded data as tuples/records
 give foundation for files structures used by access methods



... Storage Management3/234

The storage manager provides mechanisms for:

 representing database objects during query execution

 DB (handle on an authorised/opened database)
 Rel (handle on an opened relation)
 Page (memory buffer to hold contents of disk block)
 Tuple (memory holding data values from one tuple)

 referring to database objects (addresses)

 symbolic (e.g. database/schema/table/field names)
 abstract physical (e.g. PageId, TupleId)




... Storage Management4/234

Examples of references (addresses) used in DBMSs:

 PageID ... identifies (locates) a block of data

 typically, PageID = FileID + Offset
 where Offset gives location of block within file

 TupleID ... identifies (locates) a single tuple

 typically, TupleID = PageID + Offset
 where Offset gives location of tuple within page



Note that Offsets may be indexes into mapping tables giving real address.



... Storage Management5/234

Levels of DBMS related to storage management:





... Storage Management6/234

Topics to be considered:

 Disks and Files

 performance issues and organisation of disk files

 Buffer Management

 using caching to improve DBMS system throughput
 involves discussion of page replacement strategies

 Tuple/Page Management

 how tuples are represented within disk pages

 DB Object Management (Catalog)

 how tables/views/functions/types, etc. are represented


Each topic will be illustrated by its implementation in PostgreSQL.


Views of Data7/234

Users and top-level query evaluator see data as

 a collection of tables, each with a schema (tuple-type)
 where each table contains a set (sequence) of tuples







... Views of Data8/234

Relational operators and access methods see data as 

 sequence of fixed-size pages, typically 1KB to 8KB
 where each page contains tuple or index data







... Views of Data9/234

File manager sees both DB objects and file store

 maps table name + page index to file + offset






... Views of Data10/234

Disk manager sees data as

 fixed-size sectors of bytes, typically 512B
 sectors are scattered across a disk device






On typical modern databases, handled by operating system filesystem.


Storage Manager Interface11/234

The storage manager provides higher levels of system

 with an abstraction based on relations/pages/tuples
 which maps down to files/blocks/records (via buffers)

Example: simple scan of a relation:
select name from Employee

is implemented as something like
DB db = openDatabase("myDB");
Rel r = openRel(db,"Employee");
Scan s = startScan(r);
Tuple t;
while ((t = nextTuple(s)) != NULL)
{
   char *name = getField(t,"name");
   printf("%s\n", name);
}



... Storage Manager Interface12/234

The above shows several kinds of operations/mappings:

 using a database name to access meta-data
 mapping a relation name to a file
 performing page-by-page scans of files
 extracting tuples from pages
 extracting fields from tuples

The DBMS storage manager provides all of these, broken down across
several modules.


Data Flow in Query Evaluation13/234






... Data Flow in Query Evaluation14/234

Notes on typical implementation strategies:

 addresses implemented as partitioned ints 
	(e.g. PageId = (FileNum<24)||PageNum)
 addresses replaced by multiple arguments 
	(e.g. get_page(r,i,buf) rather than get_page(pid,buf))
 types such as DB and Rel are dynamic structs 
struct RelRec { int fd; int npages; int blksize; }
typedef struct RelRec *Rel;




Files in DBMSs15/234

Data sets can be viewed at several levels of abstraction in DBMSs.

Logical view: a file is a named collection of data items
	  (e.g. a table of tuples)

Abstract physical view: a file is a sequence of fixed-size data blocks.

Physical realisation: a collection of sectors scattered over ≥1 disks.

The abstraction used for managing this: PageId.


... Files in DBMSs16/234






... Files in DBMSs17/234

Two possibilities for DBMS disk managers to handle data:

 deal with the physical realisation (via disk partition)

 the DBMS implementor has to write own disk management
 gives fine-grained control for performance-critical systems
 Oracle (at least) can execute from a raw Unix disk partition

 deal with the abstract physical view (via OS filesystem)

 tables, indexes, etc. are represented as regions of ≥1 files
 disk manager handles mapping from logical → abstract physical
 different DBMSs use substantially different mappings




File System Interface18/234

Most access to data on disks in DBMSs is via a file system.

Typical operations provided by the operating system:
fd = open(fileName,mode)
  // open a named file for reading/writing/appending
close(fd)
  // close an open file, via its descriptor 
nread = read(fd, buf, nbytes)
  // attempt to read data from file into buffer 
nwritten = write(fd, buf, nbytes)
  // attempt to write data from buffer to file
lseek(fd, offset, seek_type)
  // move file pointer to relative/absolute file offset
fsync(fd)
  // flush contents of file buffers to disk



Storage Technology19/234

At this point in memory technology development:

 computational storage: fast, expensive, "small" storage is based on RAM
 bulk data storage: "slow", cheaper, large storage is based on disks

New technologies may eventually change this picture entirely

 e.g. holographic memory, large/cheap/non-volatile RAM, ...

But expect spinning disk technology to dominate for at least 5 more years.


Computational Storage20/234

Characteristics of main memory (RAM):

 linear array of bytes (or words)
 transfer unit: 1 byte (or word)
 constant time random access (≅ 10-7sec)

Accessing memory:
load   reg,byte_address
store  reg,byte_address

Cache memory has similar characteristics to RAM, but is

 faster, more expensive Rightarrow smaller

Typical capacities: RAM (256MB..64GB), Cache (64MB..2GB)


Bulk Data Storage21/234

Requirements for bulk data storage:

 non-volatile/permanent   (unlike RAM)
 high capacity   (≫ RAM)
 fast retrieval speed   (ideally ≅ RAM)
 low cost   (ideally, ≪ RAM)
 addressibility   (ideally, smallest unit possible)



... Bulk Data Storage22/234

Several kinds of bulk data storage technology currently exist:

 magnetic disks, optical disks, flash memory

Characteristics of bulk data storage technologies:

 low unit cost   (relative to RAM)
 latency in accessing data (disks)
 must read/write "blocks" of data (disks)
 block transfer size typically 512B to 4KB
 can read bytes, must write blocks (flash)
 limited number of write cycles (flash)



Magnetic Disks23/234

Classical/dominant bulk storage technology.

Characteristics:

 typical capacity   (16GB..1TB)
 data transferred per block   (512B)
 slow seek times    (10msec)
 slow rotation speed    (20msec)
 reasonable data transfer rate   (8MB/sec)

Capacity increase over last decade:   4MB → 1GB → 1TB

Modest increase in speed; good reduction in cost.


Optical Disks24/234

Optical disks provides an alternative spinning disk storage technology.

Several varieties: CD-ROM, CD-R, CD-RW, DVD-RW

Compared to magnetic disks, CD's have

 typical capacity   (300..900GB)
 limited number of write/erase cycles (CD-RW)
 data transferred per block   (2KB)
 slower seek times    (100msec)
 slower rotation speed    (20msec)
 lower data transfer rate   (150KB/sec)


More suited to write-once, read-many applications (static DBs).


Flash Memory25/234

Flash memory is a non-mechanical alternative to disk storage.

Compared to disks, flash memory has

 moderate capacity   (up to 512GB)
 limited number of write/erase cycles
 can read individual memory items
 can only erase complete blocks
 can only write onto an erased block
 good data transfer rate   (16MB/sec)
 no read latency



... Flash Memory26/234

Properties of flash memory require specialised file system

Example: updating data in flash storage

 write new copy of changed data to a fresh block
 remap file pointers
 erase old block later when storage is relatively idle

Limitations on updating reduce potential DB applications.

 acceptable for mostly-write (e.g. logs)
 not useful for frequently updated (e.g. TPS)

Overall, not yet a serious contender as a DBMS substrate.


Disk Management



Disk Manager28/234

Aim:

 handles mapping from database ID to disk address (file system)
 transfer blocks of data between buffer pool and disk
 also attempts to handle disk access error problems (retry)



... Disk Manager29/234

Basic disk management interface is simple:

void get_page(PageId p, Page buf)

 read disk block corresponding to PageId into buffer Page


void put_page(PageId p, Page buf)

 write block in buffer Page to disk block identified by PageId

PageId allocate_pages(int n)

 allocate a group of n disk blocks, optimised for sequential access

void deallocate_page(PageId p, int n)

 deallocate a group of n disk blocks, starting at PageId



Disk Technology30/234

Disk architecture:





... Disk Technology31/234

Characteristics of disks:

 collection of platters
 each platter = set of tracks (cylinders)
 each track = sequence of sectors (blocks)
 transfer unit: 1 block (e.g. 512B, 1KB, 2KB)
 access time depends on proximity of heads to required block

Accessing disk:
read  block at address (p,t,s)
write block at address (p,t,s)



Disk Access Costs32/234

Access time includes:

 seek time   (find the right track, e.g. 10-50msec)
 rotational delay   (find the right sector, e.g. 5-20msec)
 transfer time   (read/write block, e.g. 0.1msec)

Cost to write a block is similar to cost of reading

 i.e. seek time + rotational delay + block transfer time

But if we need to verify data on disk

 add full rotation delay + block transfer time



... Disk Access Costs33/234

Example disk #1 characteristics:




 3.5 inches (8cm) diameter, 3600RPM, 1 surface (platter)
 16MB usable capacity (16 × 220 = 224)
 128 tracks, 1KB blocks (sectors), 10% gap between blocks
 #bytes/track = 224/128 = 224/27 = 128KB
 #blocks/track = (0.9*128KB)/1KB = 115
 seek time:   min: 5ms (adjacent cyls),
	  avg: 25ms
	  max: 50ms 


Note that this analysis is simplified because #bytes/track and
#sectors/track varies between outer and inner tracks (same storage
density, reduced track length.



... Disk Access Costs34/234

Time Tr to read one random block on disk #1:

 3600 RPM = 60 revs per sec,   rev time = 16.7 ms
 Time over blocks = 16.7 × 0.9 = 15 ms
 Time over gaps = 16.7 × 0.1 = 1.7 ms
 Transfer time for 1 block = 15/115 = 0.13 ms
 Time for skipping over gap = 1.7/115 = 0.01 ms

Tr = seek + rotation + transfer

Minimum Tr = 0 + 0 + 0.13 = 0.13 ms

Maximum Tr = 50 + 16.7 + 0.13 = 66.8 ms

Average Tr = 25 + (16.7/2) + 0.13 = 33.5 ms


... Disk Access Costs35/234

If operating system deals in 4KB blocks:



Tr(4-blocks) = 25 + (16.7/2) + 4×0.13 + 3×0.01 = 33.9 ms

Tr(1-block) = 25 + (16.7/2) + 0.13 = 33.5 ms

Note that the cost of reading 4KB is comparable to reading 1KB.

Sequential access reduces average block read cost significantly, but

 is limited to 115 block sequences
 is only useful if blocks need to be sequentially scanned



... Disk Access Costs36/234

Example disk #2 characteristics:

 3.5 inches (8cm) diameter, 3600RPM, 8 surfaces (platters)
 8GB usable capacity (8 × 230 = 233 bytes)
 8K (213) cylinders = 8k tracks per surface
 256 sectors/track, 512 (29) bytes/sector

Addressing = 3 bits (surface) + 13 bits (cylinder) + 8 bits (sector)

If using 32-bit addresses, this leaves 8 bits (28=256 items/block).



Disk Characteristics37/234

Three important characteristics of disk subsystems:

 capacity
	  (how much data can be stored on the disk)
 access time
	  (how long does it take to fetch data from the disk)
 reliability
	  (how often does the disk fail? temporarily? catastrophically?)

Mean time to (complete) failure: 3-10 years.


... Disk Characteristics38/234

Increasing capacity:

 buy a larger disk, or buy more disks
 make the data smaller (using compression techniques)

Improving access time:

 minimise block transfers: clustering, buffering, scheduled access
 reduce seek: faster moving heads, fixed heads, scheduled access
 reduce latency: faster spinning disks, scheduled access
 layout of data on disk (file organisation) can also assist

Improving reliability:

 add redundancy by adding more disks



Increasing Disk Capacity39/234

Compress data (e.g. LZ encoding)

+ more data fits on disk

- compression/expansion overhead

For large compressible data (e.g. text), significant savings.

For most relational data (e.g. int, char(8)), no significant saving.

For high-performance memory caching, may never want to expand 
(there is current research working on "computable" compressed data formats).


Improving Disk Access Costs40/234

Approach #1: Use knowledge of data access patterns.

E.g. two records frequently accessed together 
⇒ put them in the same block (clustering)

E.g. records scanned sequentially 
⇒ place them in "staggered" blocks, double-buffer


Arranging data to match access patterns can improve throughput by 10-20 times.


 

Approach #2: Avoid reading blocks for each item access.

E.g. buffer blocks in memory, assume likely re-use


Scheduled Disk Access41/234

Low-level disk manager (driver, controller):

 collects list of read/write requests from multiple requestors
 schedules their execution to minimise head movement and latency
 using a queue with priority function based on disk states

Example head movement scheduler: elevator algorithm


 head moves uniformly out towards edge of disk, handling requests "on the way"
 reaches edge, then moves uniformly towards centre of disk, handling requests
 reaches center, then moves out towards edge of disk ...




Disk Layout42/234

If data sets are going to be frequently accessed in a pre-determined
manner, arrange data on disk to minimise access time.

E.g. sequential scan

 place subsequent blocks in same cylinder, different platters
 stagger so that as soon as block i read, block i+1 is available
 once cylinder exhausted, move to adjacent cylinder

Older operating systems provided fine-grained control of disk layout.

Modern systems generally don't, because of programmer complexity.

Unix has raw disk partitions: no file system, you write driver to manage disk.


Improving Writes43/234

Nonvolatile write buffers

 "write" all blocks to memory buffers in nonvolatile RAM
 transfer to disk when idle, or when disk head in "good" location
 some operating systems (e.g. Solaris) support this

Log disk

 write all blocks to a special sequential access file system
 transfer to real disk when idle
 additional advantage of having information available for recovery



Double Buffering44/234

Double-buffering exploits potential concurrency between disk and memory.

While reads/writes to disk are underway, other processing can be done.





With at least two buffers, can keep disk working full-time.


... Double Buffering45/234

Example: select sum(salary) from Employee

 relation =  file (= a sequence of b blocks A, B, C, D, ...)
 processing data with a single buffer:

read A into buffer then process buffer content
read B into buffer then process buffer content
read C into buffer then process buffer content
...

Costs:

 cost of reading a block = Tr
 cost of processing a block = Tp
 total elapsed time = b.(Tr+Tp)
	= bTr + bTp

Typically,   Tp < Tr
	  (depends on kind of processing)


... Double Buffering46/234

Double-buffering approach:
read A into buffer1
process A in buffer1
  and concurrently read B into buffer2
process B in buffer2
  and concurrently read C into buffer1
...

Costs:

 overall cost depends on relative sizes of Tr and Tp
 if Tp ≅ Tr, total elapsed time =
	Tr + bTp
	  (cf. bTr + bTp))



General observation: use of multiple buffers can lead to substantial
cost savings. 
We will see numerous examples where multiple memory buffers
are exploited.



Multiple Disk Systems47/234

Various strategies can be employed to improve capacity, performance
and reliability when multiple disks are available.

RAID (redundant arrays on independent disks) defines a standard set
of such techniques.

Essentially, multiple disks allow

 improved reliability by redundant storage of data
 reduced access cost by exploiting parallelism

Capacity increases naturally by adding multiple disks

(although there is obviously a trade-off between increased capacity
and increased reliability via redundancy)


RAID Level 048/234

Uses striping to partition data for one file over several disks

E.g. for n disks, block i in the file is written to disk (i mod n)

Example: file with 6 data blocks striped onto 4 disks using (pid mod 4)



Increases capacity, improves data transfer rates, reduces reliability.


... RAID Level 049/234

The disk manager and RAID controller have to perform a mapping something like:
    writePage(PageId)

to
    disk = diskOf(PageId,ndisks)
    cyln = cylinderOf(PageId)
    plat = platterOf(PageId)
    sect = sectorOf(PageId)
    writeDiskPage(disk, cyln, plat, sect)

(We discuss later how the pid might be represented and mapped)


RAID Level 150/234

Uses mirroring (or shadowing) to store multiple copies of each block.

Since disks can be read/written in parallel, transfer cost unchanged.

Multiple copies allows for single-disk failure with no data loss.

Example: file with 4 data blocks mirrored on two 2-disk partitions



Reduces capacity, improves reliability, no effect on data transfer rates.


... RAID Level 151/234

The disk manager and RAID controller have to perform a mapping something like:
    writePage(PageId)

to
    n = ndisksInPartition
    disk = diskOf(PageId,n)
    cyln = cylinderOf(PageId)
    plat = platterOf(PageId)
    sect = sectorOf(PageId)
    writeDiskPage(disk, cyln, plat, sect)
    writeDiskPage(disk+n, cyln, plat, sect)



RAID levels 2-652/234

The higher levels of raid incorporate various combinations of:

 block/bit-level striping, mirroring, and error correcting codes 
	  (ECC)


The differences are primarily in:

 the kind of error checking/correcting codes that are used
 where the ECC parity bits are stored

RAID levels 2-5 can recover from failure in a single disk.

RAID level 6 can recover from smultaneous failures in two disks.


Disk Media Failure53/234

Rarely, a bit will be transferred to/from the disk incorrectly.

Error-correcting codes can check for and recover from this.

If recovery is not possible, the operation can simply be repeated.

If repeated reads/writes on the same block fail:

 the low-level disk manager assumes permanent media failure
 marks the offending block physical address in a bad block table
 the block will be deallocated and never re-used
 if a copy of data is available, can be restored elsewhere on disk



Database Objects54/234

DBMSs maintain various kinds of objects/information:

databasecan be viewed as an super-object for all others
parametersglobal configuration information
cataloguemeta-information describing database contents
tablesnamed collections of tuples
tuplescollections of typed field values
indexesaccess methods for efficient searching
update logsfor handling rollback/recovery
proceduresactive elements



... Database Objects55/234

The disk manager implements how DB objects are mapped to file system.

References to data objects typically reduce to e.g.

 access object in buffer at position Offset
 buffer is obtained as page PageId in system
 object is addressed via a RecordID = PageId+Offset

The disk manager needs to convert buffer access to

 ensure that the relevant file is open
 locate the physical page within the file
 read/write the appropriate amount of data to/from buffer



Single-file DBMS56/234

One possible storage organisation is a single file for the entire database.

All objects are allocated to regions of this file.



Objects are allocated to regions (segments) of the file.

If an object grows too large for allocated segment, allocate an extension.

What happens to allocated space when objects are removed?


... Single-file DBMS57/234

Allocating space in Unix files is easy:

 simply seek to the place you want and write the data
 if nothing there already, data is appended to the file
 if something there already, it gets overwritten

If the seek goes way beyond the end of the file:

 Unix does not allocate disk space for the "hole" until it is written

Under these circumstances, a disk manager is easy to implement.


Single-file Disk Manager58/234

Simple disk manager for a single-file database:
// Disk Manager data/functions
#define PAGESIZE 2048   // bytes per page
typedef int PageId;     // PageId is block index
typedef struct DBrec {
   char *dbname;     // copy of database name
   int fd;           // the database file
   SpaceTable map;   // map of free/used areas 
   NameTable names;  // map names to areas + sizes
} *DB;

typedef struct Relrec {
   char *relname;    // copy of table name
   int   start;      // page index of start of table data
   int   npages;     // number of pages of table data
   ...
} * Rel;



... Single-file Disk Manager59/234

DB openDatabase(char *name) { 
   DB db = new(struct DBrec);
   db->dbname = strdup(name);
   db->fd = open(name,O_RDWR);
   db->map = readSpaceTable(DBfd);
   db->names = readNameTable(DBfd);
   return db;
}
// stop using DB and update all meta-data
void closeDatabase(DB db) {
   writeSpaceTable(db->fd,db->map);
   writeNameTable(db->fd,db->map);
   fsync(db->fd);
   close(db->fd);
   free(db);
}



... Single-file Disk Manager60/234

// set up struct describing relation
Rel openRelation(DB db, char *rname) {
   Rel r = new(struct Relrec);
   r->relname = strdup(rname);
   // get relation data from map tables
   r->start = ...;
   r->npages = ...;
   return r;
}

// stop using a relation
void closeRelation(Rel r) {
   free(r);
}

#define nPages(r)  (r->npages)
#define makePageId(r,i)  (r->first + i)



... Single-file Disk Manager61/234

// assume that Page = byte[PageSize]
// assume that PageId = block number in file

// read page from file into memory buffer
void get_page(DB db, PageId p, Page buf) {
   lseek(db->fd, p*PAGESIZE, SEEK_SET);
   read(db->fd, buf, PAGESIZE);
}

// write page from memory buffer to file
void put_page(Db db, PageId p, Page buf) {
   lseek(db->fd, p*PAGESIZE, SEEK_SET);
   write(db->fd, buf, PAGESIZE);
}



... Single-file Disk Manager62/234

// managing contents of mapping table is complex
// assume a list of (offset,length,status) tuples

// allocate n new pages at end of file
PageId allocate_pages(int n) {
   int endfile = lseek(db->fd, 0, SEEK_END);
   addNewEntry(db->map, endfile, n);
   // note that file itself is not changed
}
// drop n pages starting from p
void deallocate_pages(PageId p, int n) {
   markUnused(db->map, p, n);
   // note that file itself is not changed
}



Example: Scanning a Relation63/234

With the above disk manager, our example:
select name from Employee

might be implemented as something like
DB db = openDatabase("myDB");
Rel r = openRelation(db,"Employee");
int npages = nPages(r);
Page buffer = malloc(PAGESIZE*sizeof(char));
for (int i = 0; i < npages; i++) {
   PageId pid = makePageId(r,i);
   get_page(db, pid, buffer);
   foreach tuple in buffer {
      get tuple data and extract name
   }
}



Multiple-file Disk Manager64/234

Most DBMSs don't use a single large file for all data.

They typically provide:

 multiple files partitioned physically or logically
 mapping from DB-level objects to files (e.g. via meta-data)

Precise file structure varies between individual DBMSs.


... Multiple-file Disk Manager65/234

Structure of PageId for data pages in such systems ...

If system uses one file per table, PageId contains:

 relation indentifier (which can be mapped to filename)
 page number (to identify page within the file)

If system uses several files per table, PageId contains:

 relation identifier
 file identifier (combined with relid, gives filename)
 page number (to identify page within the file)



Oracle File Structures66/234

Oracle uses five different kinds of files:


  data files
  catalogue, tables, procedures


  redo log files
  update logs


  alert log files
  record system events


  control files
  configuration info


  archive files
  off-line collected updates




... Oracle File Structures67/234

There may be multiple instances of each kind of file:

 they may be spread across several disk devices (for load balancing)
 they may be duplicated (for redundancy/reliability)

Data files are

 typically very large (> 100MB)
 typically allocated to several different file systems
 logically partitioned into tablespaces
	  (SYSTEM, plus dba-defined others)



... Oracle File Structures68/234

Tablespaces are logical units of storage (cf directories).

Every database object resides in exactly one tablespace.

Units of storage within a tablespace:



  data block
  fixed size unit of storage (cf 2KB page)


  extent
  specific number of contiguous data blocks


  segment
  set of extents allocated to a single database object


Segments can span multiple data files; extents cannot.

To be confusing, tables are called datafiles internally in Oracle.


... Oracle File Structures69/234

Layout of data within Oracle file storage:






PostgreSQL Storage Manager70/234

PostgreSQL uses the following file organisation ...





... PostgreSQL Storage Manager71/234

Components of storage subsystem:

 mapping from relations to files   (RelFileNode)
 abstraction for open relation pool   (storage/smgr)
 functions for managing files   (storage/smgr/md.c)
 file-descriptor pool   (storage/file)

PostgreSQL has two basic kinds of files:

 heap files containing data (tuples)
 index files containing index entries


Note: smgr designed for many storage devices; only mag disk handler used



Relations as Files72/234

PostgreSQL identifies relation files via their OIDs.

The core data structure for this is RelFileNode:
typedef struct RelFileNode
{
    Oid  spcNode;  // tablespace
    Oid  dbNode;   // database
    Oid  relNode;  // relation
} RelFileNode;

Global (shared) tables (e.g. pg_database) have

   spcNode == GLOBALTABLESPACE_OID
   dbNode == 0



... Relations as Files73/234

The relpath function maps RelFileNode to file:

char *relpath(RelFileNode rnode)  // simplified
{
    char *path = malloc(ENOUGH_SPACE);

    if (rnode.spcNode == GLOBALTABLESPACE_OID) {
        /* Shared system relations live in PGDATA/global */
        Assert(rnode.dbNode == 0);
        sprintf(path, "%s/global/%u",
                DataDir, rnode.relNode);
    }
    else if (rnode.spcNode == DEFAULTTABLESPACE_OID) {
        /* The default tablespace is PGDATA/base */
        sprintf(path, "%s/base/%u/%u",
                DataDir, rnode.dbNode, rnode.relNode);
    }
    else {
        /* All other tablespaces accessed via symlinks */
        sprintf(path, "%s/pg_tblspc/%u/%u/%u", DataDir
                rnode.spcNode, rnode.dbNode, rnode.relNode);
    }
    return path;
}



File Descriptor Pool74/234

Unix has limits on the number of concurrently open files.

PostgreSQL maintains a pool of open file descriptors:

 to hide this limitation from higher level functions
 to minimise expensive open() operations

File names are simply strings: typedef char *FileName

Open files are referenced via: typedef int File

A File is an index into a table of "virtual file descriptors".


... File Descriptor Pool75/234

Interface to file descriptor (pool):

File FileNameOpenFile(FileName fileName,
                      int fileFlags, int fileMode);
     // open a file in the database directory ($PGDATA/base/...)
File PathNameOpenFile(FileName fileName,
                      int fileFlags, int fileMode);
     // open a file given a full file path
File OpenTemporaryFile(bool interXact);
     // open temp file; flag: close at end of transaction?
void FileClose(File file);
void FileUnlink(File file);
int  FileRead(File file, char *buffer, int amount);
int  FileWrite(File file, char *buffer, int amount);
int  FileSync(File file);
long FileSeek(File file, long offset, int whence);
int  FileTruncate(File file, long offset);



... File Descriptor Pool76/234

Virtual file descriptors (Vfd)

 physically stored in dynamically-allocated array



 also arranged into list by recency-of-use




VfdCache[0] holds list head/tail pointers.


... File Descriptor Pool77/234

Virtual file descriptor records (simplified):

typedef struct vfd
{
    s_short  fd;              // current FD, or VFD_CLOSED if none
    u_short  fdstate;         // bitflags for VFD's state
    File     nextFree;        // link to next free VFD, if in freelist
    File     lruMoreRecently; // doubly linked recency-of-use list
    File     lruLessRecently;
    long     seekPos;         // current logical file position
    char     *fileName;       // name of file, or NULL for unused VFD
    // NB: fileName is malloc'd, and must be free'd when closing the VFD
    int      fileFlags;       // open(2) flags for (re)opening the file
    int      fileMode;        // mode to pass to open(2)
} Vfd;



File Manager78/234

The "magnetic disk storage manager"

 manages its own pool of open file descriptors
 each one represents an open relation file (Vfd)
 may use several Vfd's to access data, if file > 2GB
 manages mapping from PageId to file+offset.

PostgreSQL PageId values are structured:
typedef struct
{
    RelFileNode rnode;    // which relation
    ForkNumber  forkNum;  // which fork
    BlockNumber blockNum; // which block 
} BufferTag;



... File Manager79/234

Access to a block of data proceeds as follows:
offset = BlockNumber * BLCKSZ
fileID = RelFileNode+ForkNumber
if (fileID is already in Vfd pool) {
    if (offset is in this file)
    	fd = use Vfd from pool
    else
        fd = allocate new Vfd for next part of file
} else {
    fd = allocate new Vfd for this file
}
seek to offset in fd
read/write data page (BLCKSZ bytes)

BLCKSZ is a global configurable constant (default: 8192).


Buffer Pool



Buffer Manager81/234

Aim:

 minimise traffic between disk and memory via caching
 mantains a (shared) buffer pool in main memory


Buffer pool

 collection of page slots (aka frames)
 each frame can be filled with a copy of data from a disk block



... Buffer Manager82/234

Buffer pool interposed between access methods and disk manager 





Access methods/page manager normally work via get_page() calls;
now work via calls to get_page_via_buffer_pool())



... Buffer Manager83/234

Basic buffer pool interface

Page request_page(PageId p);

 get disk block corresponding to page p into buffer pool

void release_page(PageId p);

 indicate that page p is no longer in use (advisory)

void mark_page(PageId p);

 indicate that page p has been modified (advisory)

void flush_page(PageId p);

 write contents of page p from buffer pool onto disk

void hold_page(PageId p);

 recommend that page p should not be swapped out


Buffer pool typically provides interface to allocate_page
and deallocate_page as well.



Buffer Pool84/234






... Buffer Pool85/234

Buffer pool data structures:

 a fixed-size, memory-resident collection of frames (page-slots)
 a directory containing information about the status of each frame

For each frame, we need to know:

 whether it is currently in use
 which Page it contains   (i.e. PageId = (relid,page#))
 whether it has been modified since loading (dirty bit)
 how many transactions are currently using it (pin count)
 time-stamp for most recent access



... Buffer Pool86/234

In subsequent discussion, we assume:

 cost of manipulating in-memory buffer pool data is insignificant
 all file access methods use request_page() instead of get_page()



Requesting Pages87/234

Call from client: request_page(pid)

If page pid is already in buffer pool:

 no need to read it again
 use the copy in the pool   (unless write-locked)


If page pid is not already in buffer pool:

 need to read page from disk into a free frame
 if no free frames, need to remove a page from the pool



... Requesting Pages88/234

Advantages:

 if a page is required several times for an operation, only read once

Disadvantages:

 overhead of managing buffer pool for each page request (insignificant)
 if page access pattern clashes with replacement, no effective caching



Releasing Pages89/234

The release_page function indicates that a page

 is no longer required by this transaction
 is a good candidate for replacement
	  (iff noone else using it)

If the page hasn't been modified, simply overwritten when replaced.

If the page has been modified, must be written to disk before replaced.


Possible problem: changes not immediately reflected on disk



... Releasing Pages90/234

Advantages:

 if page modified several times while in the pool, only written once

Disadvantages:

 overhead of managing buffer pool for each page request
	(insignificant)

If a page remains in pool over multiple transactions

 e.g. (requested,modified,released) several times but not replaced
 need to ensure that changes are guaranteed to be reflected on disk
 even if the system crashes before page is replaced

(This is generally handled by some kind of logging mechanism (e.g. Oracle redo log files).


Buffer Manager Example #191/234

Self join: an example where buffer pool achieves major efficiency gains.

Consider a query to find pairs of employees with the same birthday:
select e1.name, e2.name
from   Employee e1, Employee e2
where  e1.id < e2.id and e1.birthday = e2.birthday

This might be implemented inside the DBMS via nested loops:
for each tuple t1 in Employee e1 {
    for each tuple t2 in Employee e2 {
        if (t1.id < t2.id &&
                t1.birthday == t2.birthday)
            append (t1.name,t2.name) to result set
    }
}



... Buffer Manager Example #192/234

In terms of page-level operations, the algorithm looks like:
DB db = openDatabase("myDB");
Rel emp = openRel(db,"Employee");
int npages = nPages(emp);

for (int i = 0; i < npages; i++) {
    PageId pid1 = makePageId(emp,i);
    Page p1 = request_page(pid1);
    for (int j = 0; j < npages; i++) {
        PageId pid2 = makePageId(emp,j);
        Page p2 = request_page(pid2);
        // compare all pairs of tuples from p1,p2
        // construct solution set from matching pairs
        release_page(pid2);
    }
    release_page(pid1);
}



... Buffer Manager Example #193/234

Consider a buffer pool with 200 frames and a relation with b ≤ 200 pages:

 first request for p1 loads page 0 into buffer pool
 first request for p2 finds page 0 already loaded
 rest of first p2 iteration loads all other pages from Employee
 all subsequent requests find required page already loaded

Total number of page reads = b
	  (entire relation is read exactly once)


... Buffer Manager Example #194/234

Now consider a buffer pool with 2 frames
	(the minimum required for the join):

 first request for p1 loads page 0 into buffer pool
 first request for p2 finds page 0 already loaded
 next request for p2 loads page 1 into buffer pool
 next request for p2 finds buffer pool full 
	⇒ need to free frame (but note that no write is required)
 because page 0 is "in use", we replace page 1

(continued ...)


... Buffer Manager Example #195/234

(... continued)

 request/release ⇒ page 0 remains in buffer while scanning on p2
 on each of the b-1 subsequent p2 scans ...

 the p1 page remains resident, while we iterate over the p2 pages
 we don't need to read the p1 page (it's already resident)


Total number of page reads = b * (b-1)

Cf. 200-frame buffer vs 2-frame buffer ... if b=100, 100 reads vs 10000 reads.


The request_page Operation96/234

Method:


Check buffer pool to see if it already contains requested page. 
If not, the page is brought in as follows:

 Choose a frame for replacement, using replacement policy
 If frame chosen is dirty, write page to disk
 Read requested page into now-vacant buffer frame
 Set dirty=False and pinCount=0 for this frame


Pin the frame containing requested page (i.e. update pin count).

Return reference to frame (Page) containing requested page.



Other Buffer Operations97/234

The release_page operation:

 Decrement pin count for specified page

Note: no effect on disk or buffer contents until replacement required.

The mark_page operation:

 Set dirty bit on for specified page

Note: doesn't actually write to disk; indicates that frame needs to be written if used for replacement;

The flush_page operation:

 Write the specified page to disk (using write_page)

Note: not generally used by higher levels of DBMS; they rely on request/release protocol.


Page Replacement Policies98/234

Several schemes are commonly in use:

 Least Recently Used (LRU)


 often used for VM in operating systems; intuitively appealing but can perform badly


 First in First Out (FIFO)


 need to maintain a queue of frames; enter tail of queue when read in


 Most Recently Used
 Random

LRU works for VM because of working set model 
(recent past accesses determines future accesses)

For DBMS, we can predict patterns of page access better 
(from our knowledge of how the relational operations are implemented)


... Page Replacement Policies99/234

The cost benefits from a buffer pool (with n frames) is determined by:

 number of available frames (more ⇒ better)
 interaction between replacement strategy and page access patterns

Example (a): sequential scan, LRU or MRU, n ≥ b, no competition

First scan costs b reads; subsequent scans are ``free''.

Example (b): sequential scan, MRU, n < b, no competition

First scan costs b reads; subsequent scans cost b - n reads.

Example (c): sequential scan, LRU, n < b, no competition

All scans cost b reads; known as sequential flooding.


Page Access Times100/234

How to determine when a page in the buffer was last accessed?

Could simply use the time of the last request_page for that PageId.

But this doesn't reflect real accesses to page.

For more realism, could use last request_page or release_page time.

Or could introduce operations for examining and modifying pages in pool:

 examine_page(PageId, TupleId) and modify_page(PageId, TupleId, Tuple)
 add "last access time" field to directory entry for each frame
 above operations access the page and also update the access time field



Buffer Manager Example #2101/234

Standard join: an example where replacement policy can have large impact.

Consider a query to find customers who are also employees:
select c.name
from   Customer c, Employee e
where  c.ssn = e.ssn;

This might be implemented inside the DBMS via nested loops:
for each tuple t1 in Customer {
    for each tuple t2 in Employee {
        if (t1.ssn == t2.ssn)
            append (t1.name) to result set
    }
}



... Buffer Manager Example #2102/234

Assume that:

 the Customer relation has bC pages (e.g. 20)
 the Employee relation has bE pages (e.g. 10)
 the buffer pool has n frames (e.g. 10)
 it cannot hold either relation completely   (n < bC and n < bE)



... Buffer Manager Example #2103/234

Works well with MRU strategy:

 pins Customer page, then processes all Employee pages against it
 each Customer page read exactly once
 n - 2 Employee pages read once
 the rest are read once on each of the bC iterations

Total page reads = bC + (n - 2) + bC × (bE - (n-2)) = 20 + 9 + 20*2 = 189


Note: assumes that both request_page and release_page set the last usage timestamp.



... Buffer Manager Example #2104/234

Works less well with LRU strategy:

 pins Customer page, then starts to process Employee pages
 when pool fills starts replacing Employee pages from beginning
 each Customer page read exactly once
 each Employee page read once on each iteration

Total page reads = bC + bC × bE = 20 + 20*10 = 220


PostgreSQL Buffer Manager105/234

PostgreSQL buffer manager:

 provides a shared pool of memory buffers for all backends
 all access methods get data from disk via buffer manager

Same code used by backends which need a local buffer pool.

Buffers are located in a large region of shared memory.

Functions:  src/backend/storage/buffer/*.c

Definitions:  src/include/storage/buf*.h


... PostgreSQL Buffer Manager106/234

Buffer pool consists of:

 shared fixed array (size Nbuffers) of BufferDesc
 shared fixed array (size Nbuffers) of Buffer
 each BufferDesc contains:

 reference to memory for Buffer
 status information (e.g. pin count, lock state)

 number of buffers set in postgresql.conf, e.g.
shared_buffers = 16MB     # min 128KB, at least max_connections*2, 8KB each




... PostgreSQL Buffer Manager107/234






... PostgreSQL Buffer Manager108/234

Definitions related to buffer manager:

include/storage/buf.h

 basic buffer manager data types (e.g. Buffer)

include/storage/bufmgr.h

 definitions for buffer manager function interface 
	(i.e. the functions that other parts of the system call to user buffer manager)

include/storage/buf_internals.h

 definitions for buffer manager internals (e.g. BufferDesc)

Code in: backend/storage/buffer/


Buffer Pool Data Objects109/234

BufferDescriptors: array of structures describing buffers

 holds data showing buffer usage; implements free list

Buffer: index into BufferDescriptors

 index values run from 1..Nbuffers ⇒ need -1
 local buffers have negative indexes

BufMgrLock: global lock on buffer pool

 needs to be obtained when modifying content in buffer pool

BufferTag

 data structure holding (r,b) pair; used to hash buf ids



... Buffer Pool Data Objects110/234

Buffer manager data types:
BufFlags: BM_DIRTY, BM_VALID, BM_TAG_VALID, BM_IO_IN_PROGRESS, ...

typedef struct buftag {
   RelFileNode rnode;     /* physical relation identifier */
   ForkNumber  forkNum;
   BlockNumber blockNum;  /* relative to start of reln */
} BufferTag;

typedef struct sbufdesc {  (simplified)
   BufferTag   tag;         /* ID of page contained in buffer */
   BufFlags    flags;       /* see bit definitions above */
   uint16      usage_count; /* usage counter for clock sweep */
   unsigned    refcount;    /* # of backends holding pins */
   int         buf_id;      /* buffer's index number (from 0) */
   int         freeNext;    /* link in freelist chain */
   ...
} BufferDesc;



Buffer Pool Functions111/234

Buffer manager interface:

Buffer ReadBuffer(Relation r, BlockNumber n)

 ensures nth page of file for relation r is loaded 
(may need to remove an existing unpinned page and read data from file)
 increments reference (pin) count and usage count for buffer
 returns index of loaded page in buffer pool (Buffer value)
 assumes main fork, so no ForkNumber required


Actually a special case of ReadBuffer_Common, which also handles
variations like different replacement strategy, forks, temp buffers, ...



... Buffer Pool Functions112/234

Buffer manager interface (cont):

void ReleaseBuffer(Buffer buf)

 decrement pin count on buffer
 if pin count falls to zero, 
	ensures all activity on buffer is completed before returning 

void MarkBufferDirty(Buffer buf)

 marks a buffer as modified
 requires that buffer is pinned and locked
 actual write is done later (e.g. when buffer replaced) 



... Buffer Pool Functions113/234

Additional buffer manager functions:

Page BufferGetPage(Buffer buf)

 finds actual data associated with buffer in pool
 returns reference to memory where data is located 

BufferIsPinned(Buffer buf)

 check whether this backend holds a pin on buffer

CheckPointBuffers

 write data in checkpoint logs (for recovery)
 flush all dirty blocks in buffer pool to disk

etc. etc. etc.


... Buffer Pool Functions114/234

Important internal buffer manager function:

BufferDesc *BufferAlloc( 
                        Relation r, ForkNumber f, 
                        BlockNumber n, bool *found)

 used by ReadBuffer to find a buffer for (r,f,n)
 if (r,f,n) already in pool, pin it and return descriptor
 if no available buffers, select buffer to be replaced
 returned descriptor is pinned and marked as holding (r,f,n)
 ReadBuffer has to do the actual I/O



Clock-sweep Replacement Strategy115/234

PostgreSQL page replacement strategy: clock-sweep

 treat buffer pool as circular list of buffer slots
 NextVictimBuffer holds index of next possible evictee
 if page is pinned or "popular", leave it 

 usage_count implements "popularity/recency" measure
 incremented on each access to buffer (up to small limit)
 decremented each time considered for eviction

 increment NextVictimBuffer and try again (wrap at end)


For specialised kinds of access (e.g. sequential scan),
can allocate a private "buffer ring"
with different replacement strategy.



Record/Tuple Management



Views of Data117/234

The disk and buffer manager provide the following view:

 data is a sequence of fixed-size blocks (pages)
 blocks can be (random) accessed via a PageId

Database applications view data as:

 a collection of records (tuples)
 records can be accessed via a RecordId (RID)

Standard terminology: records are also called tuples, items, rows, ...


... Views of Data118/234

The abstract view of a relation:

 a named and (possibly) ordered sequence of tuples
 with (possibly) some additional access method data structures

The physical representation of a relation:

 an indexed sequence of pages in one or more files
 where each page contains a collection of records
 along with data structures to manage the records



... Views of Data119/234

We use the following low-level abstractions:

RecPage

 a view of a disk page ... record data + storage management info
 provides an interpretation of byte[] provided by buffer manager

Record

 physical view of a table row ... a sequence of bytes
 format of table row data used for storing on disk



... Views of Data120/234

We use the following high-level abstractions:

Relation

 logical view of a database table ... collection of tuples
 implemented via multiple pages in multiple files

Tuple

 logical view of a table row ... a collection of typed fields
 format of table row data used for manipulating in memory



Records vs Tuples121/234

A table is defined by a collection of attributes (schema), e.g.
create table Employee (
   id#  integer primary key,
   name varchar(20),   -- or char(20)
   job  varchar(10),   -- or char(10)
   dept number(4)
);

A tuple is a collection of attribute values for such a schema, e.g.
    (33357462, 'Neil Young', 'Musician', 0277)

A record is a sequence of bytes, containing data for one tuple.


Record Management122/234

Aim:

 provide Tuple and Record abstractions
 provide mapping from RecordId to Tuple
 allocate/maintain space within blocks (via RecPage abstraction)

In other words, the record manager reconciles the views of a block:

 array of bytes (physical)   vs   collection of tuples (logical)
 via the notion of records and intra-block storage management

Assumptions   (neither of which are essential):

 each block contains tuples from one relation
 every tuple is (much) smaller than a single page



Page-level Operations123/234

Operations to access records from a page ...

Record get_record(RecordId rid)

 get record rid from page; returns reference to Record

Record first_record()

 return reference to Record first record in page

Record next_record()

 return reference to Record immediately following last accessed one
 returns null if no more records left in the page



... Page-level Operations124/234

Operations to make changes to records in a page ...

void update_record(RecordId rid, Record rec)

 change value of record rid to the value stored in rec

RecordId insert_record(Record rec)

 insert new record into page and return its rid

void delete_record(RecordId rid)

 remove the record rid from the page



Tuple-level Operations125/234

Typ   getTypField(int fno)

 extract the fno'th field from a Tuple as a value of type Typ

Examples:   getIntField(1),   getStringField(2)

void   setTypField(int fno, Typ val)

 set the value of the fno'th field of a Tuple to val

Examples:   setIntField(1,42),   setStringField(2,"abc")

Also need operations to convert between Record and Tuple formats.


Relation-level Operations126/234

Tuple get_tuple(RecordId rid)

 fetch the tuple specified by rid; return reference to Tuple

Tuple first_tuple()

 return reference to record first Tuple in page

Tuple next_tuple()

 return reference to Tuple immediately following last accessed one
 returns null if no more Tuples left in the relation

Plus operations to insert, delete and modify Tuples
	 (analogous to Records)


Example Query127/234

Recall previous example of simple scan of a relation:
select name from Employee

implemented as:
DB db = openDatabase("myDB");
Rel r = openRel(db,"Employee");
Scan s = startScan(r);
Tuple t;
while ((t = nextTuple(s)) != NULL)
{
   char *name = getField(t,"name");
   printf("%s\n", name);
}



... Example Query128/234

Conceptually, the scanning implementation is simple:
// maintain "current" state of scan
struct ScanRec { Rel curRel; RecId curRec };
typedef struct ScanRec *Scan;

Scan startScan(Rel r) {
   Scan s = malloc(sizeof(struct ScanRec));
   s->curRec = firstRecId(r);
   return s;
}

Tuple nextTuple(Scan s) {
   Tuple t = fetchTuple(s->curRec);
   s->curRec = nextRecId(r,s->curRec);
   return t;
}



... Example Query129/234

The real implementation relies on the buffer manager:
struct ScanRec {
   Rel curRel; PageId curPID; RecPage curPage;
};
typedef struct ScanRec *Scan;

Scan startScan(Rel r)
{
   Scan s = malloc(sizeof(struct ScanRec));
   s->curPID = firstPageId(r);
   Buffer page = request_page(s->curPage);
   s->curPage = start_page_scan(page);
   return s;
}



... Example Query130/234

And similarly the nextTuple() function:
Tuple nextTuple(Scan s)
{
   // if more records in the current page
   Tuple t;
   if (t = next_rec_in_page(s->curPage)) != NULL)
      return t;
   while (t == null) {   // current page finished
      release_page(s->curPID);   // release current page
      s->curPID = next_page_id(s->curRel, s->curPID);
      // ... and if no more pages, then finished
      if (s->curPID == NULL) return NULL;
      Buffer page = request_page(s->curPID);
      s->curPage = start_page_scan(page);
      t = next_rec_in_page(s->curPage);
   }
   return t;
}



Record Identifiers131/234

The implementation of RecordIDs is determined by the physical storage structure of the DBMS.

A RecordId always has at least two components:

 a page number to indicate which page the record is contained in
 a slot number to indicate where the record is located within the page

If multiple files for a relation, then also need:

 a file number to indicate which file the page is contained in

(Or, more likely, use a PageId which combines both the file number and page number)

Some DBMSs provide ROWIDs in SQL to permit efficient tuple access.

PostgreSQL provides a unique OID for every row in the database.


... Record Identifiers132/234

RecordID components are

 implemented as counters (table indexes) rather than absolute offsets
 to save space and to allow for flexibility in storage management

E.g. with 4KB pages and 16 bits available for page addressing

 using file offsets allows us to address only 16 pages 
	(page addresses are all of the form 0x0000, 0x1000, 0x2000, 0x3000, ...)

 using page numbers allows us to address 65,536 pages

E.g. using indexes into a slot table to identify records within a page

 allows records to move within page without changing their RecordId



Example RecordId Structure133/234

Consider a DBMS like Oracle which uses a small number of large files.

Suitable RecordIds for such a system, using 32-bits, might be built as:

 4-bits for file number
	  (allows for at most 16 files in the database)
 20-bits for page number
	  (allows for at most 106 pages per file)
 8-bits for slot number
	  (allows for at most 256 records per page)

Example:



(Note: however you partition the bits, you can address at most 4 billion records)


... Example RecordId Structure134/234

Consider a DBMS like MiniSQL, which uses one data file per relation.

One possibility is a variation on the Oracle approach:

 9-bits for file number
          (allows for at most 512 tables in the database)
 16-bits for page number
          (allows for at most 65536 pages per file)
 7-bits for slot number
          (allows for at most 128 records per page)

Another possibility is

 to carry details about the current relation around in the code
 use the entire 32-bits of RecordId for page addressing

(Under this scheme, there will be multiple records in the DB with the same rid)


Manipulating RecordIds135/234

Functions for constructing/interrogating RecordIds:
typedef unsigned int RecordId;

RecordId makeRecordId(int file, int page, int slot) {
   return (file << 28) | (page << 8) | (slot);
}
int fileNo(RecordId rid) { return (rid >> 28) & 0xF; }

int pageNo(RecordId rid) { return (rid >> 8) & 0xFFFFF; }

int slotNo(RecordId rid) { return rid & 0xFF; }



... Manipulating RecordIds136/234

Alternative implementation if details of file/page are hidden within PageId:
typedef unsigned int PageId; //only uses 24-bits
typedef unsigned int RecordId;

RecordId makeRecordId(PageId pid, int slot) {
    return (pid << 8) | (slot);
}

int pageId(RecordId rid) { return (rid >> 8) & 0xFFFFFF; }

int slotNo(RecordId rid) { return rid & 0xFF; }



Record Formats137/234

Records are stored within fixed-length pages.

Records may be fixed-length:

 simplifies intra-block space management
(i.e. implementation of insert/delete)
 may waste some (substantial) space

Records may be variable-length:

 complicates intra-block space management
 doesn't waste (as much) space



Fixed-length Records138/234

Encoding scheme for fixed-length records:

 record format (length + offsets) stored in catalogue
 data values stored in fixed-size slots in data pages





Since record format is frequently consulted at query time, it should be memory-resident.



... Fixed-length Records139/234

Advantages of fixed-length records:

 don't need slot directory in page
	  (compute record offset as number × size)
 records are smaller
	  (formatting info stored only once, outside data pages)
 intra-page memory management is simplified
	  (as long as not data overflow)

Disadvantages of fixed-length records:

 need to allocate maximum likely space in every record slot
 leads to (potentially) considerable space wastage
	  (e.g. 40% for string values)


Note: if all records were close to specified maximum size,
this would be the most compact format.



... Fixed-length Records140/234

Handling attempts to insert values larger than available fields:

 simply refuse   (generate DBMS run-time error)
 place oversize data in an overflow page

 field contains a reference to the "overflow page" instead of value
 requires field to be at least as large as a RecordId


Alignment considerations (for numeric fields) may require:

 all records and all fields start on a 4-byte boundary
 thus, varchar fields may be rounded up to nearest 4-bytes



Variable-length Records141/234

Some encoding schemes for variable-length records:

 Prefix each field by length



 Terminate fields by delimeter



 Array of offsets






... Variable-length Records142/234

More encoding schemes for variable-length records:

 Self-describing (e.g. XML)
    <employee>
        <id#>33357462</id#> <dept>0277</dept>
        <name>Neil Young</name>
        <job>Musician</job>
    </employee>

 Java serialization

 serialization converts arbitrary Java objects into byte arrays
 serialize Tuples and use resulting byte arrays as Records
 simplifies progrmming task, but may have extra storage overhead




... Variable-length Records143/234

Advantages of variable-length records:

 minimal wasted space within records
	  (markers,lengths,delimeters)
 more flexibility in managing space within pages

Disadvantages of variable-length records:

 potential for free-space fragmentation within pages
 more complex intra-page space management algorithms



Spanned Records144/234

How to handle record that does not fit into free space in page?

Two approaches:

 waste some space



 span the record between two pages






... Spanned Records145/234

Advantages of spanned records:

 better storage utilisation   (i.e. less wasted space)
 ability to store arbitrarily large records

Disadvantages of spanned records:

 fetching a single record may require multiple page accesses

More common strategy than spanning:

 store large data values outside record in separate file



Converting Records to Tuples146/234

A Record

 is an array of bytes (byte[])
 representing the data values from a typed Tuple

The information on how to interpret the bytes

 may be contained in a schema in the DBMS catalogue
 may be stored the header for the data file
 may be stored partly in the record and partly in a DTD (for XML)

For variable-length records, further formatting information is stored in the record itself.


... Converting Records to Tuples147/234

DBMSs typically define a fixed set of field types for use in schema.

E.g.   DATE, FLOAT, INTEGER, NUMBER(n), VARCHAR(n), ...

This determines the primitive types to be handled in the implementation:


   DATE 
   time_t 


   FLOAT 
   float,double 


   INTEGER 
   int,long 


   NUMBER(n) 
   int[] 


   VARCHAR(n) 
   char[] 




Defining Tuples148/234

To convert a Record to a Tuple we need to know:

 starting location of each field in the byte array
 number of bytes in each field in the byte array
 type of value in each field

This leads to two structs: FieldDesc and RelnDesc
typedef struct {
    short offset;  // index of starting byte
    short length;  // number of bytes
    Types type;    // reference to Type data
} FieldDesc;
typedef struct {
    char     *relname;  // relation name
    ushort    nfields;  // # of fields
    FieldDesc fields[]; // field descriptors
} RelnDesc;



... Defining Tuples149/234

For the example relation:
FieldDesc fields[] = malloc(4*sizeof(FieldDesc);
fields[0] = FieldDesc(0,4,INTEGER);
fields[1] = FieldDesc(4,20,VARCHAR);
fields[2] = FieldDesc(24,10,CHAR);
fields[3] = FieldDesc(34,4,NUMBER);

This defines the schema

 for fixed-length tuples, this describes all tuple instances
 for variable-length tuples, need to compute actual lengths and offsets



... Defining Tuples150/234

A Tuple can be defined as

 a list of field descriptors for a record instance
 along with a reference to the Record data

typedef struct {
    Record    data;     // pointer to data
    ushort    nfields;  // # fields
    FieldDesc fields[]; // field descriptions
} Tuple;



... Defining Tuples151/234

A Tuple is produced from a Record in the context of a RelnDesc.

It also necessary to know how the Record byte-string is structured.

Assume the following Record structure:



Assume also that lengths are 1-byte quantities
	  (no field longer than 256-bytes).


... Defining Tuples152/234

How the Record → Tuple mapping might occur:

Tuple mkTuple(RelnDesc schema, Record record)
{
    int i, pos = 0;
    int size = sizeof(Tuple) +
               (nfields-1)*sizeof(FieldDesc);
    Tuple *t = malloc(size);
    t->data = record;
    t->nfields = schema.nfields;
    for (i=0; i < schema.nfields; i++) {
        int len = record[pos++];
        t->fields[i].offset = pos;
        t->fields[i].length = len;
        // could add checking for over-length fields, etc.
        t->fields[i].type = schema.fields[i].type;
        pos += length;
    }
    return t;
}



PostgreSQL Tuples153/234

Definitions: src/include/access/*tup*.h

Functions: src/backend/access/common/*tup*.c

PostgreSQL defines tuples via:

 a contiguous chunk of memory
 starting with a header giving e.g. #fields, nulls
 followed by the data values (as sequence of Datum)



... PostgreSQL Tuples154/234

Tuple structure:





... PostgreSQL Tuples155/234

Tuple-related data types:

// representation of a data value
// may be the actual value, or may be a pointer to it
typedef unitptr_t Datum;

The actual data value:

 may be stored in the Datum (e.g. int)
 may have a header with length (for varlen attributes)
 may be stored in a TOAST file



... PostgreSQL Tuples156/234

Tuple-related data types: (cont)

typedef struct HeapTupleFields  // simplified
{
    TransactionId   t_xmin;       // inserting xact ID
    TransactionId   t_xmax;       // deleting or locking xact ID
    CommandId       t_cid;        // inserting/deleting command ID, or both
} HeapTupleFields;
typedef struct HeapTupleHeaderData // simplified
{
    HeapTupleFields t_heap;
    ItemPointerData t_ctid;       // current TID of this or newer tuple
    uint16          t_infomask2;  // number of attributes + flags
    uint16          t_infomask;   // flags e.g. has_null, has_varwidth
    uint8           t_hoff;       // sizeof header incl. bitmap+padding
    // above is fixed size (23 bytes) for all heap tuples
    bits8           t_bits[1];    // bitmap of NULLs, variable length
    // actual data follows at end of struct
} HeapTupleHeaderData;



... PostgreSQL Tuples157/234


typedef struct tupleDesc
{
    int                natts;      // number of attributes in the tuple
    Form_pg_attribute *attrs;      // array of pointers to attr descriptors
    TupleConstr       *constr;     // constraints, or NULL if none
    Oid                tdtypeid;   // composite type ID for tuple type
    int32              tdtypmod;   // typmod for tuple type
    bool               tdhasoid;   // tuple has oid attribute in its header
    int                tdrefcount; // reference count, -1 if not counting
} *TupleDesc;



... PostgreSQL Tuples158/234

Operations on Tuples:

// create Tuple from values
HeapTuple
heap_form_tuple(TupleDesc tupDesc, Datum *values, bool *isnull)

// return Datum given Tuple, attr and descriptor
//   sets isnull to true if value is NULL
#define heap_getattr(tup, attnum, tupleDesc, isnull) ...

// returns true if attribute has no value
bool heap_attisnull(HeapTuple tup, int attnum) ...

// produce a modified tuple from an existing one
HeapTuple
heap_modify_tuple(HeapTuple tuple, TupleDesc tupleDesc,
                  Datum *replValues, bool *replIsnull,
                  bool *doReplace)



Page Formats159/234

Ultimately, a Page is simply an array of bytes (byte[]).

We want to interpret/manipulate it as a collection of Records.

Typical operations on Pages:

 get(rid) ... get a record via its TupleId
 first() ... get first record from Page (start scan)
 next() ... fetch next record during a Page scan
 insert(rec) ... add a new record into a Page
 update(rid,rec) ... update value of specified record
 delete(rid) ... remove a specified record from a Page



... Page Formats160/234

Factors affecting Page formats:

 determined by record size flexibility   (fixed, variable)
 how free space within Page is managed
 whether some data is stored outside Page

 does Page have an associated overflow chain?
 are large data values stored elsewhere? (e.g. TOAST)
 can one tuple span multiple Pages?


Implementation of Page operations critically depends on format.


... Page Formats161/234

For fixed-length records, use record slots.

Insertion: place new record in first available slot.

Deletion: two possibilities for handling free record slots:





... Page Formats162/234

Problem with packed format and no slot directory

 records must move around, so rids are not fixed

Could add a slot directory to overcome this, but wastes space.

Problem with unpacked/bitmap format

 records are not allowed to move (rids use absolute offsets)
 using rids to specify offset is more expensive than slot index
	 (e.g. 4KB page requires 12-bit offset (10-bit if word-aligned), 256 slots requires 8-bit index)



... Page Formats163/234

For variable-length records, use slot directory.

Possibilities for handling free-space within block:

 compacted (one region of free space)
 fragmented (distributed free space)

In practice, a combination is useful:

 normally fragmented (cheap to maintain)
 compacted when needed (e.g. record won't fit)



... Page Formats164/234

Compacted free space:
 




Note: "pointers" are implemented as word offsets within block.



... Page Formats165/234

Fragmented free space:
 





Storage Utilisation166/234

How many records can fit in a page?
	   (How long is a piece of string?)

Depends on:   page size, (avg) record size, slot directory, ...

For a typical DBMS application

 a record is 32..256 bytes, a page has 2K bytes
 so each page contains from 10..100 records



... Storage Utilisation167/234

Example of determining space utilisation ...

Assumptions:

 1024-byte (1KB) page size
 records of type (integer,varchar(20),char(10),number(4))
 variable-length records with 4 (1-byte) offsets at start of record
 char(10) field rounded up to 12-bytes to preserve alignment
 maximum size of second field is 20 bytes; average length is 16 bytes
 records start at 4-byte offsets ⇒ 8-bits per directory slot
 page has 4-byte overflow PageId 
	  (other header info?)



... Storage Utilisation168/234

Max record size = 4(offsets) + 4 + 20 + 12 + 4 = 44 bytes

Minimum number of records = 1024/44 = 23
   (assume all max size and no directory)

Average number of records = 1024/40 = 25
   (assume no directory)

So, allow 32 directory slots (5-bit slot indexes), and 32 bytes for directory.

Number of records = Nr,   where   44 × Nr + 32+4 ≤ 1024

Aim to maximise Nr, so Nr = 22

Notes: because there are 32 slots, could have up to 32 (small) records


... Storage Utilisation169/234

If we switched to 8KB pages, then

 directory slots need 11 bits each to address 4-byte-aligned records

Minimum number of records = 8192/44 = 186
   (assume all max size and no directory)

So, allow 256 slots (8-bit slot indexes), and 352 bytes for directory (256*11bits)

Number of records = Nr,   where   44 × Nr + 352 ≤ 8192

Aim to maximise Nr, so Nr = 178

Could reduce size of directory to allow more records ... but only so far.

Note: 11-bit directory entries also means that it's costly to access them.


Overflows170/234

Sometimes, it may not be possible to insert a record into a page:

 no free-space fragment large enough
 overall free-space is not large enough
 the record is larger than the page
 no more free directory slots in page

The first case can initially be handled by compacting the free-space.

If there is still insufficient space, we have one of the other cases.


... Overflows171/234

How the other cases are handled depends on the file organisation:

 records may be inserted anywhere that there is free space

 cases (2) and (4) can be handled by making a new page
 case (3) requires either spanned records or "overflow file"

 record placement is determined by access method (e.g. hashed file)

 case (2) requires an "overflow page"
 case (3) requires an "overflow file"
 case (4) is problematic, since the rid can only address Nr slots




... Overflows172/234

Overflow files for very large records and BLOBs:

 abandon notion of slots and simply access record via offset






... Overflows173/234

Page-based handling of overflows:

 add the PageId of the overflow page to the page header




Useful for scan-all-records type operations.


... Overflows174/234

Record-based handling of overflows:

 store the rid of the overflow record instead of the record itself




Useful for locating specific record via rid.


PostgreSQL Page Representation175/234

Functions: src/backend/storage/page/*.c

Definitions: src/include/storage/bufpage.h

Each page is 8KB (default BLCKSZ) and contains:

 header (free space pointers, flags, xact data)
 array of (offset,length) pairs for tuples in page
 free space region (between array and tuple data)
 actual tuples themselves (inserted from end towards start)
 (optionally) region for special data (e.g. index data)

Large data items are stored in separate (TOAST) files.


... PostgreSQL Page Representation176/234

PostgreSQL tuple page layout:





... PostgreSQL Page Representation177/234

Page-related data types:

// a Page is simply a pointer to start of buffer
typedef Pointer Page;

// indexes into the tuple directory
typedef unit16  LocationIndex;

// entries in tuple directory (line pointer array)
typedef struct ItemIdData
{
   unsigned   lp_off:15,    // tuple offset from start of page
              lp_flags:2,   // state of item pointer
              lp_len:15;    // byte length of tuple
} ItemIdData;



... PostgreSQL Page Representation178/234

Page-related data types: (cont)

typedef struct PageHeaderData
{
   ...
   uint16        pd_flags;    // flag bits (e.g. free, full, ...
   LocationIndex pd_lower;    // offset to start of free space
   LocationIndex pd_upper;    // offset to end of free space
   LocationIndex pd_special;  // offset to start of special space
   uint16        pd_pagesize_version;
   ...
   ItemIdData    pd_linp[1];  // beginning of line pointer array
} PageHeaderData;

typedef PageHeaderData *PageHeader;



... PostgreSQL Page Representation179/234

Operations on Pages:

void PageInit(Page page, Size pageSize, ...)

 initialize a Page buffer to empty page
 in particular, sets pd_lower and pd_upper

OffsetNumber
 PageAddItem(Page page, Item item, Size size, ...)

 insert one tuple into a Page
 fails if: not enough free space, too many tuples

void PageRepairFragmentation(Page page)

 compact tuple storage to give on large free space region



... PostgreSQL Page Representation180/234

PostgreSQL has two kinds of pages:

 heap pages which contain tuples
 index pages which contain index entries

Both kinds of page have the same page layout.

One important difference:

 index entries tend be a smaller than tuples
 can typically fit more index entries per page



Representing Database Objects



Database Objects182/234

RDBMSs manage different kinds of objects

 databases, schemas, tablespaces
 relations/tables, attributes, tuples/records
 constraints, assertions
 views, stored procedures, triggers, rules

Many objects have names (and, in PostgreSQL, all have OIDs).

How are the different types of objects represented?

How do we go from a name (or OID) to bytes stored on disk?


... Database Objects183/234


Top-level "objects" in typical SQL standard databases:

catalog ... SQL terminology for a database

 users connect to a database; sets context for interaction

schema ... collection of DB object definitions

 each schema is defined with a database/catalog
 used for name-space management (Schema.Relation)

tablespace ... collection of DB files

 files contain DB objects from multiple catalog/schemas
 used for file-space management (disk load sharing)

PostgreSQL also has cluster: a server managing a set of DBs.


... Database Objects184/234

Consider what information the RDBMS needs about relations:

 name, owner, primary key of each relation
 name, data type, constraints for each attribute
 authorisation for operations on each relation

Similarly for other DBMS objects
	 (e.g. views, functions, triggers, ...)

All of this information is stored in the system catalog.


(The "system catalog" is also called "data dictionary" or "system view")


In most RDBMSs, the catalog itself is also stored as tables.


... Database Objects185/234

Standard for catalogs in SQL:2003: INFORMATION_SCHEMA.

Schemata(catalog_name, schema_name, schema_owner, ...)

Tables(table_catalog, table_schema, table_name, table_type, ...)

Columns(table_catalog, table_schema, table_name, column_name,
        ordinal_position, column_default, is_nullable, data_type, ...)

Views(table_catalog, table_schema, table_name, view_definition,
                       check_option, is_updatable, is_insertable_into)

Role_table_grants(grantor, grantee, privilege_type, is_grantable,
                         table_catalog, table_schema, table_name, ...)
etc. etc.


For complete details, see Section 30 of the PostgreSQL 8.0.3 documentation.



System Catalog186/234

Most DBMSs also have their own internal catalog structure.

Would typically contain information such as:

Users(id:int, name:string, ...)

Databases(id:int, name:string, owner:ref(User), ...)

Schemas(id:int, name:string, owner:ref(User), ...)

Types(id:int, name:string, defn:string, size:int, ...)

Tables(id:int, name:string, owner:ref(User),
                                inSchema:ref(Schema), ...)

Attributes(id, name:string, table:ref(Table),
                           type:ref(Type), pkey:bool, ...)
etc. etc.


Standard SQL INFORMATION_SCHEMA is provided as a set of views on these tables.



... System Catalog187/234

The catalog is manipulated by a range of SQL operations:

 create Object as Definition
 drop Object ...
 alter Object   Changes
 grant Privilege on Object

where Object is one of table, view, function, trigger, schema, ...

E.g. consider an SQL DDL operation such as:
create table ABC (
    x integer primary key,
    y integer
);



... System Catalog188/234

This would produce a set of catalog changes something like ...
userID := current_user();
schemaID := current_schema();
tabID := nextval('tab_id_seq');
select into intID id
from Types where name='integer';
insert into Tables(id,name,owner,inSchema,...)
  values (tabID, 'abc', userID, schema, ...)
attrID := nextval('attr_id_seq');
insert into Attributes(id,name,table,type,pkey,...)
    values (attrID, 'x', tabID, intID, true, ...)
attrID := nextval('attr_id_seq');
insert into Attributes(id,name,table,type,pkey,...)
    values (attrID, 'y', tabID, intID, false, ...)



... System Catalog189/234

In PostgreSQL, the system catalog is available to users via:

 special commands in the psql shell (e.g. \d)
 SQL standard information_schema 
	(e.g. select * from information_schema.tables;)

The low-level representation is available to sysadmins via:

 a global schema called pg_catalog
 a set of tables/views in that schema (e.g. pg_tables) 



PostgreSQL Catalog190/234

The \d? special commands in psql are just
wrappers around queries on the low-level catalog tables, e.g.




\dt


list information about tables




\dv


list information about views




\df


list information about functions




\dp


list table access privileges




\dT


list information about data types




\dd


shows comments attached to DB objects





... PostgreSQL Catalog191/234

A PostgreSQL installation typically has several databases.

Some catalog information is global, e.g.

 databases, users, ...

 there is one copy of each such table for the whole PostgreSQL installation
 this copy is shared by all databases in the installation
	(lives in PGDATA/pg_global)


Other catalog information is local to each database, e.g

 schemas, tables, attributes, functions, types, ...

 there is a separate copy of each "local" table in each database
 a copy of many "global" tables is made when a new database is created




... PostgreSQL Catalog192/234

Global installation data is recorded in shared tables

 users/groups: pg_authid (pg_shadow), pg_auth_members (pg_group)
 DBs/namespaces: pg_database, pg_namespace

Each kind of DB object has table(s) to describe it, e.g.

 tables: pg_class, pg_attr, pg_constraint, pg_attrdef
 functions: pg_proc, pg_operator, pg_aggregate
 indexes: pg_index, pg_am, pg_amop, pg_amproc



... PostgreSQL Catalog193/234

PostgreSQL tuples contain

 owner-specified attributes (from create table)
 system-defined attributes


oid

unique identifying number for tuple (optional)



tableoid

which table this tuple belongs to



xmin/xmax

which transaction created/deleted tuple (for MVCC)




OIDs are used as primary keys in many of the catalog tables.


Representing Users/Groups194/234

In version 8, PostgreSQL merged notions of users/groups into roles.

Represented by two base tables:   pg_authid,   pg_auth_members

View pg_shadow gives a more symbolic view of pg_authid.

View pg_user gives a copy of pg_shadow with passwords "hidden".

CREATE|ALTER|DROP USER statements modify pg_authid table.

CREATE|ALTER|DROP GROUP statements modify pg_auth_members table.

Both tables are global (shared across all DBs in a cluster).


... Representing Users/Groups195/234

pg_authid table contains information about roles:



oid

unique integer key for this role



rolname

symbolic name for role (PostgreSQL identifier)



rolpassword

plain or md5-encrypted password



rolcreatedb

can create new databases



rolsuper

is a superuser (owns server process)



rolcatupdate

can update system catalogs



etc. etc.


... Representing Users/Groups196/234

pg_shadow view contains information about users:



usename

symbolic user name (e.g. 'jas')



usesysid

integer key to reference user (pg_authid.oid)



passwd

plain or md5-encrypted password



usecreatdb

can create new databases



usesuper

is a superuser (owns server process)



usecatupd

can update system catalogs



etc. etc.



... Representing Users/Groups197/234

pg_group view contains information about user groups:



groname

group name (e.g. 'developers')



grosysid

integer key to reference group



grolist[]

array containing group members 
(vector of refs to pg_authid.oid)



Note the use of multi-valued attribute (PostgreSQL extension)


Representing High-level Objects198/234

Above the level of individual DB schemata, we have:

 databases ... represented by pg_database
 schemas ... represented by pg_namespace
 table spaces ... represented by pg_tablespace

These tables are global to each PostgreSQL cluster.

Keys are names (strings) and must be unique within cluster.


... Representing High-level Objects199/234

pg_database contains information about databases:



datname

database name (e.g. 'mydb')



datdba

database owner (refs pg_authid.oid)



datpath

where files for database are stored  (if not in the PGDATA directory)



datacl[]

access permissions


datistemplate

can be used to clone new databases 
(e.g. template0, template1)



etc. etc.


... Representing High-level Objects200/234

Digression: access control lists (acl)

PostgreSQL represents access via an array of access elements.

Each access element contains:
UserName=Privileges/Grantor
group GroupName=Privileges/Grantor

where Privileges is a string enumerating privileges, e.g.
jas=arwdRxt/jas,fred=r/jas,joe=rwad/jas



... Representing High-level Objects201/234

pg_namespace contains information about schemata:



nspname

namespace name (e.g. 'public')



nspowner

namespace owner (refs pg_authid.oid)



nspacl[]

access permissions



Note that nspname is a key and must be unique across cluster.


... Representing High-level Objects202/234

pg_tablespace contains information about tablespaces:



spcname

tablespace name (e.g. 'disk5')



spcowner

tablespace owner (refs pg_authid.oid)



spclocation

full filepath to tablespace directory



spcacl[]

access permissions




Two pre-defined tablespaces:

 pg_default ... corresponds to PGDATA/base directory
 pg_global ... corresponds to PGDATA/global directory




Representing Tables203/234

Entries in multiple catalog tables are required for each user-level table.

Due to O-O heritage, base table for tables is called pg_class.

The pg_class table also handles other "table-like" objects:

 views ... represents attributes/domains of view
 composite (tuple) types ... from CREATE TYPE AS
 "toast" tables ... for holding over-sized tuples

pg_class also handles sequences, indexes, and other "special" objects.

Tuples in pg_class have an OID, used as primary key.


... Representing Tables204/234

pg_class contains information about tables:



relname

name of table (e.g. employee)



relnamespace

schema in which table defined 
(refs pg_namespace.oid)



reltype

data type corresponding to table 
(refs pg_type.oid)



relowner

owner (refs pg_authid.oid)



reltuples

# tuples in table



relacl

access permissions





... Representing Tables205/234

pg_class also holds various flags/counters for each
table:



relkind

what kind of object 

'r' = ordinary table, 'i' = index, 'v' = view 
'c' = composite type, 'S' = sequence, 's' = special




relnatts

# attributes in table 
(how many entries in pg_attribute table)



relchecks

# of constraints on table 
(how many entries in pg_constraint table)



relhasindex

table has/had an index?



relhaspkey

table has/had a primary key?



etc.



... Representing Tables206/234

pg_type contains information about data types:



typname

name of type (e.g. 'integer')



typnamespace

schema in which type defined 
(refs pg_namespace.oid)



typowner

owner (refs pg_authid.oid)



typtype

what kind of data type 
 'b' = base type, 'c' = complex (row) type, ... 



Note: a complex type is automatically created for each table 
(defines "type" for each tuple in table; also, type for functions returning SETOF)



... Representing Tables207/234

pg_type also contains storage-related information:



typlen

how much storage used for values 
(-1 for variable-length types, e.g. text)



typalign

memory alignment for values 
('c' = byte-boundary, 'i' = 4-byte-boundary, ...)



typrelid

table associated with complex type 
(refs pg_class.oid)



typstorage

where/how values are stored 
('p' = in-tuple, 'e' = in external table,   compressed?)




(We discuss more details of the pg_type table later ...)



... Representing Tables208/234

pg_attribute contains information about attributes:



attname

name of attribute (e.g. 'empname')



attrelid

table this attribute belongs to 
(refs pg_class.oid)



attnum

attribute position (1..n, sys attrs are -ve)



atttypid

data type of this attribute 
(refs pg_type.oid)



(attrelid,attnum) is unique, and used as primary key.



... Representing Tables209/234

pg_attribute also holds storage-related information:



attlen

storage space required by attribute 
(copy of pg_type.typlen for fixed-size values)



atttypmod

storage space for var-length attributes 
(e.g. 6+ATTR_HEADER_SIZE for char(6))



attalign

memory-alignment info (copy of pg_type.typalign)



attndims

number of dimensions if attr is an array






... Representing Tables210/234

pg_attribute also holds constraint/status information:



attnotnull

attribute may not be null?



atthasdef

attribute has a default values 
(value is held in pg_attrdef table)



attisdropped

attribute has been dropped from table



Also has notion of large data being stored in a separate table
(so-called "TOAST" table).



... Representing Tables211/234

An SQL DDL statement like
create table MyTable (
   a int unique not null,
   b char(6)
);

will cause entries to be made in the following tables:

 pg_class ... one tuple for the table as a whole
 pg_attribute ... one tuple for each attribute
 pg_type ... one tuple for the row-type




... Representing Tables212/234

The example leads to a series of database changes like

rel_oid := new_oid(); user_id = current_user();
insert into
   pg_class(oid,name,owner,kind,pages,tuples,...)
   values (rel_oid, 'mytable', user_id, 'r', 0, 0, ...)
select oid,typlen into int_oid,int_len
from   pg_type where typname = 'int';
insert into
   pg_attribute(relid,name,typid,num,len,typmod,notnull...)
   values (rel_oid, 'a', int_oid, 1, int_len, -1, true, ...)
select oid,typlen into char_oid,char_len
from   pg_type where typname = 'char';
insert into
   pg_attribute(relid,name,typid,num,len,typmod,notnull...)
   values (rel_oid, 'b', char_oid, 2, -1, 6+4, false, ...)
insert into
   pg_type(name,owner,len,type,relid,align,...)
   values ('mytable', user_id, 4, 'c', rel_oid, 'i', ...)




... Representing Tables213/234

pg_attrdef contains information about default values:



adrelid

table that column belongs to 
(refs pg_class.oid)



adnum

which column in the table 
(refs pg_attribute.attnum)



adsrc

readable representation of default value



adbin

internal representation of default value





... Representing Tables214/234

pg_constraint contains information about constraints:



conname

name of constraint (not unique)



connamespace

schema containing this constraint



contype

kind of constraint 

'c' = check, 'u' = unique, 
'p' = primary key, 'f' = foreign key




conrelid

which table (refs pg_class.oid)



conkey

which attributes 
(vector of values from pg_attribute.attnum)



consrc

check constraint expression




(Names are automatically generated from context (fkey,check) if not supplied)



... Representing Tables215/234

For foreign-key constraints, pg_constraint also contains:



confrelid

referenced table for foreign key



confkey

key attributes in foreign table




conkey

corresponding attributes in local table



Foreign keys also introduce triggers to perform checking.

For column-specific constraints:


consrc

readable check constraint expression



conbin

internal check constraint expression






... Representing Tables216/234

An SQL DDL statement like
create table MyOtherTable (
   x int check (x > 0),
   y int references MyTable(a),
   z int default -1
);

will cause similar entries as before in catalogs, plus

 pg_constraint ... one tuple for x and y
 pg_attrdef ... one tuple for z default




... Representing Tables217/234

The example leads to a series of database changes like

rel_oid := new_oid(); user_id = current_user();
insert into
   pg_class(oid,name,owner,kind,pages,tuples,...)
   values (rel_oid, 'myothertable', user_id, 'r', 0, 0, ...)
select oid,typlen into int_oid,int_len
from   pg_type where typname = 'int';
select oid into old_oid
from   pg_class where relname='mytable';
-- pg_attribute entries for attributes x=1, y=2, z=3
insert into
   pg_attrdef(relid,num,src,bin)
   values (rel_oid, 3, -1, {CONST :...})
insert into
   pg_constraint(type,relid,key,src,...)
   values ('c', rel_oid, {1}, '(x > 0)', ...)
insert into
   pg_constraint(type,relid,key,frelid,fkey,...)
   values ('f', rel_oid, {2}, old_oid, {1}, ...)




Representing Functions218/234

Stored procedures (functions) are defined as

create function power(int x, int y) returns int
as $$
declare  i int;  product int := 1;
begin
   for i in 1..y loop
      product := product * x;
   end loop;
   return product;
end;
$$ language plpgsql;

Stored procedures are represented in the catalog via

 an entry in the pg_proc table
 with references to pg_type table for signature




... Representing Functions219/234

pg_proc contains information about functions:



proname

name of function (e.g. substr)



pronamespace

schema in which function defined 
(refs pg_namespace.oid)



proowner

owner (refs pg_authid.oid)



proacl[]

access permissions



etc.



... Representing Functions220/234

pg_proc also contains argument/usage information:



pronargs

how many arguments



prorettype

return type (refs pg_type.oid)



proargtypes[]

argument types (ref pg_type.oid vector)



proreset

returns set of values of prorettype



proisagg

is function an aggregate?



proisstrict

returns null if any arg is null



provolatile

return value depends on side-effects? 
('i' = immutable, 's'= stable, 'v' = volatile)






... Representing Functions221/234

pg_proc also contains implementation information:



prolang

what language function written in



prosrc

source code if interpreted (e.g. PLpgSQL)



probin

additional info on how to invoke function 
(interpretation is language-specific)






... Representing Functions222/234

Consider two alternative ways of defining a x2 function.

sq.c  int square_in_c(int x) { return x * x; }

create function square(int) returns int
as '/path/to/sq.o', 'square_in_c' language 'C';

or

create function square(int) returns int
as $$
begin
    return $1 * $1;
end;
$$ language plpgsql;




... Representing Functions223/234

The above leads to a series of database changes like

user_id := current_user();
select oid,typlen into int_oid,int_len
from   pg_type where typname = 'int';
insert into
   pg_proc(name,owner,rettype,nargs,argtypes,
           prosrc,probin...)
   values ('square', user_id, int_oid, 1, {int_oid},
           'square_in_c', '/path/to/sq.o', ...)
-- or
insert into
   pg_proc(name,owner,rettype,nargs,argtypes,
           prosrc,probin...)
   values ('square', user_id, int_oid, 1, {int_oid},
           'begin return $1 * $1; end;', '-', ...)



... Representing Functions224/234

Users can define their own aggregate functions (like max()).

Requires definition of three components:

 state to accumulate partial values during the scan
 update function to maintain state after each tuple
 output function to return the final acucmulated result

This information is stored in the pg_aggregate catalog.

The aggregate's name is stored in the pg_proc catalog.


... Representing Functions225/234

Consider defining your own average() function

Need to define a new aggregate:
create aggregate average (
   basetype   = integer,
   sfunc      = int_avg_accum,
   stype      = int[],
   finalfunc  = int_avg_result,
   initcond   = '{0,0}'
);

and need to define functions to support aggregate ...


... Representing Functions226/234

create function
       int_avg_accum(state int[], int) returns int[]
as $$
declare res int[2];
begin
   res[1] := state[1] + $2; res[2] := res[2] + 1;
   return res;
end;
$$ language plpgsql;

create function
       int_avg_result(state int[]) returns int
as $$
begin
   if (state[2] = 0) then return null; end if;
   return (state[1] / state[2]);
end;
$$ language plpgsql;




... Representing Functions227/234

Users can define their own operators to use in expressions.

Operators are syntactic sugar for unary/binary functions.

Consider defining an operator for the power(x,y) function:
create operator ** (
   procedure = power, leftarg = int, rightarg = int
);

-- which can be used as
select 4 ** 3;
-- giving a result of 64

Operator definitions are stored in pg_operator catalog.



Representing Types228/234

Users can also define new data types, which includes

 data structures for objects of the type
 type-specific functions, aggregates, operators
 type-specific indexing (access) methods

Consider defining a 3-dimensional point type for spatial data:
create type point3d (
   input = point3d_in,    -- function to parse values
   output = point3d_out,  -- function to display values
   internallength = 24,   -- space for three float8's
   alignment = double     -- align tuples properly
);



... Representing Types229/234

pg_type additional fields for user-defined types:



typinput

text input conversion function



typoutput

text output conversion function




typreceive

binary input conversion function



typsend

binary output conversion function



All attributes are references to pg_proc.oid


... Representing Types230/234

All data types need access methods for querying.

The following catalogs tables are involved in this:

 pg_am ... main definition of access method
 pg_opclass ... access operator classes
 pg_amop ... operators for indexed access
 pg_amproc ... support procedures for AM



... Representing Types231/234

pg_am holds information about access methods:



amname

name of access method (e.g. btree)



amowner

owner (refs pg_authid.oid)



amorderstrategy

operator for determining sort order  (0 if unsorted)



amcanunique

does AM support unique indexes?



ammulticol

does AM support multicolumn indexes?



amindexnulls

does AM support NULL index entries?



amconcurrent

does AM support concurrent updates?





... Representing Types232/234

pg_am also contains links to access functions:



amgettuple

"next valid tuple" function



ambeginscan

"start new scan" function



amrescan

"restart this scan" function



amendscan

"end this scan" function



amcostestimate

estimate cost of index scan



All attributes are references to pg_proc.oid

Functions drive the query evaluation process.


... Representing Types233/234

pg_am also contains links to update functions:



aminsert

"insert this tuple" function



ambuild

"build new index" function



ambulkdelete

bulk delete function



amvacuumcleanup

post-vacuum cleanup function 



All attributes are references to pg_proc.oid

Functions implement different aspects of updating data/index files.


... Representing Types234/234

Built-in access methods:

 heap ...
	simple sequence of pages, sequential access
 btree ...
	ordered access by key, Lehman-Yao version
 hash ...
	assiocative access, Litwin's linear hashing
 rtree ...
	spatial data index, quadratic split version
 GiST ...
    generalised tree indexes (e.g. B-trees, R-trees)
 SP-GiST ...
    space-partitioned search trees (e.g. k-d trees)
 GIN ...
    generalised inverted index (e.g. (key,docs) pairs)

Some access methods introduce additional files (e.g. B-tree)

Produced: 29 Feb 2016</p><h3 >字段3</h3><p>Scan, Sort, Project


Implementing Relational Operations



Relational Operations2/90

DBMS core = relational engine, with implementations of

 selection,   projection,   join,   set operations
 scanning,   sorting,   grouping,   aggregation,   ...

In this part of the course:

 examine methods for implementing each operation
 develop cost models for each implementation
 characterise when each method is most effective



... Relational Operations3/90

Implementation of relational operations in DBMS:





... Relational Operations4/90

All relational operations return a set of tuples.

Can represent a typical operation programmatically as:
ResultSet = {}  // initially an empty set
while (t = nextRelevantTuple()) {
   // format tuple according to projection
   t' = formatResultTuple(t,Projection)
   // add next relevant tuple to result set
   ResultSet = ResultSet ∪ t'
}
return ResultSet

All of the hard work is in the nextRelevantTuple() function.


... Relational Operations5/90

nextRelevantTuple() for selection operator:

 find next possible result tuple in table
 check whether it satisfies selection condition

nextRelevantTuple() for join operator:

 find next possible pair of tuples from tables
 check whether pair satisfies join condition

Two ways to handle the ResultSet

 build the complete ResultSet and then return it
 return each tuple as produced (tuple-by-tuple interface)



... Relational Operations6/90

There are three "dimensions of variation" in this system:

 relational operators   (e.g. Sel, Proj, Join, Sort, ...)
 file structures   (e.g. heap, indexed, hashed, ...)
 query processing methods   (e.g. merge-sort, hash-join, ...)

We consider combinations of these, e.g.

 selection with 0/1 matching tuples on hashed/indexed file
 sort-merge join on ordered heap files
 2-dimensional range query on an R-tree-indexed file

Also consider updates (insert/delete) on file structures.


Query Types7/90

Queries fall into a number of classes:



 Type 
 SQL 
 RelAlg 
 a.k.a. 


 Scan 
 select * from R 
 R 
 - 


 Proj 
 select x,y from R 
 Proj[x,y]R 
 - 


 Sort 
 select * from R  order by x
 Sort[x]R 
 ord 




Different query classes exhibit different query processing behaviours.



... Query Types8/90




 Type 
 SQL 
 RelAlg 
 a.k.a. 


 Sel1 
 select * from R  where id = k
 Sel[id=k]R 
 one 


 Seln 
 select * from R  where a = k
 Sel[a=k]R 
 - 


 Selpmr 
 select * from R  where a=j and b=k
 Sel[a=j ∧ b=k]R 
 pmr 


 Range1d 
 select * from R  where a>j and a<k
 Sel[a>j ∧ a<k]R 
 rng 


 Rangend 
 select * from R  where a>j and a<k      and b>m and b<n
 Sel[...]R 
 space 





... Query Types9/90




 Type 
 SQL 
 RelAlg 
 a.k.a. 


 Join1 
 select * from R,S  where R.id = S.r
 R Join[id=r] S 
 - 


 EquiJoin 
 select * from R,S  where R.v=S.w and R.x=S.y
 R Join[v=w ∧ x=y] S 
 - 


 ThetaJoin 
 select * from R,S  where R.x op S.y
 R Join[...] S 
 - 


 Similar 
 select * from R  where R.* ≅ Object
 R ≅ Obj 
 sim 





Cost Models



Cost Models11/90

An important aspect of this course is

 analysis of cost of various query methods

Won't be using asymptotic complexity (O(n)) for this

Rather, we attempt to develop cost models

 for a each query method, over a range of query types
 using a (simplified) model of the behaviour of the DBMS

Cost is measured in terms of number of page reads/writes.


... Cost Models12/90

Assumptions in our cost models:

 memory (RAM) is "small", fast, byte-at-a-time


 e.g. 1GB size, 10-7 secs to compare tuples
 all computation is performed on data loaded into memory


 disk storage is very large, slow, page-at-a-time


 e.g. 1TB size, 10-2 secs to read/write a 4KB page
 cost of processing a page is 10-3 cost of reading a page


 every request to read/write a page results in a read/write


 no effective buffer-pooling ... 1 memory buffer per relation
 however, we sometimes consider multiple buffers explicitly





... Cost Models13/90

In developing cost models, we also assume:

 a relation is a set of r tuples, with average size R bytes
 the tuples are stored in b data pages on disk
 each page has size B bytes and contains up to c tuples
 the tuples which answer query q are contained in bq pages
 data is transferred disk↔memory in whole pages
 cost of disk↔memory transfer Tr/w is highest cost in system







... Cost Models14/90

Typical values for measures used in cost models:




 Quantity 
 Symbol 
 E.g. Value 


 total # tuples 
 r 
 106 


 record size 
 R 
 128 bytes 


 total # pages 
 b 
 105 


 page size 
 B 
 8192 bytes 


 # tuples per page 
 c 
 60 


 page read/write time 
 Tr,Tw 
 10 msec 


 process page in memory 
 - 
 ≅ 0 


 # pages containing  answers for query q 
 bq 
 ≥ 0 




Example file structures15/90

When describing file structures

 use a large box to represent a page
 sometimes use a small box to represent a tuple
 sometimes refer to tuples as reci
 sometimes ref to tuples via their key

 mostly, key corresponds to the notion of "primary key"
 sometimes, key means "search key" in selection condition







... Example file structures16/90

Consider three simple file structures:

 heap file ... tuples added to any page which has space
 sorted file ... tuples arranged in file in key order
 hash file ... tuples placed in pages using hash function

All files are composed of b primary blocks/pages



Some records in each page may be marked as "deleted".


... Example file structures17/90

Heap file with b = 4, c = 4:





... Example file structures18/90

Sorted file with b = 4, c = 4:





... Example file structures19/90

Hashed file with b = 3, c = 4, h(k) = k%3





... Example file structures20/90

Indexed file with b = 4, c = 4, bi = 2, ci = 8:





Scanning



Scanning22/90

Consider the query:
select * from T;

Conceptually:
for each tuple t in relation T {
   add tuple t to result set
}






... Scanning23/90

Implemented via iteration over file containing T:
for each page P in file of relation T {
   for each tuple t in page P {
      add tuple t to result set
   }
}

Cost: read every data page once

Cost = b.Tr


... Scanning24/90

In terms of file operations:
// implementation of "select * from T"

File inf;   // data file handle
int p;      // input file page number
Buffer buf; // input file buffer
int i;      // current record in input buf
Tuple t;    // data for current record

inf = openFile(fileName("T"), READ)
for (p = 0; p < nPages(inf); p++) {
    buf = readPage(inf,p);
    for (i = 0; i < nTuples(buf); i++) {
        t = getTuple(buf,i);
        add t to result set
}   }



... Scanning25/90

Scan implementation when file has overflow pages, e.g.





... Scanning26/90

In this case, the implementation changes to:
for each page P in file of relation T {
    for each tuple t in page P {
        add tuple t to result set
    }
    for each overflow page V of page P {
        for each tuple t in page V {
            add tuple t to result set
}   }   }

Cost: read each data and overflow page once

Cost = (b + bOv).Tr

where bOv = total number of overflow pages


... Scanning27/90

In terms of file operations:
// implementation of "select * from T"

File inf;   // data file handle
File ovf;   // overflow file handle
int p;      // input file page number
int ovp;    // overflow file page number
Buffer buf; // input file buffer
int i;      // current record in input buf
Tuple t;    // data for current record

inf = openFile(fileName("T"), READ)
ovf = openFile(ovFileName("T"), READ)
for (p = 0; p < nPages(inf); p++) {
    buf = readPage(inf,p);
    for (i = 0; i < nTuples(buf); i++) {
        t = getTuple(buf,i);
        add t to result set
    }
    ovp = ovflow(buf);
    while (ovp != NO_PAGE) {
        buf = readPage(ovf,ovp);
        for (i = 0; i < nTuples(buf); i++) {
            t = getTuple(buf,i);
            add t to result set
        }
        ovp = ovflow(buf);
    }
}

Cost: read data+ovflow page    Cost = (b+bov).Tr


Selection via Scanning28/90

Consider a one query like:
select * from Employee where id = 762288;

In an unordered file, search for matching record requires:





Guaranteed at most one answer; could be in any page.


... Selection via Scanning29/90

In terms of file operations (assuming var delcarations as before):
inf = openFile(fileName("Employee"), READ);
for (p = 0; p < nPages(inf); p++)
    buf = readPage(inf,p);
    for (i = 0; i < nTuples(buf); i++) {
        t = getTuple(buf,i);
        if (getField(t,"id") == 762288)
           return t;
}   }


For different selection condition, simply replace  
(getField(t,"id")==762288)



... Selection via Scanning30/90

Cost analysis for one searching in unordered file

 best case: read one page, find record
 worst case: read all b pages, find in last (or don't find)
 average case: read half of the pages (b/2)

Assumptions:

 negligible cost for scanning tuples in page
 negligible cost for checking condition on each record

Costavg = Trb/2   
Costmin = Tr   
Costmax = Trb


File Copying31/90

Consider an SQL statement like:
create table T as (select * from S);

Effectively, copies data from one file to another.




Conceptually:
make empty relation T
for each tuple t in relation S {
    append tuple t to relation T
}



... File Copying32/90

In terms of file operations:
File inf,outf; // input/output file handles
int ip,op;     // input/output page numbers
int i;         // record number in input buf
Tuple t;       // current record
Buffer buf;    // input file buffer
Buffer obuf;   // output file buffer

inf = openFile(fileName("S"), READ);
outf = openFile(fileName("T"), CREATE);
clear(obuf);
for (ip = op = 0; ip < nPages(inf); ip++) {
    buf = readPage(inf, ip);
    for (i = 0; i < nTuples(buf); i++) {
        t = getTuple(i, buf);
        addTuple(t, obuf);
        if (isFull(obuf)) {
            writePage(outf,op++,obuf);
            clear(obuf);
}   }   }
if (nTuples(obuf) > 0)
    writePage(outf,op,obuf);



... File Copying33/90

Cost analysis for file copying:

 bin = number of pages in input file 
 bout = number of pages in output file 
 bout ≤ bin
	   (less if there were "tombstones" in input file)
 read bin pages and
	write bout pages

Cost:   binTr + boutTw
   or    T.(bin + bout )
          (assuming T = Tr = Tw)



Iterators34/90

Higher-levels of DBMS are given a view of scanning as:
cursor = initScan(relName,condition);
while (tup = getNextTuple(cursor)) {
	process tup
}
endScan(cursor);

Also known as iterator.


... Iterators35/90

Implementation of simple scan iterator (via file operations):
typedef struct {
    File   inf;   // data file handle
    Buffer buf;   // input buffer
    int    curp;  // current page number
    int    curi;  // current record number
    Expr   cond;  // representation of condition
} Cursor;



... Iterators36/90

Implementation of simple scan iterator (continued):
Cursor *initScan(char *rel, char *cond)
{
    Cursor *c;
    c = malloc(sizeof(Cursor));
    c->inf = openFile(fileName(rel),READ);
    c->buf = readPage(c->inf,0);
    c->curp = 0;
    c->curi = 0;
    c->cond = makeTestableCondition(cond);
    return c;
}
void endScan(Course *c)
{
    closeFile(c->inf);
    freeExpr(c->cond);
    free(c);
}



... Iterators37/90

Implementation of simple scan iterator (continued):
Tuple getNextTuple(Cursor *c)
{
getNextTuple:
    if (c->curi < nTuples(c->buf))
        return getTuple(c->buf, c->curi++);
    else {
        // no more tuples in this page; get next page
        c->curp++;
        if (c->curp == nPages(c->inf))
            return NULL;  // no more pages
        else {
            c->buf = readPage(c->inf,c->curp);
            c->curi = 0;
            goto getNextTuple;
        }
    }
}



... Iterators38/90

Implementation of full iterator interface via file operations:
typedef struct {
    File   inf;   // data file handle
    File   ovf;   // overflow file handle
    Buffer buf;   // input buffer
    int    curp;  // current page number
    int    curop; // current ovflow page number
    int    curi;  // current record number
    Expr   cond;  // representation of condition
} Cursor;



... Iterators39/90

Implementation of full iterator interface (continued):
Cursor *initScan(char *rel, char *cond)
{
    Cursor *c;

    c = malloc(sizeof(Cursor));
    c->inf = openFile(fileName(rel),READ);
    c->ovf = openFile(ovFileName(rel),READ);
    c->buf = readPage(c->inf,0);
    c->curp = 0;
    c->curop = NO_PAGE;
    c->curi = 0;
    c->cond = makeTestableCondition(cond)
    return c;
}
void endScan(Course *c)
{
    closeFile(c->inf);
    if (c->ovf) closeFile(c->ovf);
    freeExpr(c->cond);
    free(c);
}



... Iterators40/90

Implementation of scanning interface (continued):

Tuple getNextTuple(Cursor *c)
{
getNextTuple:
    if (c->curi < nTuples(c->buf))
        return getTuple(c->buf, c->curi++);
    else {
        // no more tuples in this page; get next page
        if (c->curop == NO_PAGE) {
            c->curop = ovflow(c->buf);
            if (c->curop != NO_PAGE) {
                // start ovflow chain scan
getNextOvPage:
                c->buf = readPage(c->ovf,c->curop);
                c->curi = 0;
                goto getNextTuple;
            }
            else {
getNextDataPage:
                c->curp++;
                if (c->curp == nPages(c->inf))
                    return NULL;  // no more pages
                else {
                    c->buf = readPage(c->inf,c->curp);
                    c->curi = 0;
                    goto getNextTuple;
                }
            }
        }
        else {
            // continue ovflow chain scan
            c->curop = ovflow(c->buf);
            if (c->curop == NO_PAGE)
                goto getNextDataPage;
            else
                goto getNextOvPage;
        }
    }
}



Scanning in PostgreSQL41/90

Scanning defined in: /backend/access/heap/heapam.c

Implements iterator data/operations:

 HeapScanDesc ... struct containing iteration state
 scan = heap_beginscan(rel,...,nkeys,keys) 
	(uses initscan() to do half the work (shared with rescan))
 tup = heap_getnext(scan, direction) 
	(uses heapgettup() to do most of the work)
 heap_endscan(scan) ... frees up scan struct
 HeapKeyTest() ... implements key match test



... Scanning in PostgreSQL42/90

typedef struct HeapScanDescData
{
  // scan parameters 
  Relation      rs_rd;        // heap relation descriptor 
  Snapshot      rs_snapshot;  // snapshot ... tuple visibility 
  int           rs_nkeys;     // number of scan keys 
  ScanKey       rs_key;       // array of scan key descriptors 
  ...
  // state set up at initscan time 
  PageNumber    rs_npages;    // number of pages to scan 
  PageNumber    rs_startpage; // page # to start at 
  ...
  // scan current state, initally set to invalid 
  HeapTupleData rs_ctup;      // current tuple in scan
  PageNumber    rs_cpage;     // current page # in scan
  Buffer        rs_cbuf;      // current buffer in scan
   ...
} HeapScanDescData;



Scanning in other File Structures43/90

Above examples are for heap files

 simple, unordered, no index, no hashing

Other access file structures in PostgreSQL:

 btree, hash, gist, gin
 each implements:

 startscan, getnext, endscan
 insert, delete
 other file-specific operators




Sorting



The Sort Operation45/90

Sorting is explicit in queries only in the order by clause
select * from Students order by name;

More important, sorting is used internally in other operations:

 eliminating duplicate tuples for project
 ordering files to enhance select efficiency
 implementing various styles of join
 forming tuple groups in group by



External Sorting46/90

Sort methods such as quicksort are designed for in-memory data.

For data on disks, need external sorting techniques.

The standard external sorting method (merge sort) works by

 reading pages of data into memory buffers
 use in-memory sort to order items within buffers
 merging sorted buffers to produce output
 possibly requiring multiple passes over the data



... External Sorting47/90

Sorting tuples within pages

 need to extract sort key from each tuple
 no need to physically move tuples
 simply swap entries in page directory






Two-way Merge Sort48/90

Requires three in-memory buffers:




Assumption: cost of merge on two buffers ≅ 0.


... Two-way Merge Sort49/90

Two-way merge-sort method:
read each page into buffer, sort it, write it
numberOfRuns = b; runLength = 1;
while (numberOfRuns > 1) {
    for each pair of adjacent runs {
        merge the pair of runs to output, by
          - read pages from runs into input
              buffers, one page at a time
          - apply merge algorithm to transfer
              tuples to output buffer
          - flush output buffer when full and
              when merge finished
   }
   numberOfRuns = numberOfRuns / 2
   runLength = runLength * 2
}



... Two-way Merge Sort50/90

Example:





... Two-way Merge Sort51/90

Two-way merge-sort method (improved):
numberOfRuns = b; runLength = 1;
while (numberOfRuns > 1) {
    for each pair of adjacent runs {
        merge the pair of runs to output, by
          - read pages from runs into input
              buffers, one page at a time
          - if (runLength == 1)
              sort contents of each input buffer
          - apply merge algorithm to transfer
              tuples to output buffer
          - flush output buffer when full and
              when merge finished
   }
   numberOfRuns = numberOfRuns / 2
   runLength = runLength * 2
}

Avoids first pass to sort contents of individual pages.


... Two-way Merge Sort52/90

Consider file where b = 2k:

 pass 0 produces 2k sorted runs of 1 page
 pass 1 produces 2k-1 sorted runs of 2 pages
 pass 2 produces 2k-2 sorted runs of 4 page
 and so on, until
 pass k produces 1 sorted run of 2k pages

Method also works ok when

 b != 2k ... last run simply has less pages than others
 pages are not completely full   (nextTuple() function)



... Two-way Merge Sort53/90

Example:





Merging Two Sorted Pages54/90

Method using operations on files and buffers:

// Pre:  buffers B1,B2; outfile position op
// Post: tuples from B1,B2 output in order
i1 = i2 = 0; clear(Out);
R1 = getTuple(B1,i1); R2 = getTuple(B2,i2);
while (i1 < nTuples(B1) && i2 < nTuples(B2)) {
    if (lessThan(R1,R2))
        { addTuple(R1,Out); i1++; R1 = getTuple(B1,i1); }
    else
        { addTuple(R2,Out); i2++; R2 = getTuple(B2,i2); }
    if (isFull(Out))
        { writePage(outf,op++,Out); clear(Out); }
}
for (i1=i1; i1 < nTuples(B1); i1++) {
    addTuple(getTuple(B1,i1), Out);
    if (isFull(Out))
        { writePage(outf,op++,Out); clear(Out); }
}
for (i2=i2; i2 < nTuples(B2); i2++) {
    addTuple(getTuple(B2,i2), Out);
    if (isFull(Out))
        { writePage(outf,op++,Out); clear(Out); }
}
if (nTuples(Out) > 0) writePage(outf,op,Out);



Merging Runs vs Merging Pages55/90

In the above, we merged two input buffers.

In general, we need to merge sorted "runs" of pages.

The only difference that this makes to the above method:
R1 = getTuple(B1,i1);

becomes
if (i1 == nTuples(B1)) {
    B1 = readPage(inf,ip++); i1 = 0;
}
R1 = getTuple(B1,i1);



Comparison for Sorting56/90

Above assumes that we have a function to compare tuples.

Mechanism needs to be generic, to handle all of:
select * from Employee order by eid;
select * from Employee order by name;
select * from Employee order by age;

Envisage a function tupCompare(r1,r2,f) (cf. C's strcmp)

 takes two tuples r1, r2 and a field name f
 returns negative value if r1.f < r2.f
 returns positive value if r1.f > r2.f
 returns zero value if r1.f == r2.f



... Comparison for Sorting57/90

SQL allows sorting based on multiple fields, e.g.
select * from Employee order by name,age;

How to specify sort condition for several attributes?

E.g. sort key is (A1,A2,..An)

int tupCompare(Tuple r1, Tuple r2, SortKeys A)
{
         if (r1.A1 != r2.A1) return(r1.A1 - r2.A1);
    else if (r1.A2 != r2.A2) return(r1.A2 - r2.A2);
    else if ...
    ...
    else if (r1.An != r2.An) return(r1.An - r2.An);
    else return 0;
}


Assume that ri.Ai is a method for extracting value of Aith attribute of record ri



Cost of Two-way Merge Sort58/90

For a file containing b data pages:

 require ⌈log2b⌉ passes to sort,
 each pass requires b page reads, b page writes

Gives total cost:   2.b.⌈log2b⌉

Example: Relation with r=105 and c=50
	  ⇒   b=2000 pages.

Number of passes for sort:   ⌈log22000⌉  =  11

Reads/writes entire file 11 times!    Can we do better?


n-Way Merge Sort59/90

Merge passes use:
	  B total memory buffers,
	  n input buffers,
	  B-n output buffers





Typically, consider only one output buffer, i.e. B = n + 1

Note that initial sort pass uses all B buffers ..

 read B pages into memory buffers
 sort tuples across all B pages in memory
 write out B-page-long run of sorted tuples



... n-Way Merge Sort60/90

Method:

// Produce B-page-long runs
for each group of B pages in Rel {
    read pages into memory buffers
    sort group in memory
    write pages out to Temp
}
// Merge runs until everything sorted
// n-way merge, where n=B-1
numberOfRuns = ⌈b/B⌉
while (numberOfRuns > 1) {
    for each group of n runs in Temp {
        merge into a single run via input buffers
        write run to newTemp via output buffer
    }
    numberOfRuns = ⌈numberOfRuns/n⌉
    Temp = newTemp // swap input/output files
}



... n-Way Merge Sort61/90

Method for merging n runs (n input buffers, 1 output buffer):

for i = 1..n {
   read first page of run[i] into a buffer[i]
   set current tuple cur[i] to first tuple in buffer[i]
}
while (more than 1 run still has tuples) {
   s = find buffer with smallest tuple as cur[i]
   copy tuple cur[i] to output buffer
   if (output buffer full) { write it and clear it}
   advance cur[i] to next tuple
   if (no more tuples in buffer[i]) {
      if (no more pages in run[i])
         mark run[i] as complete
      else {
         read next page of run[i] into buffer[i]
         set cur[i] to first tuple in buffer[i]
}  }  }
copy tuples in non-empty buffer to output



Cost of n-Way Merge Sort62/90

Consider file where b = 4096, B = 16 total buffers:

 pass 0 produces 256 × 16-page sorted runs
 pass 1 produces 18 × 240-page sorted runs
 pass 2 produces 2 × 3600-page sorted run
 pass 3 produces 1 × 4096-page sorted run

(cf. two-way merge sort which needs 11 passes)

For b data pages and n=15 input buffers (15-way merge)

 first pass: read/writes b pages, gives b0 = ⌈b/B⌉ runs
 then need ⌈lognb0⌉ passes until sorted
 each pass reads and writes b pages (2.b )

Cost = 2.b.(1 + ⌈lognb0⌉),   where b0 = ⌈b/B⌉


... Cost of n-Way Merge Sort63/90

Costs (number of passes) for varying b and B (n=B-1):




 b 
 B=3 
 B=16 
 B=128 


 100 
 7 
 2 
 1 


 1000 
 10 
 3 
 2 


 10,00 
 13 
 4 
 2 


 100,000 
 17 
 5 
 3 


 1,000,000 
 20 
 5 
 3 



 

In the above, we assume that

 the first pass uses all B buffers as inputs
 subsequent merging passes use n=B-1 input buffers, and one output buffer

Elapsed time could be reduced by double-buffering

 fill one output buffer while the other is being flushed to disk
 but this needs two output buffers => n-1-way merging, so maybe more merge passes



Sorting in PostgreSQL64/90

Sort uses a polyphase merge-sort (from Knuth):

 backend/utils/sort/tuplesort.c

Tuples are mapped to SortTuple structs for sorting:

 containing pointer to tuple and sort key
 no need to reference actual Tuples during sort
 unless multiple attributes used in sort

If all data fits into memory, sort using qsort().

If memory fills while reading, form "runs" and do disk-based sort.


... Sorting in PostgreSQL65/90

Disk-based sort has phases:

 divide input into sorted runs using HeapSort
 merge using seven N buffers, one output buffer
 N = as many buffers as workMem allows

Many references to "tapes" since Knuth's original algorithm
was described in terms of merging data from magnetic tapes.

Effectively, a "tape" is a sorted run.

Implementation of "tapes": backend/utils/sort/logtape.c


... Sorting in PostgreSQL66/90

Sorting is generic and comparison operators are defined in catalog:
// gets pointer to function via pg_operator
SelectSortFunction(Oid sortOperator,
                   bool nulls_first,
                   Oid *sortFunction,
                   int *sortFlags);

// returns negative, zero, positive
ApplySortFunction(FmgrInfo *sortFunction,
                  int sortFlags,
                  Datum datum1, bool isNull1,
                  Datum datum2, bool isNull2);

Flags indicate: ascending/descending, nulls-first/last.


Projection



The Projection Operation68/90

Consider the query:
select distinct name,age from Employee;

If the Employee relation has four tuples such as:
(94002, John, Sales, Manager,   32)
(95212, Jane, Admin, Manager,   39)
(96341, John, Admin, Secretary, 32)
(91234, Jane, Admin, Secretary, 21)

then the result of the projection is:
(Jane, 21)   (Jane, 39)   (John, 32)

Note that duplicate tuples (e.g. (John,32)) are eliminated.


... The Projection Operation69/90

The projection operation needs to:

 scan the entire relation as input 
	(straightforward, whichever file organisation is used)
 remove unwanted attributes in output 
	(straightforward, manipulating internal record structure)
 eliminate any duplicates produced  
	(not as simple as other operations ...)

There are two approaches for task 3: sorting or hashing.


Removing Attributes70/90

Projecting attributes involves creating a new tuple,
using only some values from the original tuple.

Precisely how to achieve this depends on tuple internals.

Removing attributes from fixed-length tuples:





... Removing Attributes71/90

Removing attributes from variable-length tuples:





Sort-based Projection72/90

Overview of the method:

 Scan input relation Rel and produce a
	file of tuples containing only the projected attributes
 Sort this file of tuples using the combination of
	all attributes as the sort key
 Scan the sorted result, comparing adjacent tuples,
	and discard duplicates

Requires a temporary file/relation (Temp)


... Sort-based Projection73/90

The method, in detail:
// Inputs: relName, attrList
inf = openFile(fileName(relName),READ);
tempf = openFile(tmpName,CREATE);
clear(outbuf); j = 0;
for (p = 0; p < nPages(inf); p++) {
    buf = readPage(inf,p);
    for (i = 0; i < nTuples(buf); i++) {
        tup = getTuple(buf,i);
        newtup = project(tup,attrList);
        addTuple(newtup,outbuf);
        if (isFull(outbuf)) {
            writePage(tempf,j++,outbuf);
            clear(outbuf);
        }
    }
}
mergeSort(tempf);

(continued ...)


... Sort-based Projection74/90

(... continued)
tempf = openFile(tmpName,READ);
outf = openFile(result,CREATE);
clear(outbuf); prev = EMPTY; j = 0;
for (p = 0; p < nPages(tempf); p++) {
    buf = readPage(tempf,p);
    for (i = 0; i < nTuples(buf); i++) {
        tup = getTuple(buf,i);
        if (tupCompare(tup,prev) != 0) {
            addTuple(tup,outbuf);
            if (isFull(outbuf)) {
                writePage(outf,j++,outbuf);
                clear(outbuf);
            }
            prev = tup;
        }
    }
}



Cost of Sort-based Projection75/90

The costs involved are (assuming B+1 buffers for sort):

 scanning original relation Rel:   bR
 writing Temp relation:   bT
 sorting Temp relation:   2.bT(1 + ⌈logBb0⌉ ) where b0 = ⌈bT/B⌉
 removing duplicates from Temp:   bT
 writing the result relation:   bOut

Total cost = sum of above = bR + 2.bT + 2.bT(1 + ⌈logBb0⌉ ) + bOut


Note that we often ignore cost of writing the result;
especially when comparing different algorithms for the same relational operation.



Improving Sort-based Projection76/90

Some approaches for improving the cost:

 remove first stage; do projection during first phase of sort

 reduce sorting costs by:

 using more memory buffers (but there is a limit)
 eliminating duplicates during the merge phase


 minimise scanning cost by laying pages out
on disk appropriately 
(generally, we don't have this luxury since the O/S handles it for us)



Hash-based Projection77/90

Overview of the method:

 Scan input relation Rel and produce a
	set of hash partitions based on the projected attributes
 Scan each hash partition looking for duplicates
 Once each partition is duplicate-free, write out the
	remaining tuples

The method requires:

 two different hash functions using all projected fields
 "sufficient" main memory buffers and good hash functions



Hash Functions78/90

Hash function h(tuple,range):

 maps attribute values  →  page address

Implementation issues for hash functions:

 range of values is typically larger than range of page addresses
 use mod function to "fit" hash value into address range
 expect many tuples to hash to one page
	  (but not too many)
 try to spread addresses uniformly
	  (impossible if data distrib is skew)
 make address computation cheap



... Hash Functions79/90

Usual approach in hash function:

 convert key into numeric value 
 (method depends on key type)
 fit into page address space

Example hash function for character strings:

unsigned int hash(char *val, int b)
{
    char *cp;
    unsigned int v, sum = 0;
    for (c = val; *c != '\0'; c++) {
        v = *c + (*(c+1) << 8);
        sum += (sum + 2153*v) % 19937;
    }
    return(sum % b);
}



Hash-based Projection80/90

Partitioning phase:





... Hash-based Projection81/90

Algorithm for partitioning phase:
for each page P in relation Rel {
    for each tuple t in page P {
        t' = project(t, attrList)
        H = h1(t', B-1)
        write t' to partition[H]
}   }

Each partition could be implemented as a simple data file.


... Hash-based Projection82/90

Duplicate elimination phase:





... Hash-based Projection83/90

Algorithm for duplicate elimination phase:
for each partition P in 0..B-2 {
    for each tuple t in partition P {
        H = h2(t, B-1)
        if (!(t occurs in buffer[H]))
            append t to buffer H
    }
    output contents of all buffers
    clear all buffers
}



Cost of Hash-based Projection84/90

The total cost is the sum of the following:

 scanning original relation Rel:   bR
 writing partitions:   bP ≥ bR, but likely bP ≅ bR
 re-reading partitions:   bP
 writing the result relation:   bOut

To ensure that B is larger than the largest partition ...

 use hash functions (h1,h2) with uniform spread
 allocate at least sqrt(bR) buffers



... Cost of Hash-based Projection85/90

If the largest partition had more than B-1 pages

 some in-memory hash buckets would fill up
 overflow would then need to be dumped to disk
 for each subsequent record hashing to that bucket

 look for duplicates in contents of in-memory hash bucket
 and read dumped bucket contents and look for duplicates


This would potentially increase the cost by a large amount 
(worst case is one additional page read for every record after hash bucket fills)


Index-only Projection86/90

Under the conditions:

 relation is indexed on (A1,A2,...An)
 projected attributes are a prefix of (A1,A2,...An)

can do projection without accessing data file.

Basic idea:

 attribute values for (A1,A2,...An) are stored in the index
 scan through index file (which is already sorted on attributes)
 duplicates are already adjacent in index, so easy to skip



... Index-only Projection87/90

Method:
for each entry I in index file {
    tup = project(I.key, attrList)
    if (tupCompare(tup,prev) != 0) {
        addTuple(outbuf,tup)
        if (isFull(outbuf)) {
            writePage(outf,op++,outbuf);
            clear(outbuf);
        }
        prev = tup;
}   }


"for each index entry": loop over index pages and loop over entries in each page



Cost of Index-only Projection88/90

Assume that the index (see details later):

 is a file containing values of indexing keys
 consisting of bi pages   (where bi ≪ bR)

Costs involved in index-only projection:

 scanning whole index file Index:   bi
 writing tuples to Result:   bOut

Total cost:   bi + bOut   ≪   bR + bOut


Comparison of Projection Methods89/90

Difficult to compare, since they make different assumptions:

 index-only: needs an appropriate index
 hash-based: needs buffers and good hash functions
 sort-based: needs only buffers ⇒ use as default


Best case scenario for each (assuming B+1 in-memory buffers):

 index-only:   bi + bOut   ≪   bR + bOut
 hash-based:   bR + 2.bP + bOut   ≅   3.bR + bOut
 sort-based:   bR + 2.bT(2+logBb0) + bOut



Projection in PostgreSQL90/90

Code for projection forms part of execution iterators:

 backend/executor/execQual.c

Functions involved with projection:

 ExecProject(projInfo,...) ... extracts/stores projected data
 ExecTargetList(...) ... makes new tuple from old tuple + projection info
 ExecStoreTuple(newTuple,...) ... save tuple in output slot


Produced: 13 Jun 2016</p><h3 >字段4</h3><p>Selection on One Attribute


Selection



Varieties of Selection2/160

Selection is a relational algebra operation that ...

 filters a subset of tuples from one relation
 based on a condition on the attribute values

We consider three distinct styles of selection:

 1-d (one dimensional)   (condition uses only 1 attribute)
 n-d (multi-dimensional)   (condition uses >1 attribute)
 similarity   (approximate matching, with ranking)

Each style has several possible file-structures/techniques.


... Varieties of Selection3/160

We can view a relation as defining a tuple space

 assume relation R with attributes a1,...,an
 attribute domains of R specify a n-dimensional space
 each tuple (v1,v2,...,vn) ∈ R is a point in that space
 queries specify values/ranges on N≥1 dimensions
 a query defines a point/line/plane/region of the n-d space
 results are tuples lying at/on/in that point/line/plane/region

E.g. if N=n, we are checking existence of a tuple at a point


One-dimensional Selection



Operations for 1d Select5/160

One-dimensional select queries = condition on single attribute.

 one = single tuple access, e.g.
select * from Employees where id = 15567;

 pmr = multiple tuple access, e.g.
select * from Employees where age = 25;

 range = range queries, e.g.
select * from Employees where age>20 and age<50;




... Operations for 1d Select6/160






... Operations for 1d Select7/160

Other operations on relations:

 ins = insert a new tuple
insert into Employees(id,name,age,dept,salary)
values (12345, 'John', 21, 'Admin', 55000.00);

 del = delete matching tuples, e.g.
delete Employees where age > 55;

 mod = update matching tuples, e.g.
update Employees
set    salary = salary * 1.10
where  dept = 'Admin';




... Operations for 1d Select8/160

In the algorithms below, we assume:

 one queries are of the form
select * from R where k = val

where k is a unique attribute and val is a constant
 pmr queries are of the form
select * from R where k = val

where k is a non-unique attribute and val is a constant
 range queries are of the form
select * from R where k between lo and hi

where k is any attribute and lo and hi are constants
 R.k gives the value of attribute k in table R



Implementing Select Efficiently9/160

Two basic approaches:

 physical arrangement of tuples

 sorting
	  (search strategy)
 hashing
	  (static, dynamic, n-dimensional)

 additional indexing information

 index files
	  (primary, secondary, trees)
 signatures 
	  (superimposed, disjoint)



Our analyses assume: 1 input buffer available for each relation.

If more buffers are available, most methods benefit.


Heap Files




Note: this is not "heap" as in the top-to-bottom ordered tree.

It means simply an unordered collection of tuples in a file.




Heap File Structure11/160

The simplest possible file organisation.

New tuples inserted at end of file; tuples deleted by marking.






Selection in Heaps12/160

For all selection queries, the only possible strategy is:
// select * from R where C
f = openFile(fileName("R"),READ);
for (p = 0; p < nPages(f); p++) {
    buf = readPage(f, p);
    for (i = 0; i < nTuples(buf); i++) {
        tup = getTuple(buf,i);
        if (matches(tup,C))
            add tup to result set
    }
}

i.e. linear scan through file searching for matching tuples


... Selection in Heaps13/160

The heap is scanned from the first to the last page:



Costrange  =  Costpmr  =  b

If we know that only one tuple matches the query (one query), 
a simple optimisation is to stop the scan once that tuple is found.

Costone :      
Best = 1     
Average = b/2     
Worst = b


Insertion in Heaps14/160

Insertion: new tuple is appended to file (in last page).
f = openFile(fileName("R"),READ|WRITE);
b = nPages(f);
buf = readPage(f, b-1);  // request page
if (isFull(buf)) // all slots used
    { b++; clear(buf); }
if (tooLarge(newTup,buf)) // not enough space
    { deal with oversize tuple }
addTuple(newRec, buf);
writePage(f, b, buf);  // mark page as dirty & release

Costinsert  =  1r + 1w
       
(plus possible extra writes for oversize recs)


... Insertion in Heaps15/160

Alternative strategy:

 find any page from R with enough space
 preferably a page already loaded into memory buffer

PostgreSQL's strategy:

 use last updated page of R in buffer pool
 otherwise, search buffer pool for page with enough space
 assisted by free space map (FSM) associated with each table
 for details: backend/access/heap/{heapam.c,hio.c}



... Insertion in Heaps16/160

PostgreSQL's tuple insertion:
heap_insert(Relation relation,    // relation desc
            HeapTuple newtup,     // new tuple data
            CommandId cid, ...)   // SQL statement


 finds page which has enough free space for newtup
 loads page into buffer pool and locks it
 copies tuple data into page buffer, sets header data
 writes details of insertion into transaction log
 returns OID of new tuple if relation has OIDs



Deletion in Heaps17/160

Deletion for one conditions, e.g.
delete from Employees where id = 12345

Method:

 scan until page containing required tuple is loaded
 mark tuple as deleted and write page to disk

Costdelete1:
  best: 1r+1w
  avg: (b/2)r+1w
  worst: br+1w


... Deletion in Heaps18/160


Deletion for pmr or range conditions, e.g.
delete from Employees where dept = 'Marketing'
delete from Employees where 40 <= age and age < 50

Method:

 scan all pages
 mark matching tuples as deleted (e.g. update slot directory)
 write affected (modified) pages to disk

Costdelete  =  br + bqw
   
(where bq is #pages with matches)


... Deletion in Heaps19/160

Implementation of deletion:
// Deletion in heaps ... delete from R where C
f = openFile(fileName("R"),READ|WRITE);
for (p = 0; p < nPages(f); p++) {
    buf = readPage(f, p);
    ndels = 0;
    for (i = 0; i < nTuples(buf); i++) {
        tup = getTuple(buf,i);
        if (matches(tup,C))
            { ndels++; delTuple(buf,i); }
    }
    if (ndels > 0) writePage(f, p, buf);
}



... Deletion in Heaps20/160

How to deal with free slots:

 re-use on subsequent insertion
	  (requires modified insertion (see below))
 periodic file reorganisation
	  (requires full table locking for extended time)


Insertion with free slots:
f = openFile(fileName("R"),READ|WRITE);
for (p = 0; p < nPages(f); p++) {
    buf = readPage(f, p);
    if (spaceInBuf(buf,newTup))
        { addTuple(newTup,buf); break; }
}
if (p == nPages(f)) // not yet added
    { clear(buf); addTuple(newTup,buf); }
writePage(f, p, buf);



... Deletion in Heaps21/160

PostgreSQL tuple deletion:
heap_delete(Relation relation,    // relation desc
            ItemPointer tid, ..., // tupleID
            CommandId cid, ...)   // SQL statement


 gets page containing tuple into buffer pool and locks it
 sets flags, commandID and xmax in tuple; dirties buffer
 writes indication of deletion to transaction log (at commit time)



Updates in Heaps22/160

Example:   update  R  set  F = val  where  Condition

Analysis for updates is similar to that for deletion

 scan all pages
 replace any updated tuples   (within each page)
 write affected pages to disk

Costupdate  =  br + bqw

A complication: new version of tuple may not fit in page.

Solution:   delete and then insert 

(Cost harder to analyse; depends on how many "oversize" tuples are produced)



... Updates in Heaps23/160


// Update in heaps ... update R set F = val where C
f = openFile(fileName("R"),READ|WRITE);
for (p = 0; p < nPages(f); p++) {
    buf = readPage(f, p);
    nmods = 0;
    for (i = 0; i < nTuples(buf); i++) {
        tup = getTuple(buf,i);
        if (matches(tup,C)) {
            nmods++;
            modTuple(tup,F,val);
            if (!tooLarge(tup,buf))
                addTuple(tup,buf);
            else {
                delTuple(buf,i);
                InsertTupleInHeap(f,tup); 
            }
    }   }
    if (nmods > 0) writePage(f, p, buf);
}



... Updates in Heaps24/160

PostgreSQL tuple update:
heap_update(Relation relation,     // relation desc
            ItemPointer otid,      // old tupleID
            HeapTuple newtup, ..., // new tuple data
            CommandId cid, ...)    // SQL statement


 essentially does delete(otid), then insert(newtup)
 also, sets old tuple's ctid field to reference new tuple
 can also update-in-place if no referencing transactions



Heaps in PostgreSQL25/160

PostgreSQL stores all table data in heap files (by default).

Typically there are also associated index files.

If a file is more useful in some other form:

 PostgreSQL may make a transformed copy during query execution
 programmer can set it via   create index...using hash

Heap file implementation:   src/backend/access/heap


... Heaps in PostgreSQL26/160

PostgreSQL "heap files" use ≥ 3 physical files

 files are named after the OID of the corresponding table
 first data file is called simply OID
 if size exceeds 1GB, create a fork called OID.1
 add more forks as data size grows (one fork for each 1GB)
 other files:

 free space map (OID_fsm), visibility map (OID_vm)
 optionally, TOAST file (if table has varlen attributes)

 for details: Chapter 55 in PostgreSQL documentation



... Heaps in PostgreSQL27/160

Free space map

 contains data on free-space-per-data page
 organised as a tree of free-space values (one byte per page)
 fast to discover: no page has space for new tuple
 fast to discover: page which has enough space

Visibility map

 keeps track of pages with all "live" tuples 
(i.e. all tuples in page are visible to all active transactions)
 organised as a bit-map (one bit per page)
 makes vacuum faster (can ignore all-live pages)
 may help to sped up tuple visibility checks in future



Sorted Files



Sorted Files29/160


Records stored in file in order of some field k
(the sort key).

Makes searching more efficient; makes insertion less efficient





... Sorted Files30/160

In order to simplify insertion, use overflow blocks.





Large-scale file re-arrangement occurs less frequently.


... Sorted Files31/160

Conceptual view of sorted file structure:



Total number of overflow blocks = bov.

Average overflow chain length = Ov = bov/b.

Bucket = data page + its overflow page(s)


Selection in Sorted Files32/160

For one queries on sort key, use binary search.
// select * from R where k = val  (sorted on R.k)
lo = 0; hi = b-1
while (lo <= hi) {
    mid = (lo+hi) div 2;
    buf = getPage(f,mid);
    (tup,loVal,hiVal) = searchBucket(f,mid,k,val);
    if (tup != null) return tup;
    else if (val < loVal) hi = mid - 1;
    else if (val > hiVal) lo = mid + 1;
    else return NOT_FOUND;
}
return NOT_FOUND;



... Selection in Sorted Files33/160

Search a page and its overflow chain for a key value
searchBucket(f,p,k,val)
{
    tup = null;
    buf = readPage(f,p);
    (tup,min,max) = searchPage(buf,k,val,+INF,-INF)
    if (tup != null) return(tup,min,max);
    ovf = openOvFile(f);
    ovp = ovflow(buf);
    while (tup == NULL && ovp != NO_PAGE) {
        buf = readPage(ovf,ovp);
        (tup,min,max) = searchPage(buf,k,val,min,max)
        ovp = ovflow(buf);
    }     
    return (tup,min,max);
}



... Selection in Sorted Files34/160

Search within a page for key; also find min/max values
searchPage(buf,k,val,min,max)
{
    for (i = 0; i < nTuples(buf); i++) {
        tup = getTuple(buf,i);
        if (tup.k == val) res = tup;
        if (tup.k < min) min = tup.k;
        if (tup.k > max) max = tup.k;
    }
	return (res,min,max);
}



... Selection in Sorted Files35/160

The above method treats each bucket as a single large page.

Cases:

 best: find tuple in first data page we read
 worst: full binary search, and not found

 examine log2b data pages
 plus examine all of their overflow pages

 average: examine some data pages + their overflow pages


Costone :    
Best = 1     
Worst = log2 b + bov


... Selection in Sorted Files36/160

Average Costone is messier to analyse:

 complete most of binary search ((log2b)-1)
 in each unsuccessful bucket, examine 1+Ov pages
 in the successful bucket, examine (1+Ov)/2 pages

Costone :    Average  ≅  ((log2b)-1)(1+Ov)

To be more "precise"

 n = (log2b)-1 = number of buckets examined
 Average Cost = n(1+Ov) + 1 + (Ov/2)

In general, average case is difficult to analyse since dependent on
other factors such as data distribution.


... Selection in Sorted Files37/160

For pmr query, on non-unique attribute k

 assume file is sorted on k
 tuples containing k may appear in several pages





Begin by locating a page p containing k=val
	  (as for one query).

Scan backwards and forwards from p to find matches.

Thus, Costpmr  =  Costone + (bq-1).(1+Ov)



... Selection in Sorted Files38/160

For range queries on unique sort key (e.g. primary key):

 use binary search to find lower bound
 read sequentially until reach upper bound

Costrange  =  Costone + (bq-1).(1+Ov)

If secondary key, similar method to pmr.





... Selection in Sorted Files39/160

So far, have assumed query condition involves sort key k.

If condition contains attribute j, not the sort key

 file is unlikely to be sorted by j as well
 sortedness gives no searching benefits


Costone,
   Costrange,
   Costpmr
  as for heap files


Insertion in Sorted Files40/160

Insertion is more complex than for Heaps.

 find appropriate page for tuple (via binary search)
 if page not full, insert into page
 otherwise, insert into next overflow block with space

Thus, Costinsert  =  Costone + δw

where δ  =  1 or 2, depending on whether we need to create a new overflow block


Deletion in Sorted Files41/160

Deletion involves:

 find matching tuple(s)
 mark them as deleted

Cost depends on selection criteria
	  (i.e. on how many matching tuples there are)

Thus, Costdelete  =  Costselect + bqw


Hashed Files



Hashing43/160

Basic idea: use key value to compute page address of tuple.



e.g. tuple with key = v is stored in page i

Requires: hash function h(v) that maps KeyDomain → [0..b-1].


Hash Functions44/160

Points to note:

 convert a key value into a page address in range 0 .. b-1
 deterministic
      (given key value k always maps to same block address)
 key domain size is typically much larger than 0 .. b-1
 hash is typically 32-bit value 0 .. 232-1
 use mod function to "fit" hash value into block address range
 expect many tuples to hash to one block
      (but not too many)
 spread values uniformly over address range
      (pseudo-random)
 cost of computing hash function must be cheap



... Hash Functions45/160

Usual approach in hash function:

 convert key into numeric value 
	 (how to do this depends on key type)
 numeric value can be viewed as an arbitrary-length bit-string
 value is mapped into page address space

May need separate hash function for each data type

Or, convert each data value to a string and hash that.


... Hash Functions46/160

PostgreSQL hash function (simplified):

hash_any(unsigned char *k, register int keylen)
{
    register uint32 a, b, c, len;
    /* Set up the internal state */
    len = keylen;
    a = b = 0x9e3779b9;
    c = 3923095;
    /* handle most of the key */
    while (len >= 12) {
        a += (k[0] + (k[1] << 8) + (k[2] << 16) + (k[3] << 24));
        b += (k[4] + (k[5] << 8) + (k[6] << 16) + (k[7] << 24));
        c += (k[8] + (k[9] << 8) + (k[10] << 16) + (k[11] << 24));
        mix(a, b, c);
        k += 12; len -= 12;
    }

    /* collect any data from last 11 bytes into a,b,c */
    mix(a, b, c);
    return c;
}



... Hash Functions47/160

Where mix is defined as:

#define mix(a,b,c) \
{ \
  a -= b; a -= c; a ^= (c>>13); \
  b -= c; b -= a; b ^= (a<<8);  \
  c -= a; c -= b; c ^= (b>>13); \
  a -= b; a -= c; a ^= (c>>12); \
  b -= c; b -= a; b ^= (a<<16); \
  c -= a; c -= b; c ^= (b>>5);  \
  a -= b; a -= c; a ^= (c>>3);  \
  b -= c; b -= a; b ^= (a<<10); \
  c -= a; c -= b; c ^= (b>>15); \
}

i.e. scrambles all of the bits from the bytes of the key value

See backend/access/hash/hashfunc.c for details.


... Hash Functions48/160

Above functions give hash value as 32-bit quantity (uint32).

Two ways to map raw hash value into a page address:

 if b = 2k, bitwise AND with k low-order bits set to one
uint32 hashToPageNum(uint32 hval) {
    uint32 mask = 0xFFFFFFFF;
    return (hval & (mask >> (32-k)));
}

 otherwise, use mod to produce value in range 0..b-1
uint32 hashToPageNum(uint32 hval) {
    return (hval % b);
}




Hashing Performance49/160

Aims:

 distribute tuples evenly amongst blocks
 have most blocks nearly full 
	  (attempt to minimise wasted space)

Note: if data distribution not uniform,
block address distribution cannot be uniform.

Best case: every block contains same number of tuples.

Worst case: every tuple hashes to same block.

Average case: some blocks have more tuples than others.


... Hashing Performance50/160

With non-uniform distribution or too many tuples, some blocks will fill.

Use overflow blocks to contain "excess" tuples; one chain of overflow
blocks for each primary block
	  (containing all tuples with same hash value).





... Hashing Performance51/160

Trade-off (assuming reasonably uniform hash function):

 adding more pages to data file reduces collisions (⇒ overflows)

effect: reduces long overflow chains which cause query-time overhead
 adding more pages to data file results in more unused storage space

effect: half-full blocks result in storage overhead

Want to minimise overflow chain length, yet keep blocks as full as possible.


... Hashing Performance52/160

Two important measures for hash files:

 load factor:   L  =  r/b.C
 average overflow chain length:   Ov  =  bov/b

Three cases for distribution of tuples in a hashed file:



 Case 
 L 
 Ov 


 Best 
 ≅ 1 
 0 


 Worst 
 >> 1 
 ** 


 Average 
 < 1 
 0<Ov<1 


(** performance is same as Heap File)


To achieve average case, aim for   0.75  ≤  L  ≤  0.9.


Selection with Hashing53/160

Best performance occurs for one queries on hash key field.

Basic strategy:

 compute page address via hash function hash(val)
 fetch that page and look for matching tuple
 possibly fetch additional pages from overflow chain

Best Costone  =  1
    (find in data page)

Average Costone  =  1+Ov/2
    (scan half of ovflow chain)

Worst Costone  =  1+max(OvLen)
    (find in last page of ovflow chain)


... Selection with Hashing54/160

Select via hashing on unique hash key k (one)

Overview:
P = hashToPage(val)
for each tuple t in page P {
    if (t.k = val) return t
}
for each overflow page Q of P {
    for each tuple t in page Q {
        if (t.k = val) return t
}   }



... Selection with Hashing55/160

Details of select via hashing on unique key k:
// select * from R where k = val
f = openFile(relName("R"),READ);
p = hash(val) % nPages(f);
buf = readPage(f, p)
for (i = 0; i < nTuples(buf); i++) {
    tup = getTuple(buf,i);
    if (tup.k == val) return tup;
}
ovp = ovflow(buf);
while (ovp != NO_PAGE) {
    buf = readPage(ovf,ovp);
    for (i = 0; i < nTuples(Buf); i++) {
        tup = getTuple(buf,i);
        if (tup.k == val) return tup;
}   }



... Selection with Hashing56/160

Select via hashing on non-unique hash key k (pmr)
// select * from R where k = val
f = openFile(relName("R"),READ);
p = hash(val) % nPages(f);
buf = readPage(f, p)
for (i = 0; i < nTuples(buf); i++) {
    tup = getTuple(buf,i);
    if (tup.k == val) append tup to results
}
ovp = ovflow(buf);
while (ovp != NO_PAGE) {
    buf = readPage(ovf,ovp);
    for (i = 0; i < nTuples(Buf); i++) {
        tup = getTuple(buf,i);
        if (tup.k == val) append tup to results
}   }

Costpmr  =  1 + Ov


... Selection with Hashing57/160

Unless the hash function is order-preserving
(and most aren't)
hashing does not help with range queries.

Costrange = b + bov

Selection on attribute j which is not hash key ...

Costone,
   Costrange,
   Costpmr
 = 
b + bov


Insertion with Hashing58/160

Insertion uses similar process to one queries.

Overview:
P = hashToPage(val)
if room in page P {
    insert t into P; return
}
for each overflow page Q of P {
    if room in page Q {
        insert t into Q; return
}   }
add new overflow page Q
link Q to previous overflow page
insert t into Q



... Insertion with Hashing59/160

Details of insertion into hashed file:
f = openFile(fileName("R"),READ|WRITE);
p = hash(tup.k) % nPages(f);
buf = readPage(f,p);
if (!isFull(buf) && addTuple(buf,tup) >= 0)
    { writePage(f, p, buf); return; }
ovf = openFile(ovFileName("R"),READ|WRITE);
p = ovFlow(buf); hasOvFlow = (p != NO_PAGE);
while (p != NO_PAGE) {
    buf = readPage(ovf,p);
    if (!isFull(buf) && addTuple(buf,tup) >= 0)
        { writePage(ovf, p, buf); return; }
    p = ovflow(buf);
}
ovflow(buf) = nPages(ovf);
writePage((hasOvFlow?ovf:f), p, buf);
clear(buf);
addTuple(buf, tup);
writePage(ovf, nPages(ovf), buf);



... Insertion with Hashing60/160

Variation in costs determined by iteration on overflow chain.

Best Costinsert  =  1r + 1w

Average Costinsert  =  (1+Ov)r + 1w

Worst Costinsert  =  (1+max(OvLen))r + 2w


Deletion with Hashing61/160

Similar performance to select:
// delete from R where k = val
// f = data file ... ovf = ovflow file
p = hash(val) % b;
buf = readPage(f,p);
delTuples(f,buf,k,val);
p = ovFlow(buf)
while (p != NO_PAGE) {
    buf = readPage(ovf,p);
    delTuples(ovf,buf,k,val);
    p = ovFlow(buf);
}

Extra cost over select is cost of writing back modified blocks.

Method works for both unique and non-unique hash keys.


... Deletion with Hashing62/160

Function for deleting all matches in a buffer:
delTuples(outf, buf, k, val)
{
    int i;
    int ndels = 0;
    for (i = 0; i < nTuples(buf); i++) {
        tup = getTuple(buf,i);
        if (tup.k == val)
            { ndels++; delTuple(buf,i); }
    }
    if (ndels > 0)
        writePage(outf, p, buf);
}

Realistic deletion would also handle free-list of empty pages.


Another Problem with Hashing...63/160

So far, discussion of hashing has assumed a fixed file size
(fixed b).

This is known as static hashing.

What size file to use?

 the size we need right now
	  (performance degrades as file overflows)
 the maximum size we might ever need
	  (signifcant waste of space)



... Another Problem with Hashing...64/160

If we change the file size, by adding more blocks to accomodate more tuples,
then we are effectively changing the hash function.

In order to be able to use the new hash function to find existing tuples,
all tuples need to be removed and re-inserted.

In other words, this requires complete re-organisation of the file.

Obvious disadvantages:

 the cost of re-organisation 
	 (consider re-inserting 106 tuples)
 lack of access to the data during reorganisation.



Flexible Hashing65/160

Several hashing schemes have been proposed to deal with dynamic files.

Two methods access blocks via a directory:

 extendible hashing,   dynamic hashing

A third method expands files systematically, so needs no directory:

 linear hashing

All methods:

 treat hash value as bit-string   (e.g. unsigned 32-bit int)
 adjust hash function by altering number of bits considered



... Flexible Hashing66/160

Require a function to act on hash values to give d bits
#define HashSize 32
typedef unsigned int HashVal;

// return low-order d bits
HashVal bits(d int, h HashVal)
{
    HashVal mask;
    assert(d > 0 && d <= HashSize);
    mask = 0xffffffff >> (HashSize-d);
    return (h & mask);
}

// return high-order d bits
HashVal bits'(d int, h HashVal)
{
    assert(d > 0 && d <= HashSize);
    return (h >> (HashSize-d));
}



... Flexible Hashing67/160

Important concept for flexible hashing: splitting

 consider one page (all tuples have same hash value)
 recompute page numbers by considering one extra bit
 if current page is 101, new pages have hashes 0101 and 1101
 some tuples stay in page 0101 (was 101)
 some tuples move to page 1101 (new page)
 also, rehash any tuples in overflow pages of page 101

Aim: expandable data file, requiring minimal large reorganisation


... Flexible Hashing68/160

Example of splitting:




Tuples only show key value; assume h(val) = val



Extendible Hashing69/160

File organisation:

 file of primary data blocks
 directory containing block addresses
 directory indexed using first/last d bits from hash value 
(can make use of bits either high→low or low→high bits
 data file and directory expand as more tuples added 
(expansion occurs whenever pages fill, so there are no overflow pages)


Directory could live in page 0 and be cached in memory buffer.



... Extendible Hashing70/160






Selection with Ext.Hashing71/160

Size of directory = 2d; d is called depth.

Hash function h produces a (e.g. 32-bit) bit-string.

Use first d bits of it to access directory to obtain block address.

Selection method for one queries:
p = directory[bits'(d,h(val))];
buf = readPage(f,p);
for (i = 0; i < nTuples(buf); i++) {
    tup = getTuple(buf,i);
    if (tup.k == val) return tup;
}

No overflow blocks, so Costone  =  1

Assume: a copy of the directory is pinned in DBMS buffer pool.


Insertion with Ext.Hashing72/160

For insertion, use selection method to determine block.

If selected block S not full, simply insert tuple.

What to do if selected block full?

 add a new block N to the file
 partition tuples between blocks S and N

The partitioning process is called splitting.


... Insertion with Ext.Hashing73/160

Basis for splitting?

All tuples r in this block have same bits'(d,hash(r.k))

So ... use d+1 bits of hash (i.e. bits'(d+1,hash(r.k))

This gives twice as many possible hash values.

Since hash values index directory, directory size is doubled.

E.g. d=4 gives indexes 0..15, d=5 gives indexes 0..31


... Insertion with Ext.Hashing74/160

Doubling directory size and adding one block creates many empty directory slots.

What to do with these slots?   Make them point to existing (buddy) pages.






The idea: some parts are hashed effectively using d bits, others using d+1 bits.



... Insertion with Ext.Hashing75/160

If we split a block with two pointers to it, no need to make directory larger.






Ext.Hashing Example76/160

Consider the following set of tuples describing bank deposits:


 Branch 
 Acct.No 
 Name 
 Amount 


 Brighton 
 217 
 Green 
 750 


 Downtown 
 101 
 Johnshon 
 500 


 Mianus 
 215 
 Smith 
 700 


 Perryridge 
 102 
 Hayes 
 400 


 Redwood 
 222 
 Lindsay 
 700 


 Round Hill 
 305 
 Turner 
 350 


 Clearview 
 117 
 Throggs 
 295 




... Ext.Hashing Example77/160

We hash on the branch name, with the following hash function:


 Branch 
 Hash Value 


 Brighton 
 0010 1101 1111 1011 


 Clearview 
 1101 0101 1101 1110 


 Downtown 
 1010 0011 1010 0000 


 Mianus 
 1000 0111 1110 1101 


 Perryridge 
 1111 0001 0010 0100 


 Redwood 
 1011 0101 1010 0110 


 Round Hill 
 0101 1000 0011 1111 





... Ext.Hashing Example78/160

Assume we have a file with   c=2.

Start with an initially empty file:



Add Brighton... tuple (hash=0010...). 
Add Downtown... tuple (hash=1010...).





... Ext.Hashing Example79/160

Add Mianus... tuple (hash=1000...).





... Ext.Hashing Example80/160

Add Perryridge... tuple (hash=1111...).





... Ext.Hashing Example81/160

Add Redwood... tuple (hash=1011...).





... Ext.Hashing Example82/160

Add Round Hill... tuple (hash=0101...).





... Ext.Hashing Example83/160

Add Clearview... tuple (hash=1101...).





... Ext.Hashing Example84/160

Add another Perryridge... tuple (hash=1111...).





... Ext.Hashing Example85/160

Add another Round Hill... tuple (hash=0101...).





... Ext.Hashing Example86/160

Asumption: directory is small enough to be stored in memory.

Most of the time we read and write exactly one block.

If we need to split, we write just one extra block.

Conditions to minimise splitting:

 hash function is good   (distributes tuples uniformly)
 blocks contains many tuples   (b ≫ 2)

On average, Costinsert  =  1r + (1+δ)w

(δ is a small value depending on b, load, hash function,...)


... Ext.Hashing Example87/160

A potential problem:

 split might leave all tuples in same page (p)
 new tuple hashes to page p ⇒ no room
 need to split again  
	(eventually tuples differ in d+n bits)

A degenerate case:

 each block holds c tuples
 more than c tuples hash to exact same value
 splitting never disambiguates tuples ⇒ directory explodes



Deletion with Ext.Hashing88/160

Similar to ordinary static hash file - mark tuple as removed.

However, might also wish to reclaim space:

 remove block when empty
 merge two "buddy" blocks if both half full

Both cases:

 remove a single block
 require simple adjustments in directory



... Deletion with Ext.Hashing89/160

What to do with empty blocks from file perspective?

Empty block might create hole in middle of file.

Two methods to deal with this:

 maintain a list of free pages, and re-use on next split
 shrink file: copy last page into "hole", adjust directory



Linear Hashing90/160

File organisation:

 file of primary data blocks
 file of overflow data blocks
 a register called the split pointer

Advantages over other approaches:

 does not require auxiliary storage for a directory
 does not require periodic file reorganisation

Uses systematic method of growing data file ...

 hash function "adapts" to changing address range
 systematic splitting controls length of overflow chains



... Linear Hashing91/160

File grows linearly (one block at a time, at regular intervals).

Can be viewed as having "phases" of expansion; during each phase, b doubles.






Selection with Lin.Hashing92/160

If b=2d, the file behaves exactly like standard hashing.

Use d bits of hash to compute block address.
// select * from R where k = val
P = bits(d,hash(val));  --least sig bits
for each tuple t in page P
         and its overflow pages {
    if (t.k == val) return t;
}

Average Costone  =  1+Ov


... Selection with Lin.Hashing93/160

If b != 2d, treat different parts of the file differently.



Parts A and C are treated as if part of a
file of size 2d+1.

Part B is treated as if part of a file of size 2d.

Part D does not yet exist (B expands into it).


... Selection with Lin.Hashing94/160

Modified algorithm:
// select * from R where k = val
h = hash(val);
p = bits(d,h);
if (p < sp) { p = bits(d+1,h); }
buf = readPage(f,p);
// now proceed as before
for each tuple R in block p
         and its overflow blocks {
    if (R.k == val) return R;
}



Insertion with Lin.Hashing95/160

To see why selection works, need to look at how file expands.





Lin.Hashing Example96/160

Assume we have a file with: c=2.

Start with an initially empty file:





Add Brighton... tuple (hash=...1011). 
Add Downtown... tuple (hash=...0000).






... Lin.Hashing Example97/160

Add Mianus... tuple (hash=...1101).





Add Perryridge... tuple (hash=...0100).






... Lin.Hashing Example98/160

Add Redwood... tuple (hash=...0110).






... Lin.Hashing Example99/160

Add Round Hill... tuple (hash=...1111).






... Lin.Hashing Example100/160

Add Clearview... tuple (hash=...1110).






... Lin.Hashing Example101/160

Add another Clearview... tuple.






Linear Hashing Insertion102/160

Abstract view:
p = bits(d,hash(tup.k));
if (p < sp) p = bits(d+1,hash(val));
// bucket p = page p + its ovflow pages
for each page Q in bucket p {
    if (space in Q) {
        insert tuple into Q
        break
    }
}
if (no insertion) {
    add new ovflow page to bucket p
    insert tuple into new page
}
if (need to split) {
    partition tuples from bucket sp
          into buckets sp and sp+2^d
    sp++;
    if (sp == 2^d) { d++; sp = 0; }
}



... Linear Hashing Insertion103/160

Detailed algorithm:
h = hash(tup.k);
p = bits(d,h);
if (p < sp) p = bits(d+1,h);
//start insertion into p bucket
inserted = 0; inOvflow = 0;
buf = readPage(f,p);
if (isFull(buf))
    { p = ovFlow(buf); }
else {
    // have space in data page
    addTuple(buf,tup); // assume it fits
    writePage(f, p, buf);
    inserted = 1;
}
...



... Linear Hashing Insertion104/160

...
// attempt insertion into ovflow pages
while (!inserted && p != NO_PAGE) {
    inOvflow = 1;
    buf = readPage(ovf,p);
    if (!isFull(buf)) {
        addTuple(buf,tup);
        writePage(ovf, p, buf);
        inserted = 1;
    }
    if (!inserted)
        { p = ovFlow(buf); }
}
// add a new ovflow page
if (!inserted) {
    ovflow(buf) = nPages(ovf);
    outf = inOvflow ? ovf : f;
    writePage(outf, p, buf);
    clear(buf);
    addTuple(buf, tup);
    writePage(ovf, nPages(ovf), buf);
}
// tuple inserted



Splitting105/160

How to decide that we "need to split"?

In extendible hashing, blocks were split on overflow.

In linear hashing, we always split block sp.

Two approaches to triggering a split:

 split every time a tuple is inserted into full block
 split when load factor reaches threshold (every k inserts)



... Splitting106/160

How do we accomplish partitioning?

All tuples in sp plus its overflow blocks have same hash (e.g. 01).

New block is at address sp+2d.

We consider d+1 bits of hash (giving e.g. 001, 101).

Gives addresses for each tuple of sp or sp+2d.

Place tuples according to d+1 hash bits.


... Splitting107/160

Partitioning process for block sp=01:






... Splitting108/160

Some observations on splitting:

 before split, page sp has e.g. n ovflow pages
 after split, page sp has e.g. n/2 ovflow pages
 after split, page sp+2d has e.g. n/2 ovflow pages
 even if split is uneven, still have n total ovflow pages

Simplest if we maintain free list of overflow pages.


... Splitting109/160

Detailed splitting algorithm:

// partitions tuples between two buckets
newp = sp + 2^d; oldp = sp;
buf = readPage(f,sp);
clear(oldBuf); clear(newBuf);
for (i = 0; i < nTuples(buf); i++) {
    tup = getTuple(buf,i);
    p = bits(d+1,hash(tup.k));
    if (p == newp) 
        addTuple(newBuf,tup);
    else
        addTuple(oldBuf,tup);
}
p = ovflow(buf);  oldOv = newOv = 0;
while (p != NO_PAGE) {
    ovbuf = readPage(ovf,p);
    for (i = 0; i < nTuples(ovbuf); i++) {
        tup = getTuple(buf,i);
        p = bits(d+1,hash(tup.k));
        if (p == newp) {
            if (isFull(newBuf)) {
                nextp = nextFree(ovf);
                ovflow(newBuf) = nextp;
                outf = newOv ? f : ovf;
                writePage(outf, newp, newBuf);
                newOv++; newp = nextp; clear(newBuf);
            }
            addTuple(newBuf, tup);
        }
        else {
            if (isFull(oldBuf)) {
                nextp = nextFree(ovf);
                ovflow(oldBuf) = nextp;
                outf = oldOv ? f : ovf;
                writePage(outf, oldp, oldBuf);
          	oldOv++; oldp = nextp; clear(oldBuf);
            }
            addTuple(oldBuf, tup);
        }
    }
    addToFreeList(ovf,p);
    p = ovflow(buf);
}
sp++;
if (sp == 2^d) { d++; sp = 0; }



Insertion Cost110/160

If no split required, cost same as for standard hashing:

Best Costinsert  =  1r + 1w

Average Costinsert  =  (1+Ov)r + 1w

Worst Costinsert  =  (1+max(Ov))r + 1w


... Insertion Cost111/160

If split occurs, incur Costinsert plus cost of partitioning.

Partitioning cost involves:

 reading block sp   (plus all of its overflow blocks)
 writing block sp   (and its new overflow blocks)
 writing block sp+2d   (and its new overflow blocks)

On average,
Costpartition  =  (1+Ov)r + (2+Ov)w


Deletion with Lin.Hashing112/160

Deletion is similar to ordinary static hash file.

But might wish to contract file when enough tuples removed.

Rationale: r shrinks, b stays large ⇒ wasted space.

Method: remove last block in file (contracts linearly).

Involves a coalesce procedure which is an inverse split.


Hash Files in PostgreSQL113/160

PostgreSQL uses linear hashing on tables which have been:
create index Ix on R using hash (k);

Hash file implementation: backend/access/hash

 hashfunc.c ... a family of hash functions
 hashinsert.c ... insert, with overflows
 hashpage.c ... utilities + splitting
 hashsearch.c ... iterator for hash files

Note: "bucket" = data page + associated ovflow pages


... Hash Files in PostgreSQL114/160

PostgreSQL uses slightly different file organisation ...

 has a single file containing main and overflow pages
 has groups of main pages of size 2n
 in between groups, arbitrary number of overflow pages
 maintains collection of "split pointers" in header page
 each split pointer indicates start of main page group

If overflow pages become empty, add to free list and re-use.


... Hash Files in PostgreSQL115/160

PostgreSQL hash file structure:





Indexes



Selection via Trees117/160

Tree-based file access methods

 developed from the notion of indexes 
	(calling file access methods "indexing schemes" probably derives from this)
 are the oldest kinds of access methods
 are the most widely available inside commercial RDBMSs
 have been developed for most query types



Indexing118/160

The basic idea behind an index is as for books.



A table of (key,usage) pairs:

 order by key   (for efficient lookup)
 usage is a list of places where the key occurs



Indexing File Structure119/160






Indexes120/160

A 1-d index is based on the value of a single attribute A.

Some possible properties of A:

 may be used to sort the file
	  (or may be sorted on some other field)
 values may be unique
	  (or there may be multiple instances)

Taxonomy of index types, based on properties of index attribute:


 primary 
 index on unique field, file sorted on this field 


 clustering 
 index on non-unique field, file sorted on this field 


 seconday 
 file not sorted on this field 




... Indexes121/160

Indexes themselves may be structured in several ways:


 dense 
 every tuple is referenced by an entry in the index file 


 sparse 
 only some tuples are referenced by index file entries 


 single-level 
 tuples are accessed directly from the main index file 


 multi-level 
 may need to access several index pages to reach tuple 


Index structures aim to minimise storage and search costs.


... Indexes122/160

A relation/file may have:

 an index on a single (key) attribute 
	  (useful for handling primary/secondary key queries e.g. one, range)
 separate indexes on many attributes 
	  (useful for handling queries involving several fields e.g. pmr, space)
 a combined index incorporating many attributes 
	  (multi-dimensional indexing for e.g. spatial, multimedia databases)

Indexes are typically stored in separate files to data
	  (possibly cached in memory).


Primary Index123/160

Primary index: ordered file whose "tuples" have two fields:

 key value (for primary key attribute)
 tuple id (for tuple containing key value)

Dense: one index tuple per data tuple
	  (no requirement on data file ordering)

Sparse: one index tuple per data page  
	  (requires primary key sorted data file)

Index file has blocking factor ci
	  (where typically ci ≫ c)

Index has total i blocks:   
Dense: i = ⌈ r/ci ⌉   
Sparse: i = ⌈ b/ci ⌉


... Primary Index124/160

One possible implementation for index blocks:
typedef struct {
    IndexHeader  info,
    IndexEntry[] entries
} IndexBlock;

typedef struct {
    Datum   key,
    int     pageNum,
    int     tupleNum 
} IndexEntry;

Note that pageNum+tupleNum is equivalent to TupleId.


Dense Primary Index125/160






Sparse Primary Index126/160






Index Overheads127/160

Consider a relation and index with the following parameters:

 data file tuples sorted on integer key A
 page size B=4096 bytes  
	(for both data and index files)
 each data page has 256-byte header   ⇒   3840 bytes for tuples
 data tuple size R=50 bytes
	  ⇒   tuples/page c=76
 total number of tuples r=16,000
	  ⇒   total data pages b=211
 key value is integer so 4 bytes, assume TupleID is 4 bytes
 since index entry is (value,TupleID)   ⇒   index entry is 8-bytes
 assume that each index file page has a 64-byte header

(continued)


... Index Overheads128/160

For each page in the index file ...

 Bi = space available for index entries = 4096-64 = 4032
 index entries per page ci = ⌊Bi/8⌋ = 504

Size of index file:

 dense index: one index entry for every tuple

 index pages i = ⌈ r/ci ⌉ = ⌈ 16,000/504 ⌉ = 32

 sparse index: one index entry for page (min key value)

 index pages i = ⌈ b/ci ⌉ = ⌈ 211/504 ⌉ = 1




Selection with Prim.Index129/160

For one queries:
ix = binary search index for entry with key K
if nothing found { return NotFound }
b = getPage(ix.pageNum)
t = getTuple(b,ix.tupleNum)
   -- may require reading overflow pages
return t

Worst cost:   read log2i index pages  +  read 1+Ov data pages.

Thus, Costone,prim  =  log2 i + 1 + Ov


... Selection with Prim.Index130/160

For range queries on primary key:

 use index search to find lower bound
 read sequentially until reach upper bound

For pmr queries involving primary key:

 search as if performing one query. 

For space queries involving primary key:

 as for above cases

For queries not involving primary key, index gives no help.


Insertion with Prim.Index131/160

Overview:
insert tuple into page P
find location for new entry in index file
  // could check whether it already exists
insert new index entry (k,P,i) into index file

Problem: order of index entries must be maintained.

 can't user overflow pages; index search must be fast
 need to reorganise index file


On average, this requires us to read/write half of index file.

Costinsert,prim  = 
 log2ir + i/2.(1r+1w) + (1+Ov)r + (1+δ)w



Deletion with Prim.Index132/160

Overview:
find tuple using index
mark tuple as deleted
remove index entry for tuple

If we delete index entries by marking ...

 Costdelete,prim  = 
 (log2 i + 1 + Ov)r + 2w

If we delete index entry by index file reorganisation ...

 Costdelete,prim  = 
 (log2 i + 1 + Ov)r + i/2.(1r+1w) + 1w



Clustering Index133/160

Index on non-unique ordering attribute Ac.

Usually a sparse index; one pointer to first tuple containing value.

Assists with:

 range queries on Ac
	  (find lower bound, then scan)
 pmr queries involving Ac
	  (search index for specified value)
 ordered scan with Ac as ordering key



... Clustering Index134/160






... Clustering Index135/160

Insertions are expensive: rearrange index file and data file.

This applies whether new value or new tuple containing known value.

One "optimisation": reserve whole block for one particular value.

Deletions relatively cheap (similar to primary index).

(Note: can't mark index entry for value X until all X tuples are deleted)


Secondary Index136/160

Generally, dense index on non-unique attribute As

 data file is not ordered on attribute As
 index file is ordered on attribute As

Problem: multiple tuples with same value for As.

Three possible solutions:

 have several index entries for value
 have variable length index entries 
	(key value + multiple tupleId's)
 have blocks of tuple addresses, referenced by ``primary index''



... Secondary Index137/160

First style of secondary index:





... Secondary Index138/160

Third (favoured) style of secondary index:





Selection with Sec.Index139/160

Since non-unique attribute, there are no (explicit) one queries.

For pmr queries involving As:
binary search for key K in index file 1
for each entry for K in index file 2 {
    b = getPage(ix.pageNum)
    search for tuple within buffer b
}

Assume that answering the query q requires:

 binary search of log2i index file 1 pages
 read aq pages in index file 2
 read bq data blocks + overflow blocks

Costpmr  = 
 (log2i + aq + bq.(1 + Ov))r


... Selection with Sec.Index140/160

For range queries e.g. Lo≤ As ≤Hi:
binary search for key Lo in index file 1
FOR each entry in index file 2 until Hi DO
    access tuple R via TupleID from index
END

Analysis almost same as for pmr ...

Costrange  = 
 (log2i + aq + ∑k=LoHi(bk.(1 + Ov)))r

Note: may read some blocks multiple times 
	  (alleviate with buffer pool) 


Insertion/Deletion with Sec.Index141/160

Insertion:

As for primary index:

 may need overflow blocks in data file
 may need to rearrange index file

Deletion:

Can use mark-style (tombstone) deletion for tuples.

Caution: can only mark index-entry for k when
no more entries for k in block-address blocks.


Multi-level Indexes142/160

In indexes described so far, search begins with binary search of index.

Requires log2 i index block accesses.

We can improve this by putting a second index on the first index.

Second index is sparse; (key,pointer) to first entry in first index block.

If there are i first-level index blocks, 
there will be i2 = ⌈ i/ci ⌉  second-level index blocks.

If second-level index too big, add third level ...

Maximum levels:   t  =  ⌈ logci r ⌉     (dense index)

Top-level of index is always just a single block.


... Multi-level Indexes143/160






Select with ML.Index144/160

For one query on indexed key field:
I = top level index block
for level = t to 1 {
    read index block I
    search index block for J'th entry
        where index[J].key <= K < index[J+1].key
    if J=0 { return NotFound }
    I = index[J].block
}
-- I is now address of data block
search block I and its overflow blocks

Read t index blocks and 1+Ov data blocks.

Thus, Costone,mli  =  (t + 1 + Ov)r

(Recall that t = ⌈ logcir ⌉ and single-level index needs log2i)


B-Trees145/160

B-trees are MSTs with the properties:

 they are updated so as to remain balanced
 each node has at least (n-1)/2 entries in it
 each tree node occupies an entire disk page

B-tree insertion and deletion methods

 are moderately complicated to describe
 can be implemented very efficiently

Advantages of B-trees over general MSTs

 better storage utilisation (around 2/3 full)
 better worst case performance (shallower)



... B-Trees146/160

Example B-tree (depth=3, n=3):





B-Tree Depth147/160

Depth depends on effective branching factor
	 (i.e. how full nodes are).

Simulation studies (random insertions and deletions) show typical B-tree nodes are 69% full.

Gives   load Li = 0.69 × ci
  and  
depth of tree ~ ⌈ logLi r ⌉.

Example: ci=128,    Li=88


 Level 
 #nodes 
 #keys 


 root 
 1 
 87 


 1 
 88 
 7656 


 2 
 7744 
 673728 


 3 
 681472 
 59288064 


Note: ci is generally larger than 128 for a real B-tree.


B+Trees148/160

In database context, nodes are index blocks.

Two possible ways to structure them:

 B trees:

 every entry contains (key, data block) pair
 have separate index block pointers

 B+ trees:

 high-level entries contain (key, index block) pairs
 first-level entries contain (key, data block) pairs


Higher levels of B+ tree have greater ci
	  ⇒   B+ tree may have less levels.


B-Tree Index149/160






B+Tree Index150/160






Insertion into B-Trees151/160

Overview of the method:

 find leaf node and position in node where entry would be stored
 if node is not full, insert entry into appropriate spot
 if node is full, split node into two half-full nodes and
 if parent full, split and promote

Note: if duplicates not allowed and key is found, may stop after step 1.


... Insertion into B-Trees152/160

When splitting a node and promoting the middle element,
we may find that the parent node is full.

In this case we split the parent in the same way as the child
and promote its middle element.

This splitting may propagate all the way to root.

In this case we create a new root containing a single entry.

This is the only exception to the half-full rule.


B-Tree Insertion Cost153/160

Insertion cost = CosttreeSearch + CosttreeInsert + CostdataInsert

Best case: write one page (most of time)

 traverse from root to leaf
 read/write data page, write updated leaf

 Costinsert  =  Dr + 1w + 1r + 1w

Common case: 3 node writes (rearrange 2 leaves + parent)

 traverse from root to leaf, holding nodes in buffer
 read/write data page
 update/write leaf, parent and new sibling

 Costinsert  =  Dr + 1r + 1w + 3w


... B-Tree Insertion Cost154/160

Worst case: 2D-1 node writes (propagate to root)

 traverse from root to leaf, holding nodes in buffer
 read/write data page
 update/write leaf, parent and sibling
 repeat previous step D-1 times

 Costinsert  =  Dr + (2D-1)w + 1r + 1w


Deletion from B-Trees155/160

Overview of the method:

 find entry in node nd
 if nd is a non-leaf node:

 remove entry
 promote its immediate successor from child node to take its place
 continue as if operation were deletion of promoted child

 if nd is a leaf node

 remove entry
 if still more than half full, compact
 if  <  half full, rearrange entries between parent
and siblings to make half full




Selection with B+Trees156/160

For one queries:
N = B-tree root node
while (N is not a leaf node) {
   scan N to find reference to next node
   N = next node
}
scan leaf node N to find entry for K
access tuple t using TupleId from N

Costone  =  (D + 1)r

Generally, D is around 2 or 3, even for very large data files.

A further optimisation is to buffer B-tree root page
	  (⇒ total D page reads)


... Selection with B+Trees157/160


For range queries (assume sorted on index attribute):
search index to find leaf node for Lo
for each leaf node entry until Hi found {
	access tuple t using TupleId from entry
}

Costrange  =  (D + bi + bq)r

(If hold root block in memory ⇒ read D-1 index blocks)


... Selection with B+Trees158/160

For pmr, need index on ≥ 1 query attribute.

Could have indexes on several attributes:
Pages = {}
for each Ai = k in query {
    find pages P containing Ai = k
    Pages = Pages ∩ P
}
for each P in Pages {
    scan P for matching tuples
}

If q mentions ai, aj, ak, then

Costpmr  = 
(Di + Dj + Dk + |bqi ∩ bqj ∩ bqk|)r

For space queries, treat as a combination of pmr and range.


B-trees in PostgreSQL159/160

PostgreSQL implements Lehman/Yao-style B-trees.

A variant that works effectively in high-concurrency environments.

B-tree implementation: backend/access/nbtree

 nbtree.c ... interface functions (for iterators)
 nbtsearch.c ... traverse index to find key value
 nbtinsert.c ... add new entry to B-tree index



... B-trees in PostgreSQL160/160

Interface functions for B-trees
// build Btree index on relation
Datum btbuild(rel,index,...)
// insert index entry into Btree
Datum btinsert(rel,key,tupleid,index,...)
// start scan on Btree index
Datum btbeginscan(rel,key,scandesc,...)
// get next tuple in a scan
Datum btgettuple(scandesc,scandir,...)
// close down a scan
Datum btendscan(scandesc)


Produced: 1 Nov 2018</p><h3 >字段5</h3><p>Selection on Multiple Attributes


Multi-dimensional (Nd) Selection



Operations for Nd Select2/150

N-dimensional select queries = condition on several attributes.

 pmr = partial-match retrieval, e.g.
select * from Employees
where  job = 'Manager' and gender = 'M';

 space = tuple-space queries, e.g.
select * from Employees
where  age > 55 and dept = 'Sales';




Tuple Space3/150

One view of N-dimensional selection on a relation R...

 attribute domains of R specify a D-dimensional space
 each tuple (v1,v2,...,vD) ∈ R is a point in that space
 queries specify values/ranges on N>1 dimensions
 queries identify a point/line/plane/region of the D-dim space
 results are tuples lying at/on/within that point/line/plane/region

E.g. if N=D, we are checking existence of a tuple at a point


Heaps4/150

Heap files can handle pmr or space using standard method:
// select * from R where Cond
f = openFile(fileName("R"),READ);
for (p = 0; p < nPages(f); p++) {
    buf = getPage(f, p);
    for (i = 0; i < nTuples(buf); i++) {
        tup = getTuple(buf,i);
        if (matches(tup,Cond))
            add tup to result set
    }
}

Costpmr  =  Costspace  =  b
    (i.e. O(n), worst case scenario)


Multiple Indexes5/150

DBMSs already support building multiple indexes on a table.

Which indexes to build, depends on which queries are asked.

If we don't know queries, index all attribute subsets:
create table R (a int, b int, c int);
create index Rax on R (a);
create index Rbx on R (b);
create index Rcx on R (c);
create index Rabx on R (a,b);
create index Racx on R (a,c);
create index Rbcx on R (b,c);
create index Rallx on R (a,b,c);



... Multiple Indexes6/150

Characteristics of indexes:

 if have index for all attributes in query ⇒ efficient
 if no query attributes are indexed ⇒ linear scan
 indexes cause space and update overhead proportional to #indexes

Since single-attribute indexes are common in DBMSs, consider first
how to use these.


... Multiple Indexes7/150

Generalised view of pmr and space queries:
select * from R
where  a1 op1 C1 and a2 op1 C2
       and ... and an opn Cn 

where, for pmr, all opi are equality tests

Possible approaches to handling such queries ...

 use index on one ai to reduce tuple tests
 use indexes on all ai, and intersect answer sets



Selection using One Index8/150

Using index on one attribute:

 requires an index for at least one attribute as used in query
 consult that index to find matches for as ops Cs
 for each matching tuple, evaluate conjuncts ai opi Ci, for i ≠ s

If several attributes have indexes:

 choose index with greatest selectivity
	  (minimum # matches)
 but requires statistics on data distributions for all indexed ai



... Selection using One Index9/150

Abstract algorithm:
// select * from R where Cond
// Cond = a1op1C1 and a2op2C2 and a3op3C3 ...
Tids = IndexLookup(R, a[i], op[i], C[i])
// gives { tid1, tid2, ...} for tuples satisfying aiopiCi
Pages = { }
foreach tid in Tids {
    if (pageOf(tid) ∉ Pages)
        Pages = Pages ∪ {pageOf(tid)}
}
f = openFile(fileName("R"),READ)
foreach page in Pages {
    Buf = getPage(f,page);
    foreach tup in Buf {
        if (matches(tup, (a1 op1 C1 ...))
            add tup to Results
}   }



... Selection using One Index10/150

Cost of this approach to selection:

 same as cost of using index on ai to answer
select * from R where ai opi Ci

 cost of index lookup   (depends on index type)
 cost of reading "matching" pages   (depends on query, i.e. bq)

Note: some pages might contain

 some tuples that match on ai opi Ci
 but no tuples that satisfy matches(tup,Cond)



Selection using Many Indexes11/150

Using indexes on several attributes:

 requires an index for >1 attribute ai used in query
 consult each index   →   Mi = set of matches on attribute ai
 form intersection of Mi   →   set of answers

Mi = { tid(t) | t ∈ R ∧ t[ai] opi Ci }

 tids can be taken from index ⇒ no data access needed



... Selection using Many Indexes12/150

Abstract algorithm:
// select * from R where a1 op1 C1 and a2 op2 C2 ...
for (i = 1; i <= #QueryTerms; i++) {
    if (no index on a[i]) continue
    Tids = IndexLookup(R, a[i], op[i], C[i])
    PageSets[i] = { }
    foreach tid in Tids {
        if (pageOf(tid) ∉ PageSets[i])
            PageSets[i] = PageSets[i] ∪ pageOf(tid)
}   }
// PageSets[i] contains pages with tuples matching (ai opi Ci)
...



... Selection using Many Indexes13/150

...
// compute intersection of Pages[i]
Pages = PageSets[1]
for (i = 2; i <= #QueryTerms; i++) {
    Pages = Pages ∩ PageSets[i];
}
// finally ... fetch data pages
f = openFile(fileName("R"),READ);
foreach page in Pages {
    Buf = getPage(f,page);
    // this page has at least 1 match
    foreach tup in Buf {
        if (matches(tup, (a1 op1 C1 ...))
            add tup to Results
}   }



Bitmap Indexes14/150

A bitmap index assists computation of result sets in pmr.






... Bitmap Indexes15/150

Structure of bitmap index:

 one bitmap for each value/range for a given attribute
 bitmap has r bits, one bit for each tuple
 if tuple i has value k for attribute a

 bitmap[a,v] has ith bit set to 1 where v=k
 bitmap[a,v] has ith bit set to 0 where v≠k

 assume we have a mapping from i to TupleIdi



... Bitmap Indexes16/150

Consider using bitmap index to solve query:
select * from Employees where dept='Sales' and gender='M'

Method:
tSales = bitmap[dept,'Sales']
tMales = bitmap[gender,'M']
matches = tSales & tMales
for (i in 0..r-1) {
   if (matches[i] == 1) {
      tid = slotToTupleId(i)
      fetch tuple t via TupleId tid
}  }



... Bitmap Indexes17/150

Costs associated with bitmap indexes ...

Storage costs:

 each bitmap requires ⌈r/8⌉ bytes
 one 8KB page can hold bitmap for 64K records 
	(⇒ ≥1 bitmap fits in each page)
 one bitmap for each value/range for each indexed attribute

Query-time costs:

 read one bitmap for each indexed attribute in query (nq)
 perform bitwise AND on bitmaps (in memory)
 read pages containing matching tuples (bq)



N-dimensional Hashing



Hashing and pmr19/150

Consider these pmr queries on a table hashed using salary
Q1: select * from Employees
    where  salary = 60000 and gender = 'M'

Q2: select * from Employees
    where  dept = 'Sales' and salary = 60K

Q3: select * from Employees
    where  salary > 60000 and dept = 'Sales'

CostQ1 = 1+Ov,
    CostQ2  =  b+bOv,
    CostQ3  =  b+bOv 


... Hashing and pmr20/150

Problem: hashing only effective if hash key used in query.

No problem if all queries involve conditions like key=val

But frequently tables are accessed in multiple ways.

Can we devise hashing that "involves"  attributes?

Yes, using multi-attribute hashing (mah) ...

 form composite hash by combining hashes from  attributes
 a query involves some attributes ⇒ some hash bits known
 using known bits, can limit number of pages we need to read



... Hashing and pmr21/150

Multi-attribute hashing ...

 combines hash values for multiple attributes
 to give a single hash value for each tuple
 uses hash to determine page in which to store tuple

Multi-attribute hashing "parameterises" the indexing scheme:

 we choose how much each ai contributes
 we choose which bits in combined hash come from ai

Not as good as hashing on one attribute; better than linear scan.


... Hashing and pmr22/150

Multi-attribute hashing parameters:

 file size = b = 2d pages
	  ⇒   use d-bit hash values
 relation has n attributes:  
	a1, a2, ...an
 attribute ai has hash function hi
 attribute ai contributes di bits
	(to the combined hash value)
 total bits d = ∑i=1n di
 a choice vector (cv) specifies for all k ... 
	bit j from hi(ai) contributes
	bit k in combined hash value



MA.Hashing Example23/150

Consider branch,acctNo,name,amount) table
(+ hash values)


 branch 
 h(B) 
 acctNo 
 h(Ac) 
 name 
 h(N) 
 amount 


 Brighton 
 1011 
 217 
 1001 
 Green 
 0101 
 750 


 Downtown 
 0000 
 101 
 0101 
 Johnson 
 1101 
 512 


 Mianus 
 1101 
 215 
 1011 
 Smith 
 0001 
 700 


 Perryridge 
 0100 
 102 
 0110 
 Hayes 
 0010 
 400 


 Redwood 
 0110 
 222 
 1110 
 Lindsay 
 1000 
 695 


 Round Hill 
 1111 
 305 
 0001 
 Turner 
 0110 
 350 


 Clearview 
 1110 
 117 
 0101 
 Throggs 
 0110 
 295 



Note that we ignore the amount attribute; we are assuming,
effectively,  that nobody will want to ask queries like
  select * from where amount=533



... MA.Hashing Example24/150

Hash parameters:   
d=3     d1=1    d2=1    d3=1

Choice vector:




This choice vector tells us:

 bit 0 in hash comes from bit 0 of h1(a1)
	  ( b1,0 )
 bit 1 in hash comes from bit 0 of h2(a2)
	  ( b2,0 )
 bit 2 in hash comes from bit 0 of h3(a3)
	  ( b3,0 )
 bit 3 in hash comes from bit 1 of h1(a1)
	  ( b1,1 )
 etc. etc.   (up to as many bits of hashing as required, e.g. 32)



... MA.Hashing Example25/150

Consider the tuple:


 branch 
 acctNo 
 name 
 amount 


 Brighton 
 217 
 Green 
 750 


Hash value (page address) is computed by:





... MA.Hashing Example26/150

Consider the tuple:


 branch 
 acctNo 
 name 
 amount 


 Downtown 
 101 
 Johnston 
 512 


Hash value (page address) is computed by:





... MA.Hashing Example27/150

Consider the tuple:


 branch 
 acctNo 
 name 
 amount 


 Round Hill 
 305 
 Turner 
 350 


Hash value (page address) is computed by:





... MA.Hashing Example28/150

Hash values for all tuples:


 tuple 
 Hash 


 (Brighton,217,Green,750) 
 111 


 (Downtown,101,Johnshon,512) 
 110 


 (Mianus,215,Smith,700) 
 111 


 (Perryridge,102,Hayes,400) 
 000 


 (Redwood,222,Lindsay,695) 
 000 


 (Round Hill,305,Turner,350) 
 011 


 (Clearview,117,Throggs,295) 
 010 




... MA.Hashing Example29/150

If inserted, in order of appearance, into a database containing 8 pages:





MA.Hashing Hash Functions30/150

Auxiliary definitions:
#define HashSize 32
typedef unsigned int HashVal;

// extracts i'th bit from hash value
#define bit(i,h) ((h) & (1 << (i)))

// choice vector elems
typedef struct { int attr, int bit } CVelem;
typedef CVelem ChoiceVec[HashSize];

// compute hash for i'th attribute of tuple t
HashVal hash(Tuple t, int i) { ... }



... MA.Hashing Hash Functions31/150

Produce combined d-bit hash value for tuple tup:
HashVal hash(Tuple tup, ChoiceVec cv, int d)
{
    HashVal h[nAttr(tup)];  // hash for each attr
    HashVal res = 0, oneBit;
    int     i, attr, bpos;
    for (i = 0; i < nAttr(tup); i++)
        h[i] = hash(tup,i);
    for (i = 0; i < d; i++) {
        attr = cv[i].attr;  bpos = cv[i].bit;
        oneBit = bit(bpos, h[attr]);
        res = res | (oneBit << (i-bpos));
    }
    return res;
}



Queries with MA.Hashing32/150

In a partial match query:

 values of some attributes are known
 values of other attributes are unknown

E.g.
select amount
from   Deposit
where  branch = 'Brighton' and name = 'Green'

for which we use the shorthand   (Brighton, ?, Green, ?)


... Queries with MA.Hashing33/150

If we try to form a composite hash for a query,
we don't know values for some bits:





... Queries with MA.Hashing34/150

How to answer this query?


 query 
 Hash 


 (Brighton, ?, Green, ?) 
 1*1 


Any matching tuples must be in pages with addresses 101 or 111.

We need to read just these pages.


... Queries with MA.Hashing35/150

Consider the query:
select amount
from   Deposit
where  name = 'Green'




Need to check pages: 100, 101, 110, 111.

(With original hashing scheme, would have read all pages)


... Queries with MA.Hashing36/150

Consider the query:
select amount
from   Deposit
where  branch='Brighton' and acctNo=217 and name='Green'




Need to check only page 111.


MA.hashing Query Algorithm37/150

// Build the partial hash value (e.g. 10*0*1)
// Treats query like tuple with some attr values missing
nstars = 0;
for each attribute i in query Q {
    if (hasValue(Q,i)) {
        set d[i] bits in composite hash
            using choice vector and hash(Q,i)
    } else {
        set d[i] *'s in composite hash
            using choice vector
        nstars++;
    }
}
...



... MA.hashing Query Algorithm38/150

...
// Use the partial hash to find candidate pages
f = openFile(fileName("R"),READ);
for (i = 0; i < 2**nstars; i++) {
    P = composite hash
    replace *'s in P
        using i and choice vector
    Buf = getPage(f, P);
    for each tuple T in Buf {
        if (T satisfies pmr query)
            add T to results
    }
}



Query Cost for MA.Hashing39/150

Multi-attribute hashing handles a range of query types, e.g.
select * from R where a=1
select * from R where d=2
select * from R where b=3 and c=4
select * from R where a=5 and b=6 and c=7

A relation with n attributes has 2n different query types.

Different query types have different costs
	  (different no. of *'s)


... Query Cost for MA.Hashing40/150

A query type Q may be defined via a set of attribute numbers, 
indicating which attributes have values specified.

Example Query Types:


 Query 
 Type 


 (v1, ?, ?, ?) 
 {1} 


 (v1, ?, v3, ?) 
 {1,3} 


 (?, ?, v3, v4) 
 {3,4} 


 (v1, v2, v3, v4) 
 {1,2,3,4} 


 (?, ?, ?, ?) 
 {}   (open query)




... Query Cost for MA.Hashing41/150

In practice, different query types are not equally likely.

E.g. maybe most of the queries asked are of the form
select * from R where a=1

with occasionally a query of the form
select * from R where b=3 and c=4

For each application, we can define a query distribution 
which gives the probability pQ of asking each query type Q.


... Query Cost for MA.Hashing42/150

Consider a query of type Q with m attributes unspecified.

Each unspecified Ai contributes di *'s.

Total number of *'s is   s  =  ∑i ∉ Q di.

⇒ Number of pages to read is   2s  =  ∏i ∉ Q 2di.

If we assume no overflow pages, Cost(Q) = 2s
	(where s is determined by Q)


... Query Cost for MA.Hashing43/150

Min query cost occurs when all attributes are used in query

Min Costpmr  =  1

Max query cost occurs when no attributes are specified

Max Costpmr  =  2d  =  b

Average cost is given by weighted sum over all query types:

Avg Costpmr  =  ∑Q pQ ∏i ∉ Q 2di

Since all query types are possible, aim to minimise average cost.


Optimising MA.Hashing Cost44/150

For a given application, the aim is to minimise Costpmr.

Can be achieved by choosing appropriate values for di
  (cv)

Heuristics:

 distribution of query types
(more bits to frequently used attributes)
 size of attribute domain
(≤ #bits to represent all values in domain)
 discriminatory power
(more bits to highly discriminating attributes)

Trade-off: making query type Qj more efficient makes
Qk less efficient.

This is a combinatorial optimisation problem, and can be handled by 
standard optimisation techniques e.g. simulated annealing.


MA.Hashing Cost Example45/150

Our example database has 16 possible query types:


 Query type 
 Cost 
 pQ 


 (?, ?, ?, ?) 
 8 
 0 


 (br, ?, ?, ?) 
 4 
 0.25 


 (?, ac, ?, ?) 
 4 
 0 


 (br, ac, ?, ?) 
 2 
 0 


 (?, ?, nm, ?) 
 4 
 0 


 (br, ?, nm, ?) 
 2 
 0 


 (?, ac, nm, ?) 
 2 
 0.25 


 (br, ac, nm, ?) 
 1 
 0 


 (?, ?, ?, amt) 
 8 
 0 


 (br, ?, ?, amt) 
 4 
 0 


 (?, ac, ?, amt) 
 4 
 0 


 (br, ac, ?, amt) 
 2 
 0 


 (?, ?, nm, amt) 
 4 
 0 


 (br, ?, nm, amt) 
 2 
 0.5 


 (?, ac, nm, amt) 
 2 
 0 


 (br, ac, nm, amt) 
 1 
 0 


Cost values are based on earlier choice vector
	  (dbr = dac = dnm = 1) 
pQ values can be determined by observation of DB use.


... MA.Hashing Cost Example46/150

Consider r=106, Nr=100, b=104, d=14.

Attribute br occurs in 0.5+0.25 used query types 
⇒ allocate many bits to it e.g. d1=6.

Attribute nm occurs in 0.5+0.25 of queries 
⇒ allocate many bits to it e.g. d3=4.

Attribute amt occurs in 0.5 of queries 
⇒ allocate less bits to it e.g. d4=2.

Attribute ac occurs in 0.25 of queries 
⇒ allocate least bits to it e.g. d2=2.


... MA.Hashing Cost Example47/150

With bits distributed as: d1=6, d2=2, d3=4, d4=2


 Query type 
 Cost 
 pQ 


 (br, ?, ?, ?) 
 28 = 256 
 0.25 


 (?, ac, nm, ?) 
 28 = 256 
 0.25 


 (br, ?, nm, amt) 
 22 = 4 
 0.5 



Cost  =  0.5 × 22 + 0.25 × 28 + 0.25 × 28  =  130


MA.Hashing vs One-Attribute Hashing48/150

Multi-attribute hashing is basically just a different
way of forming hash value for each tuple.

Hence, can be used with all flexible hashing schemes.

As the file grows, the di's are increased one by one,
using the choice vector.

Difference between mah and standard hashing: 
queries "suggest" a set of pages rather than a single page.

Cost for insertion, deletion, ordered scan is same
as for underlying hashing scheme.


Grid Files49/150

The mah approach attempts to combine all attributes in the hash value.

Alternative approach is to keep hash values separate
and use auxiliary structure to provide combination.

A generalisation of extendible hashing, called grid files
provides this.

Ext.hashing uses a directory on one attribute.

Grid files use a k-dimensional directory to handle k attributes.


... Grid Files50/150






A simple 2-dimensional grid file



Selection with Grid Files51/150

Consider the following grid file directory:



for a table R with two attributes a and b


... Selection with Grid Files52/150

If  d1=3  and  d2=2 ...


 select...where a=C1 and b=C2 
 one cell 


 select...where a=C1 
 one row (four cells)


 select...where b=C2 
 one column (eight cells)



Number of directory cells visited is similar to number of pages visted in mah.



... Selection with Grid Files53/150

Execution of select...where b=C2:



where  hash(C1) = ...001  and  hash(C2) = ...110


... Selection with Grid Files54/150

Selection for pmr in a 2d grid file ( nAttrs = d )
for (i = 0; i < nAttr(tup); i++) {
    if (!hasValue(tup,i))
        { lo[i] = 0; hi[i] = 2**d-1 }
    else {
        val = bits(d[i], getAttr(tup,i))
        lo[i] = val; hi[i] = val
}   }
Pages = []
for (i = lo[1]; i <= hi[1]; i++) {
    for (j = lo[2]; j <= lo[2]; j++) {
        if (!(grid[i][j] in Pages))
            add grid[i][j] to Pages
}   }
for each P in Pages {
    Buf = getPage(f,P)
    check Buf for solutions
}



Query Cost for Grid Files55/150

Cost depends on:

 how many attributes are unspecified in query
 "page density" along each dimension of grid

For pmr with all attributes specified

 determine single grid cell coordinate
 read page containing that cell
 read correpsonding data page

 C(x,y,z)  =  2 


... Query Cost for Grid Files56/150

For pmr with j < k attributes specified

 access all grid cells in k-j dimensional region
 i.e. g cells located in gq grid directory pages
 then need to read bq ≤ m ≤ g pages

C(?,y,?)  =  (gq + m) 

Typically, 1 ≤ gq ≤ 50
	  (i.e. grid directories are relatively small)


Other Operations on Grid Files57/150

Insertion has several cases:

 no splitting required   (2r + 1w)
 splitting but no dir. expansion   (2r + 3w) 
	(write the two data pages from split, plus page to update on directory cell)
 splitting + directory expansion (expensive)

Deletion can be achieved by marking (so Costpmr + bq w)

As with all hashing schemes, grid file does not help ordered scan.


N-dimensional Tree Indexes



Multi-dimensional Tree Indexes59/150

We have seen how hashing can be extended to handle multiple attributes.

Can tree-based index structures be generalised as well?

Yes ... and a very large number of techniques have
been suggested in research literature.

E.g.   BSP-trees, Cell-trees, LSD-trees, SS-trees, TV-trees, X-trees, ...

We consider three popular examples:   kd-trees, Quad-trees, R-trees.


Example 2d Relation60/150

As an example for multi-dimensional trees, consider the following relation:
create table Rel (
    A1 char(1),  # 'a'..'z'
    A2 integer   # 0..9
);

with example tuples:
Rel('a',1)  Rel('a',5)  Rel('b',2)
Rel('d',1)  Rel('d',2)  Rel('d',4)
Rel('d',8)  Rel('g',3)  Rel('j',7)
Rel('m',1)  Rel('r',5)  Rel('z',9)



... Example 2d Relation61/150

And the tuple-space for the above tuples:





kd-Trees62/150

kd-trees are multi-way search trees where

 each level of the tree uses a different attribute to partition tuples
 each node contains n-1 key values, pointers to n subtrees

For a tree with L levels and k attributes:

 if L < k, some attributes are not indexed
 if L = k, each attribute is used once in partitioning
 if L > k, "cycle through" attributes 
	(e.g. L=5, k=3 ... level 1 uses a1,
		level 2 uses a2,
		level 3 uses a1,
		level 4 uses a2,
		level 5 uses a1)

Partitions "tuple space" into smaller and smaller regions at each level.


Example kd-Tree63/150




Each leaf node contains a reference to a bucket containing tuples from
a small region of k-dimensional space.


... Example kd-Tree64/150

How this tree partititons the tuple space:





Searching in kd-Trees65/150

Consider first how to answer pmr queries ...

If all attributes have values specified:

 use A1 value in root node
 use A2 value in first level node
 use A1 value in second level node, ...
 until reach leaf node

If e.g. attribute A2 is unspecified

 use A1 value in root node
 examine all sub-trees of first level node
 use A1 value in second level node, ...
 until reach leaf node



... Searching in kd-Trees66/150

As a tree traversal, the algorithm is best specified recursively.

Parameters for search function:

 Q ... the query, including unspecified attributes
 L ... current level in search tree
 N ... current node in search tree

Other available information:

 attrLev ... array indicating which ai for which level 

The search commences via   Search(Q, 0, kdTreeRoot)


... Searching in kd-Trees67/150

Search(Query Q, Level L, Node N)
{
    if (isLeaf(N)) {
        Buf = getPage(f,N.page)
        check Buf for matching tuples
    } else {
        ai = attrLev[L]
        if (hasValue(Q,ai)) {
            Val = getAttr(Q,ai)
            newN = search N for Val
            Search(Q, L+1, newN)
        } else {
            for each child newN of N
                Search(Q, L+1, newN)
}   }   }



... Searching in kd-Trees68/150

For space queries ...

We require a change to the above Search algorithm.
if (hasValue(Q,ai)) {
    Val = getAttr(Q,ai)
    newN = search N for Val
    Search(Q, L+1, newN)
}

becomes
if (isRange(Q,ai)) {
    for each node N on current level {
        for each value V in range(Q,ai)) {
            newN = childOf(N,V)
            Search(Q, L+1, newN)
}   }   }



Query cost for kd-Trees69/150

Different kinds of queries (i.e. different specified attributes)

 lead to different amounts of the tree being traversed
 lead to different numbers of data pages being accessed

The query cost will clearly also be affected by the depth of the tree.

If we include all n attributes as indexes, then tree has depth ≥ n.

Depth of tree is also affected by branching factor at each node.

Note: kd-trees are not necessarily balanced.


... Query cost for kd-Trees70/150

Query examples:

Open query:   (?,?,?,?,?,...)    (zero attributes specified)

 traverses entire tree (LNR,depth-first) and fetches every data page

Point query:   (a,b,c,d,e,...)   (all attributes specified)

 traverses a single path through the tree and fetches a single data page

Generic pmr query:   (a,?,c,?,e,...)

 one path from top-level node, then all paths on second-level, ...



Insertion into kd-Trees71/150

Input: tuple with all values specified.

Method:
traverse tree to leaf node
    (reaching data page P1)
if (not full P1)
    insert tuple
else {
    create new data page P2
    compute new partition point Part
    distribute tuples between P1 and P2
        (using Part)
    create new internal node N with Part
    link N into tree, link N to P1 and P2
}
write any modified pages (data or index)



... Insertion into kd-Trees72/150

Example insertion into non-full data page:





... Insertion into kd-Trees73/150

Example insertion into full data page:





kd-B-Trees74/150

kd-B-trees: a disk-based index structure using kd-tree ideas.

Aims to have a tree which is:

 is reasonably well-balanced (⇒ shallow)
 reasonable occupancy of nodes

Basic idea: distribute kd-tree structure over a number of pages:

 root page contains levels 1..m of kd-tree
 child pages contain levels m+1..2m of kd-tree,   etc.

Requires modification of insertion method to do re-balancing 
(changes to insertion make it more complex and potentially much more expensive)


... kd-B-Trees75/150

Simple example kd-B-tree
	   (max 2 kd-tree levels in each node):





... kd-B-Trees76/150

What this produces:

 each index page partitions a region of tuple space (disjoint)
 root page partitions whole space
 child nodes partition one sub-region from parent
 union of partitions on one level always covers whole space

Potential problems:

 some index pages are "under-occupied"
	  (sparse region of space)
 re-balancing can require significant tree re-organisation



Selection in kd-B-Trees77/150

To answer a pmr query:
Pages = kdBsearch(query,root)
for each page P in Pages {
    Buf = getPage(f,P)
    scan Buf for matching tuples
}

kdbSearch(Query, Page)
{
    if (Page is a data page)
        Pages = Page
    else {
        Buf = getPage(kdbf,Page)
        search kd-tree in Buf
            using attributes from Query
        Pages = []
        for each indicated external subtree S {
            P = kdbSearch(Query, S)
            add P to Pages
    }   }
    return Pages
}



... Selection in kd-B-Trees78/150

Size of kd-B-tree:

 each kd-tree node is (lchild,keyVal,rchild)
 for integer key, node could be represented in 12 bytes
 for 4KB index pages, #nodes/page is approx 300
 for balanced kd-tree, can store 8 levels per index page
 gives 128 level-8 nodes ⇒ 256 possible child index pages
 also, gives total "depth" of tree d = 2 
	(max number of page reads needed to reach any leaf kd-tree node)
 this allows us to index 64K data pages

Obviously, depth increases for not perfectly-balanced trees.


... Selection in kd-B-Trees79/150

If all attributes are specified (i.e. point search):

Costpmr  =  (d+1)

If some attributes unspecified ...

 need to search multiple paths for unknown attributes
 leading to many leaf nodes and thus many data pages

Can potentially search a large region of tree:

 which requires us to read e.g. dozens of index pages
 and may require us to read hundreds of data pages


space queries can be handled similarly to pmr queries.


Insertion/Deletion in kd-B-Trees80/150

Conceptually, insertion is same as for kd-tree:

 first perform point search to find data page
 if not full, add new tuple
 if full, add new data page and kd-node, and partition

Added complication: kd-tree is spread over many index pages

 if index page not full, split needs no new index page
 if index page is full, need to consider

 adding new index page and expanding tree
 reorganising tree within existing index page 
	(which requires re-partitioning tuples between data pages) 
	(may need to propagate re-organisation up to parent index page)




... Insertion/Deletion in kd-B-Trees81/150

Minimum insert cost is 1w
	  (no split, no change to kd-tree)

Insert cost with split and kd-tree update: 3w 
	(need to write re-partitioned data pages and modified index page)

Worst case insert cost is high (tree/data re-organisation).

Deletion is straightforward (mark as deleted).

If data page occupancy falls too low, might consider

 merging content of sparse data pages
 removing corresponding leaf nodes from kd-tree



Quad Trees82/150

Quad trees use a regular, recursive partitioning of kd tuple space.

At each level, partition space into non-overlapping kd hypercubes.

For 2d case (others are much harder to visualise):

 partition space into quadrants (NW, NE, SW, SE).
 each quadrant can be further subdivided into four,   etc.

Aim: each "leaf" partition contains approx same number of tuples.


... Quad Trees83/150

Quad-tree on example 2d tuple space:





... Quad Trees84/150

Basis for the partitioning:

 a quadrant that has no sub-partitions is a leaf quadrant
 each leaf quadrant maps to a single data page
 subdivide until points in each quadrant fit into one data page
 ideal: same number of points in each leaf quadrant (balanced)
 point density varies over space 
	⇒ different regions require different levels of partitioning
 this means that the tree is not necessarily balanced



... Quad Trees85/150

The previous partitioning gives this tree structure, e.g.





... Quad Trees86/150

Each node of a k dimensional quad-tree has 2k children.

Quad-trees originally devised as memory based structures, for spatial data.

In this domain, k = 2 or k = 3 and branching is fairly small.

For index pages in a disk-based data structure:

 if k < 6, then can store multiple nodes in a page
 if 8 ≤ k ≤ 10, then one node per page
 if k > 10, this storage structure becomes less feasible



Insertion into Quad Tree87/150

Inserting a new tuple into a 2d quad-tree indexed file involves:
traverse tree to leaf node
    (reaching data page P1)
if (not full P1)
    insert tuple
else {
    create new data pages P2, P3, P4
    distribute tuples between P1 .. P4
    create new internal node N
    link N into tree, link N to P1 .. P4
}
write any modified pages (data or index)



... Insertion into Quad Tree88/150

Example:



 


Cost
  =  
traversal cost + update cost



  =  
read ≥ 1 node pages + write ≥ 1 data pages




... Insertion into Quad Tree89/150

Potential problems with quad-trees:

 creating empty data pages during partitioning
 creating tree-nodes with 2k entries 
	(many of which are likely to be empty on the first split)

The problems can be overcome by:

 not allocating pages until there is data to go in them 
	  (so, also need to mark corresponding entries in tree-nodes)
 storing tree-nodes in a more compact form 
	  (don't store entries that don't (yet) have a pointer to a data page)



Query with Quad Tree90/150

For pmr query

 if zero attributes known, quad-tree provides no assistance
 if one attribute known, need to traverse two branches from each node
	⇒ examine half of tree
 if two attributes known, follow single branch from each node

For kd quad-tree, and query with n specified attributes:

 need to follow 2k-n branches at each level
 e.g. for k=5, n=3, follow 4 (out of 32) branches at each level



... Query with Quad Tree91/150

For space query

 query condition identifies a region of tuple-space
 need to examine all branches of tree which intersect this region

Example: 2d quad-tree with query a ≤ A1 ≤ b ∧ c ≤ A2 ≤ d

 identifies rectangular region with vertices (a,c),(a,d),(b,d),(b,c)
 read all leaf quadrants (⇒ data pages) intersecting this region



... Query with Quad Tree92/150

Space query example:



Need to traverse: red(NW), green(NW,NE,SW,SE), blue(NE,SE).


R-Trees93/150

R-trees use a flexible, hierachical partitioning of kd tuple space.

 each node in the tree represents a kd hypercube
 its children represent (possibly overlapping) subregions 
 the child regions do not need to cover the entire parent region

Could be viewed as a more flexible version of quad-tree.


... R-Trees94/150

Example 2d R-tree node (with two levels of children):





... R-Trees95/150






... R-Trees96/150

The R-tree was initally designed for indexing polygons 
   (via their minimum bounding rectangles (MBR))

It adapts naturally to tuple-space points (i.e. tuples) 
   (each MBR represents a region of the tuple-space)

Goals in constructing R-trees (cf. B-trees)

 every node, except root, contains between m and M entries
 the root node has at least two entries, unless it is also a leaf
 the tree is height-balanced  (all data pages at same level)



Query with R-trees97/150

Designed to handle space queries and "where-am-I" queries.

"Where-am-I" query: find all regions containing a given point P:

 start at root, select all children whose subregions contain P
 if there are zero such regions, search finishes with P not found
 otherwise, recursively search within node for each subregion
 once we reach a leaf, we know that that region contains P

Space (region) queries are handled in a similar way

 we traverse down any path that intersects the query region



... Query with R-trees98/150

Algorithm for "where am I" query in 2d R-tree:
Regions = RtreeSearch(a,b,root)

RtreeSearch(x,y,Node) {
    Regions = []
    if (leaf(Node)) {
        for each (x1,y1,x2,y2,P) in Node {
            if (x1,y1,x2,y2) contains (a,b) {
                add (x1,y1,x2,y2) to Regions
    }   }   }
    else {
        for each (x1,y1,x2,y2,Child) in Node {
            if (x1,y1,x2,y2) contains (a,b) {
                Rs = RtreeSearch(x,y,Child)
                add Rs to Regions
    }   }   }
    return Regions;
}



Insertion into R-tree99/150

Insertion of an object R occurs as follows:

 start at root, look for children that completely contain R
 if no child completely contains R,
	choose one of the children
	and expand it so that it does contain R
 if several children contain R, choose one and proceed to child
 repeat above containment search in children of the current node
 once we reach data page, insert R if there is room
 if no room in data page, replace by two data pages
 partition existing objects between two data pages
 update node pointing to data pages 
	(may cause B-tree-like propagation of node changes up into tree)

Note that R may be a point or a polygon.


... Insertion into R-tree100/150

Heuristics in R-tree insertion ...

Choose child to expand to contain R

 maximise overlap with child region; minimise expansion of child

Choose a child (from several) to contain R

 use information about regions further down tree
 if not available, make an arbitrary choice

Partition objects between old/new data pages

 requires finding a suitable "split point"
 different to B-tree (1d) where can always find 50:50 split point
 for d>1, cannot generally do this without overlap (minimise)
 common technique for splitting: quadratic (time) split



... Insertion into R-tree101/150

Quadratic split method to partition objects in S:
find objects a and b in S
    that produce the largest MBR
place a in S1
place b in S2
for every other object c in the S {
    s1 = current size of S1
    i1 = size of S1 with c added
    s2 = current size of S2
    i2 = size of S2 with c added
    if ((s1-i1) < (s2-i2))
        add c to S1
    else
        add c to S2
}



R-tree variants102/150

R+ tree does not permit overlapping regions.

 objects that intersect n regions are stored n times 
   (i.e. stored in each region that they intersect)

R* tree uses delayed splitting on insertion.

 the aim is to minimise region overlap, within original framework

SS-tree uses (hyper)spherical regions rather than (hyper)cubes

 the aim is to reduce amount of overlap among regions



R-trees in PostgreSQL103/150

Up to version 8.2, PostgreSQL had separate R-tree implementation

 src/backend/access/rtree
 rtget.c ... iterator for R-tree scans
 rtree.c ... R-tree construction operations
 uses quadratic split (rtpicksplit() in rtree.c)

From version 8.2 on, R-trees implemented using GiST.


GiST in PostgreSQL104/150

PostgreSQL provides an implementation of Generalized Search Trees (GiST).

Above discussion of tree-based indexes shows commonalities:

 trees are built of nodes containing many index entries
 choice of branches is determined by a predicate p
 for each index entry (p,ptr) in a leaf node, 
	page ptr holds tuples staisfying p
 for each index entry (p,ptr) in a non-leaf node, 
	all tuples reachable via ptr satisfy p

Some examples:

 for B-trees, predicates are order operators on keys
 for R-trees, predicates are containment on MBRs
 for kd-trees, predicates alternate keys on each level



... GiST in PostgreSQL105/150

GiST trees have the following structural constraints:

 every node is at least fraction f full (e.g. 0.5)
 the root node has at least two children (unless also a leaf)
 all leaves appear at the same level

However, the "predicate" constraints are looser than
those for any trees discussion so far.

Users are free to implement their own p
to produce their own "flavour" of search tree.

Details: src/backend/access/gist


Signature-based Selection



Indexing with Signatures107/150

Signature-based indexing: an "efficient" linear scan.

Why abandon sub-linear complexity?    (e.g. O(1) for hashing)

Arising from work with high-d feature vectors in multimedia:

 all indexing methods degenerate to O(n) as d increases
 absolute bound on d before linear scan is best d = 610
 in practice, most methods degenerate for 10 ≤ d ≤ 40

We consider two applications of signatures to indexing:

 superimposed codewords for pmr queries on relational data
 VA files for kNN queries on multimedia data



... Indexing with Signatures108/150

A signature is a compact (lossy) descriptor for a tuple.

Scanning the whole data file is too expensive, so

 maintain a signature file in parallel with the data file
 to answer queries, scan all signatures to choose likely tuples
 fetch only data pages which are now known to contain "matches"

For this to be better than a linear scan of the data, require:

 signatures are substantially smaller than data tuples
 cost of checking signatures is much less than checking tuples
 that signatures must provide effective filtering
	  (minimal false matches)



Signature Files109/150

File organisation for signature indexing (two files)



(One signature slot per tuple slot in the data file; unused signature slots are zeroed)


... Signature Files110/150

Note that signatures do not determine tuple placement.

The only requirement is that

 after a tuple has been placed in the data file
 a signature must be placed in the corresponding slot in the signature file

(clearly, this means that if there are r tuples in the data file, there will be r signatures)

The data file could be organised via hashing, B-tree, curves, etc.

Thus, signatures can be used in conjunction with other indexing schemes.



Signatures111/150

Signatures (descriptors)

 must ``summarise'' all (indexed) fields in a tuple
 must be able to be "checked" efficiently in queries

There are two main types of tuple signature:

 superimposed codewords
	  (overlay "signatures" for all attributes)
 disjoint codewords
	  (concatenate "signatures" for all attributes)

We use the following terminology consistently:

 descriptor = signature for an entire tuple
 codeword = signature for a single attribute



... Signatures112/150

A tuple r consists of n attributes A1..An.

A codeword cw(Ai) is

 a bit-string, m bits long, where k bits are set to 1 (k ≪ m) 
(this kind of representation is called k-in-m encoding)
 derived from the value of a single attribute  Ai

A tuple descriptor is built by combining n codewords.


Generating Codewords113/150

A method for generating a k-in-m codeword for attribute Ai
seed = hash(A[i])
nbits = 0
codeword = 0  /* m zero bits */
while (nbits < k) {
    i = random(0,m-1)
    if (bit i not set in codeword) {
        set bit i in codeword
        nbits++
    }
}

Requires bit-string operations and a pseudo-random number generator.


Superimposed Codewords (SIMC)114/150

In a superimposed codewords (simc) indexing scheme

 a tuple descriptor is formed by overlaying attribute codewords

A tuple descriptor desc(r) is

 a bit-string, m bits long, where j ≤ nk bits are set to 1
 formed by bitwise OR of n attribute codewords
 desc(r) = cw(A1) OR cw(A2) OR ... OR cw(An)



SIMC Example115/150

Consider the following tuple (from a bank deposit database)


 branch 
 acctNo 
 name 
 amount 


 Perryridge 
 102 
 Hayes 
 400 


It has the following codewords/descriptor (for m = 12,   k = 2)


 Ai 
 cw(Ai) 


 Perryridge 
 010000000001 


 102        
 000000000011 


 Hayes      
 000001000100 


 400        
 000010000100 


 desc(r)  
 010011000111 




... SIMC Example116/150

Consider a very small example database, with associated signatures:


 Signature 
 Tuple 


 100101001001 
 (Brighton,217,Green,750) 


 010101010101 
 (Clearview,117,Throggs,295) 


 101001001001 
 (Downtown,101,Johnshon,512) 


 101100000011 
 (Mianus,215,Smith,700) 


 010011000111 
 (Perryridge,102,Hayes,400) 


 100101010011 
 (Redwood,222,Lindsay,695) 


 011110111010 
 (Round Hill,305,Turner,350) 




SIMC Queries117/150

To answer query q in SIMC

 first generate a query descriptor desc(q)
 then use the query descriptor to search the signature file


desc(q) is formed by bitwise OR of codewords for supplied attributes.

E.g. consider the query (Perryridge, ?, ?, ?).


 Ai 
 cw(Ai) 


 Perryridge 
 010000000001 


 ?        
 000000000000 


 ?        
 000000000000 


 ?        
 000000000000 


 desc(q)  
 010000000001 




... SIMC Queries118/150

Once we have a query descriptor, we search the signature file:
for each descriptor D[i] in signature file {
    if (D[i] matches desc(q))
        mark tuple R[i] as a match
}
for each marked tuple R {
    fetch tuple R
}

The critical assumption to make this approach viable:


scanning the signature file and comparing descriptors 
is faster than scanning the tuple file and checking tuples



... SIMC Queries119/150

What sort of signature comparison is required to answer the example query?

Clearly, any matching tuple must have the value "Perryridge" for A1.

Any tuple with "Perryridge" must have these bits 010000000001 set.

So, checking whether each descriptor has these bits set, gives us the matches.

This can be generalised to multiple supplied values in the query:

 we superimpose all the codewords for all supplied values
 any matching tuples must have all of those bits set

In other words ... 

for any tuple r that is a match for query q 
the 1-bits in desc(q) are a subset of the 1-bits in desc(r)



... SIMC Queries120/150

Query/tuple descriptor matching can be implemented efficiently using 
logic operations on bit-strings.

Note that if   bits(A) ⊆ bits(B)   then   A AND B = A.

Thus, the query/tuple descriptor matching can be implemented as:


matches(Ri,q) if desc(q) AND desc(Ri) = desc(q)



Example SIMC Query121/150

Consider the query and the example database:


 Signature 
 Tuple 


 010000000001 
 (Perryridge,?,?,?) 


 100101001001 
 (Brighton,217,Green,750) 


 010101010101 
 (Clearview,117,Throggs,295) 


 101001001001 
 (Downtown,101,Johnshon,512) 


 101100000011 
 (Mianus,215,Smith,700) 


 010011000111 
 (Perryridge,102,Hayes,400) 


 100101010011 
 (Redwood,222,Lindsay,695) 


 011110111010 
 (Round Hill,305,Turner,350) 




... Example SIMC Query122/150

The query has potential matches:


 branch 
 acctNo 
 name 
 amount 


 Clearview 
 117 
 Throggs 
 295 


 Perryridge 
 102 
 Hayes 
 400 


The first is an example of a false match.

False matches are caused by:

 codeword hashing collisions
	  (two different values generate same codeword)
 ``unfortunate'' overlapping
	  (bitwise OR from several attributes)



False Matches123/150

Example:


 Ai 
 cw(Ai) 


 Perryridge 
 010000000001 


 102        
 000000000011 


 Hayes      
 000001000100 


 400        
 000010000100 


 desc(r)  
 010011000111 


 Ai 
 cw(Ai) 


 Clearview  
 010000010000 


 117        
 000100000001 


 Throggs    
 000001000100 


 295        
 000100000100 


 desc(r)  
 010101010101 




... False Matches124/150

How to reduce likelihood of false matches?

 hash each attribute using a different hash function
	  (hi for Ai)
 increase descriptor size (m)
 choose k so that ≅ half of bits are set
	  (maximises different possible descriptors)

Increasing m helps, but at the expense of reading more descriptor data.

Having k too high  ⇒  increased overlapping. 

Having k too low  ⇒  increased codeword collisions.

Thus, for SIMC schemes, there is the notion of choosing optimal m, k.


Design with SIMC125/150

SIMC schemes have the notion of false match probability pF.

As suggested above, pF is affected by m, k and n (#attributes)

In designing a SIMC index for a data file, the aim is to choose
indexing parameters (m, k) to minimise the
likelihood of false matches.

 start by choosing acceptable pF  
	(e.g. pF ≤ 10-5 i.e. one false match in 10,000)
 then choose m and k to achieve no more than this pF.

Formulae to derive m and k given pF and n:


 k  =  1/loge2 . loge ( 1/pF ) 

 m  =  ( 1/loge 2 )2 . n . loge ( 1/pF ) 



... Design with SIMC126/150

Design example:

 a relation with 4 attributes ( n=4 )
 with acceptable false match probability pF=10-5
 optimal indexing parameters are    m=96,   k=16  
	(i.e. 12-byte descriptors)



Query Cost for SIMC127/150

A page-oriented view of how SIMC queries are answered:
determine query descriptor QD
for each page SB of tuple descriptors {
    for each tuple descriptor RD in page SB {
        if (RD matches QD) {
	    add tuple R to list of matches
            add pageOf(R) to Pages
        }
    }
}
for each page P in Pages {
    Buf = getPage(f,P)
    check Buf for matching tuples
}

With a buffer manager, the previous tuple-oriented version would behave like this.


... Query Cost for SIMC128/150

To answer pmr query, must read r signatures.

Since one signature per tuple, cannot feasibly store them in memory.

If signature contains m bits, can fit ND = ⌊ 8B/m ⌋ signatures per page.

Thus, must read bD = ⌈ r/ND ⌉ pages of signature data.

E.g. m=32,   B=1024,   r=105     ⇒     ND = 256,   bD=391 

How many data pages do we read?


... Query Cost for SIMC129/150

Number of data pages read depends on:

 number of matching tuples rq
 number of false matches rF
 how matching tuples are distributed

Total tuples to read = rq + rF.

Expected false matches = rF  =  (r - rq).pF.

Assume matching tuples are uniformly distributed over the data pages.

Then total data pages read = 
 bq  =  ⌈ (rq+rF)/r × b ⌉

Costpmr  =  bD + bq


... Query Cost for SIMC130/150

For one queries, can use same optimisation as for Heaps.

That is, stop the search as soon as the single matching tuple is found.

The average cost in the case would be:

 reading half of the descriptors
	  bD/2
 reading the page containing the match 
 reading some page(s) containing false matches
	  δ ≤ rF

Costone :   Best = 2     Average = bD/2 + δ     Worst = bD + b


... Query Cost for SIMC131/150

SIMC provides no assistance in answering range, space queries, unless the data file is sorted. 

If the data file is sorted,

 search for the first tuple with the low value
	  (as for a one query)
 then use sequential scan to find the rest


SIMC provides no asistance at all for sim queries   (linear scan).


Applications of SIMC132/150

SIMC is useful for pmr, which allows a wide range of query types, e.g.
select * from R where a=1
select * from R where a=3 and c=1 and d=4
select * from R where a=2 and b=3 and c=4 and d=5

The above discussion has assumed that n attributes are indexed.

In fact, SIMC is reasonably flexible about how many attributes are involved

Thus, it also has applications in information retrieval:

 form a document signature by superimposing codewords for each keyword in the document
 form a query signature by superimposing codewords for each keyword in the query
 allows us to find a set of documents containing all query keywords



Two-level SIMC133/150

Scanning one descriptor for every tuple is not efficient.

How to reduce number of descriptors?

Have a smaller number of larger descriptors.

E.g. one descriptor for each data page.

Every attribute of every tuple in page contributes to descriptor.

How large is a page descriptor (BD)?

Use formulae above, but with Nr.n ``attributes''.

Typically, pages are 1..8KB  ⇒  8..64 BD/page (NBD).


Two-Level SIMC Files134/150

File organisation for two-level superimposed codeword index





... Two-Level SIMC Files135/150

Alternative file organisation:



(Assumes that we need to read the data pages anyway, so may as well include tuple descriptors there ... or simply omit them altogether)


Queries with Two-level SIMC136/150

Method for answering queries:
form query descriptor QD
for each page descriptor BD {
    if (BD matches QD)
        add P to Pages
}
for each page P in Pages {
    read page P
    check for matching tuples
    // could do this using tuple descriptors
}

Costpmr  =  ⌈ b/NBD ⌉ + bq


Bit-sliced SIMC137/150

Reading all page descriptors is still not efficient,
especially when only a few bits are set.

To improve efficiency, reconsider the matching process:




 Signature 
 Tuple 


 010000000001 
 (Perryridge,?,?,?) 


 100101001001 
 (Brighton,217,Green,750) 


 010101010101 
 (Clearview,117,Throggs,295) 


 101001001001 
 (Downtown,101,Johnshon,512) 


 101100000011 
 (Mianus,215,Smith,700) 


 010011000111 
 (Perryridge,102,Hayes,400) 


 100101010011 
 (Redwood,222,Lindsay,695) 


 011110111010 
 (Round Hill,305,Turner,350) 





... Bit-sliced SIMC138/150

Rather than storing b m-bit page descriptors,
store m b-bit descriptor slices.






... Bit-sliced SIMC139/150

To answer queries:
form query descriptor QD
// Result is a bit-slice
Result = AllOnes
for each bit b set in QD {
    read bit-slice b
    Result = Result AND b
}

Result has bit i set if page i is candidate page.

Queries much more efficient because often only a few bits set in QD.

However, updates are expensive ... need to set k different slices
for each insert.

Changing b is very expensive.


Query Cost for Bit-sliced SIMC140/150

A major cost in SIMC queries is reading the page-descriptor slices.

Each slice is b bits long (one bit for each data file page).

Thus, each slice occupies ⌊ 8B/b ⌋ bytes.

Generally, we can fit several page descriptor slices per page.

However, we make the pessimistic assumption ...


reading a slice means reading one page from the page descriptor slices file



... Query Cost for Bit-sliced SIMC141/150

Consider a query (val1, ?, val2, ?, ...) for an n attribute relation:

 if m attribute values are supplied in the query
 then approximately mk bits are set in the query page-descriptor
 and so we need to read mk descriptor slices

In fact, we can optimise further by always
reading just the first j of these mk slices.

Thus, the descriptor-reading cost in queries can be made constant.

The downside of this trick is a (slight) increase in likelihood of false matches.

Costpmr   =   j + bq


Disjoint Codewords (DJC)142/150

In a disjoint codewords (djc) indexing scheme

 a tuple descriptor is formed by concatenating attribute codewords

A tuple descriptor desc(t) is

 a bit-string, nm bits long, where nk bits are set to 1
 formed by concatentation of n attribute codewords
 desc(t) = cw(A1)cw(A2)...cw(An)



DJC example143/150

Consider the following tuple (from a bank deposit database)


 branch 
 acctNo 
 name 
 amount 


 Perryridge 
 102 
 Hayes 
 400 


It has the following codewords/descriptor (for m = 4,   k = 2)


 Ai 
 cw(Ai) 


 Perryridge 
 0101 


 102        
 0011 


 Hayes      
 1001 


 400        
 0101 


 desc(t)  
 0101001110010101 


(Note: length of tuple = ~28 bytes,   length of descriptor = 16 bits = 2 bytes)


DJC Queries144/150

Use a similar approach to SIMC:

 first generate a query descriptor desc(q)
 then use query descriptor to search signature file

desc(q) is formed by concatentation of codewords

 each supplied attribute Ai contributes cw(Ai)
 each unknown attribute contributes 0000
	(as many zero bits as needed)



... DJC Queries145/150

E.g. consider the query (Perryridge, ?, Hayes, ?)


 Ai 
 cw(Ai) 


 Perryridge 
 0101 


 ?        
 0000 


 Hayes      
 1001 


 ?        
 0000 


 desc(t)  
 0101000010010000 


If the query has s supplied attributes, then desc(q) has sk bits set to 1. 


... DJC Queries146/150

Searching the signature file is done in the same way as for SIMC:
Matches = {}
for each descriptor D[i] in sig file {
    if (D[i] matches desc(q))
        Matches = Matches ∪ TupleId[i]
}
for each Tid in Matches
    fetch tuple t via Tid

The matching process is also the same as for SIMC


matches(Ti,q) if desc(q) AND desc(Ti) = desc(q)



Example DJC Query147/150

Consider the query and the example database:


 Signature 
 Tuple 


 0101 0000 1001 0000 
 (Perryridge,?,Hayes,?) 


 1010 0110 1001 0110 
 (Brighton,217,Green,750) 


 0101 0101 0101 0101 
 (Clearview,117,Throggs,295) 


 1010 1010 1001 1010 
 (Downtown,101,Johnson,512) 


 1010 0011 0011 0101 
 (Mianus,215,Smith,700) 


 0101 0011 1001 1010 
 (Perryridge,102,Hayes,400) 


 1001 0101 0011 1010 
 (Redwood,222,Lindsay,695) 


 0101 0110 1001 0110 
 (Round Hill,305,Turner,350) 




False Matches148/150

The query has potential matches:


 branch 
 acctNo 
 name 
 amount 


 Perryridge 
 102 
 Hayes 
 400 


 Round Hill 
 305 
 Turner 
 350 


DJC also suffers from the "false match" problem, due to:

 codeword hashing collisions
	  (two different values generate same codeword)

Note:

 there are no false matches due to overlapping in DJC
 likelihood of hash collisions is much higher for DJC than for SIMC 
	(e.g.   k=2, m=4 ⇒ 6 codewords for DJC
	  vs   k=2, m=16 ⇒ 120 codewords for SIMC)



... False Matches149/150

How to reduce likelihood of false matches?

 increase descriptor size (m)
 choose k so that ≅ half of bits are set
	  (maximises distinct descriptors)

Since descriptor is formed from concatentation of codewords:

 need to keep codewords short to keep descriptor reasonably small
 no need to use the same number of bits for each attribute codeword 
	⇒ choose more bits for attributes that are often supplied in queries

Thus, for each Ai, mi bits in the codeword, with mi/2 1-bits.

DJC can be "tuned" to the mix of pmr queries used on the relation.


Query Cost for DJC150/150

If we assume that

 DJC descriptors are the same length as SIMC descriptors
 pF is similar for both schemes   (reasonable)

then the average query cost analysis is exactly the same as for SIMC.

However, we can make sure that certain query types are answered
with less likelihood of false matches by adjusting the codeword lengths.

The optimisations from SIMC (two-level, bit-slice) can also be applied.

As for SIMC, DJC assists with one and pmr, but not
	range, space, sim.

Produced: 3 Nov 2018</p><h3 >字段6</h3><p>Similarity-based Selection


Similarity Selection1/65

Relational selection is based on a boolean condition C

 evaluate C for each tuple t
 if C(t) is true, add t to result set
 if C(t) is false, t is not part of solution
 result is a set of tuples { t1, t2, ..., tn } all of which satisfy C

Uses for relational selection:

 precise matching on structured data



... Similarity Selection2/65

Similarity selection is used in contexts where

 cannot define a precise matching condition
 can define a measure d of "distance" between tuples
 d=0 is an exact match, d>0 is less accurate match
 result is a list of pairs [ (t1,d1), (t2,d2), ..., (tn,dn) ]   (ordered by di)

Uses for similarity matching:

 text or multimedia (image/music) retrieval
 ranked queries in conventional databases



Similarity-based Retrieval3/65

Similarity-based retrieval typically works as follows:

 query is given as a query object q   (e.g. sample image)
 system finds objects that are like q   (i.e. small distance)

The system can measure distance between any object and q ...

How to restrict solution set to only the "most similar" objects:

 threshold dmax
	  (only objects t such that dist(t,q) ≤ dmax)
 count k
	  (k closest objects (k nearest neighbours))



... Similarity-based Retrieval4/65

Tuple structure for storing such data typically contains

 id to uniquely identify object   (e.g. PostgreSQL oid)
 metadata   (e.g. artist, title, genre, date taken, ...)
 value of object itself   (e.g. PostgreSQL BLOB or bytea)

Properties of typical distance functions   (on objects x,y,z)

 dist(x,y) ≥ 0,     dist(x,x) = 0,      dist(x,y) = dist(y,x)
 dist(x,z) < dist(x,y) + dist(y,z)   (triangle inequality)
 often require substantial computational effort



... Similarity-based Retrieval5/65

Naive approach to similarity-based retrieval
q = ...    // query object
dmax = ... // dmax > 0  =>  using threshold
knn = ...  // knn > 0   =>  using nearest-neighbours
Dists = [] // empty list
foreach tuple t in R {
    d = dist(t.val, q)
    insert (t.oid,d) into Dists  // sorted on d
}
n = 0;  Results = []
foreach (i,d) in Dists {
    if (dmax > 0 && d > dmax) break;
    if (knn > 0 && ++n > knn) break;
    insert (i,d) into Results  // sorted on d
}
return Results;

Cost  =  read all r feature vectors  +  compute distance() for each


... Similarity-based Retrieval6/65

For some applications, Cost(dist(x,y)) is comparable to Tr

⇒   computing dist(t.val,q) for every tuple t is infeasible.

To improve this aspect:

 compute feature vector which captures "critical" object properties
 store feature vectors "in parallel" with objects   (cf. signatures)
 compute distance using feature vectors   (not objects)

i.e. replace dist(t,tq) by dist'(vec(t),vec(tq)) in previous algorithm.

Further optimisation: dimension-reduction to make vectors smaller


... Similarity-based Retrieval7/65

Content of feature vectors depends on application ...

 image ... colour histogram (e.g. 100's of values/dimensions)
 music ... loudness/pitch/tone (e.g. 100's of values/dimensions)
 text ... term frequencies (e.g. 1000's of values/dimensions)

Typically use multiple features, concatenated into single vector.

Feature vectors represent points in a very high-dimensional space.

Query: feature vector representing one point in vh-dim space.

Answer: list of objects "near to" query object in this space.


Example: Content-based Image Retrieval8/65

User supplies a description or sample of desired image (features).

System returns a ranked list of "matching" images from database.






... Example: Content-based Image Retrieval9/65

At the SQL level, this might appear as ...
create view Sunset as
select image from MyPhotos
where  title = 'Pittwater Sunset'
       and taken = '2009-01-01';

create view SimilarSunsets as
select title, image
from   MyPhotos
where  (image ~~ (select * from Sunset)) < 0.05
order  by (image ~~ (select * from Sunset));

where the ~~ operator measures distance between images.


... Example: Content-based Image Retrieval10/65

Implementing content-based retrieval requires ...

 a collection of "pertinent" image features

 e.g. colour, texture, shape, keywords, ...

 some way of describing/representing image features

 typically via a vector of numeric values

 a distance/similarity measure based on features

 e.g. Euclidean distance between two vectors

	  dist(x,y) = √( (x1-y1)2 +
		(x2-y2)2 +
		... (xn-yn)2 )




... Example: Content-based Image Retrieval11/65

Data structures for naive similarity-retrieval:






... Example: Content-based Image Retrieval12/65

Insertion of an image into the database:

 use image processing algorithms to compute feature vector
 insert image   (either into file system or as a BLOB in the DBMS)
 insert tuple (img,vec) associating image and feature vector 

Insertion cost:

 image processing   (relatively expensive, ≅ Tr)
 copy image in file sys  OR  insertion of image as BLOB
 insertion of (img,vec) tuple   (1r + 1w)



... Example: Content-based Image Retrieval13/65

Inputs to content-based similarity-retrieval:

 a database of r objects
  (obj1, obj2, ..., objr)
  plus associated ...
 r × n-dimensional feature vectors
  (vobj1, vobj2, ..., vobjr)
 a query image q with associated n-dimensional vector (vq)
 a distance measure   D(vi,vj) : [0..1)
	   (D=0 → vi=vj)

Outputs from content-based similarity-retrieval:

 a list of the k nearest objects in the database 
  [a1,   a2,   ...   ak]
 ordered by distance
   D(va1,vq)  ≤ 
	D(va2,vq)  ≤  ...  ≤ 
	D(vak,vq) 



Approaches to kNN Retrieval14/65

Partition-based

 use auxiliary data structure to identify candidates
 space-partitioning methods: Grid file, k-d-B-tree, quadtree
 data-partitioning methods: R-tree, X-tree, SS-tree, TV-tree, ...
 unfortunately, such methods "fail" when #dims > 10..20

Approximation-based

 use approximating data structure to identify candidates
 signatures: VA-files 
 projections: iDistance, LSH, MedRank, CurveIX, Pyramid



... Approaches to kNN Retrieval15/65

Above approaches mostly try to reduce number of objects considered.

Other optimisations to make kNN retrieval faster

 reduce I/O by reducing size of vectors   (compression, d-reduction)
 reduce I/O by placing "similar" tuples together   (clustering)
 reduce I/O by remembering previous pages   (caching)
 reduce cpu by making distance computation faster



VA Files



VA (Signature) Files17/65

Vector Approximation (VA) file developed by Weber and Blott.

 targetted at very high-dimensional feature vectors
 abandons the idea of search complexity better than O(n)

Why give up on sub-linear complexity?

 analysis shows that all methods degenerate to this as d increases
 absolute upper bound on d before linear scan is best d = 610
 in practice, observe that most methods degenerate for 10 ≤ d ≤ 40

Note: d = number of dimensions


... VA (Signature) Files18/65

Uses a signature file "parallel" to the main feature vector file






... VA (Signature) Files19/65

VA signatures have properties:

 (much) more compact than feature vectors
 provide approximation to information in vectors

Approach to querying:

 perform filtering by fast scan of signatures, then
 compute expensive D only on (hopefully) small set of candidates



VA-File Signatures20/65

Computing VA signatures:

 partition space into small number of regions on each dimension
 use region values in object signatures  
	(coarse-grained view of data)

Implemented by taking m high-order bits from each feature vector element.

Consider a 3-d RGB feature vector, with 0..255 for each RGB value:

 feature vector: v = (255,128,0) = (11111111,10000000,0000000)
 partition each dimension into 4 regions (⇒ m=2 bits per d)
 VA signature for this vector: va(v) = (11,10,00)



... VA-File Signatures21/65






Insertion with VA-Files22/65

Given: a feature vector vobj for a new object.

Insertion is extremely simple.
sig = signature(vobj)
append vobj to feature vector file
append sig to signature file

Storage overhead determined by m   (e.g. 3/32).


Query with VA-Files23/65

Input:   query vector vq
results = [];  maxD = infinity;
for (i = 0; i < r; i++) {
   // fast distance calculation
   dist = minDistance(region[va[i]], vq)
   if  (#results < k  ||  dist < maxD)  {
      dist = distance(v[i],vq)
      if (#results < k  ||  dist < maxD)  {
         insert (tid[i],dist) into results
         // sorted(results) && length(results) <= k
         maxD = largest distance in results
      }
   }
}

Result is a list  f tids of k database objects nearest to query.


... Query with VA-Files24/65

VA signatures allow fast elimination of objects by region


 if nearest point in region containing object is further than maxD, ignore object



Given query vector q, data vector v, can quickly compute:

 lower(q,v) = distance between q and nearest point in region R[v]
 upper(q,v) = distance between q and furthest point in region R[v]

Thus, instead of r expensive D computations, we require

 r cheap computations of (lower(q,v), upper(q,v))
 small number of real distance computations
	 (how many are actually required depends on effectiveness of filtering)



... Query with VA-Files25/65




Marks on each dimension i are stored as array Ri[0], Ri[1], .. Ri[2m].


... Query with VA-Files26/65

Distance bound formula:
lower(q,v) = √∑i=1dlowi2
   where
      lowi = qi - Ri[vi+1],               if vi < qi
           = 0,                          if vi = qi
           = Ri[vi] - qi,                 if vi > qi

upper(q,v) = √∑i=1duppi2
   where
      uppi = qi - Ri[vi],                 if vi < qi
           = max(qi-Ri[vi], Ri[vi+1]-qi),  if vi = qi
           = Ri[vi+1] - qi,               if vi > qi




Cost of VA File Searching27/65

Performing VA search requires:

 read r VA signatures
 compute lower(q,vj) for each of r signatures
	 (could potentially cache region-based distance results in look-up table)
 compute real distance, D, rf times,
	  where f is filtering factor 
	(expect an f value somewhere near 0.001 under ideal conditions)

A problem with VA files:

 works best with uniform data distribution (many image DBs are not)
 observed filtering levels are 0.01-0.1 rather than 0.001

Can fix this problem using uneven region sizes (but needs more storage)


... Cost of VA File Searching28/65

Example:   d=200, m=3, r=106, B=4096, TD≅0.001Tr

 size of tuples (tid,vec) = 4 bytes + 400 bytes = 404 bytes
 number of tuples per page ⌊4096/404⌋ = 10
 total number of feature vector pages = ⌈106/10⌉ = 100000

Cost (without VA file) = 100000Tr + 106TD ≅ 101000Tr

 size of VA signature = 200 × 3 bits = 75 bytes
 number sigs in VA file page = ⌊4096/75⌋ = 54
 total number of VA file pages = ⌈106/54⌉ = 18518
 with f=0.001, fetch only 1000 feature vectors

Cost (with VA file) = 18518Tr + 1000Tr + 1000TD ≅ 19519Tr


Improved VA Search Algorithm29/65

An improved query algorithm that guarantees minimal D computations:
vaq = vasignature(vq)
results = [];  maxD = infinity;  pending = [];
for (i = 0; i < r; i++) {
   lowD = lower(vq,region[va[i]])
   uppD = upper(vq,region[va[i]])
   if  (#results < k  ||  dist < uppD)  {
      sortedInsert (i,uppD) into results
      heapInsert (i,lowD) into pending
   }
}
results = [];  heapRemove (i,lowD) from pending
while (lowD < maxD) {
   dist = distance(v[i],vq)
   if (#results < k  ||  dist < maxD) {
      sortedInsert (i,dist) into results
      // sorted(results) && length(results) <= k
      maxD = largest distance in results
   }
   heapRemove (i,lowD) from pending
}



Curve-based Similarity Search



Curve-based Searching31/65

The strategy when using curve-based indexing is as follows:

 dealing with many dimensions (d > 10) is too difficult
 so, project the feature space onto one dimension (line/curve)
 this gives a linear ordering of the objects
 can then perform search over this linear ordering
 using existing efficient 1d access methods (e.g. B-trees)


Indexing based on curves needs to ...

 minimise information loss to reduce missing answers
 at the same time, maximise filtering effectiveness



... Curve-based Searching32/65

Mapping from 3-d to 1-d





Note: impossible to preserve all NN-in-space as NN-on-curve.


Curve Index File Organisation33/65

Data structures for curve-based searching:





Insertion with Curve Index34/65

For each image inserted into the database:

 determine a feature vector v for the image
 determine a tid for the image   (file name?)
 map the feature vector onto the curve   p = map(v)
 p is the primary key for the curve index file
 insert (p,v,tid) into the curve index file using e.g. B-tree



Searching with Curve Index35/65

Overview of curve-based searching algorithm:
Input: query feature vector vq

p = map(vq)
cur1 = cur2 = lookup(p)
while (not enough answers) {
    (pt,vec,tid) = next(cur1)
    remember tid if D(vec,vq) small enough
    (pt,vec,tid) = prev(cur2)
    remember tid if D(vec,vq) small enough
}

Output: collected tids of similar objects



... Searching with Curve Index36/65

How curve is scanned to find potential near-neighbours:






... Searching with Curve Index37/65

What kind of curves are required to make this approach viable?

 must pass through every data point exactly once
 must pass through every "point" in the underlying space exactly once

Possible candidate curves:

 space-filling curves (e.g. Hilbert curve, Peano curve, Gray ordering)

Experiments have shown that the Hilbert curve is the most suitable for data access.


2d Space-filling Curves38/65






... 2d Space-filling Curves39/65






The CurveIx Scheme40/65


What we would like from the above curve mapping:

 object close to the query on the curve ⇒ close in feature space
 object close to the query in feature space ⇒ close on the curve

With a single Hilbert curve (although not some other curves)

 map(v) ≅ map(vq)  ⇒  D(v,vq) ≅ 0   is generally true 
	(in other words, we usually find similar objects near the query on the curve)
 D(v,vq) ≅ 0  ⇒  map(v) ≅ map(vq)   is sometimes false 
	(in other words, we sometimes fail to find objects that are similar to the query)

One curve is not enough, so use several "complementary" curves.


... The CurveIx Scheme41/65

Example curveix scheme with 2 curves on a 2d space:





... The CurveIx Scheme42/65

The basic idea:

 project d-dimensional vector onto 1-d space-filling curve
 collect set of curve-neighbours (should contain some kNN)
 repeat above for several curves with different paths through space
 union of neighbour sets from all curves gives candidates
 retrieve and compute distance only for candidates

We want candidate set to contain most/all of kNN ...

 how many curves do we need?
 how many neighbours do we examine on each curve?



Curve Mapping Functions43/65

Each Ci has the following properties:

 maps d-dimensional vectors onto points on a line 
	i.e. [0,1)d → [0,1)
 mapping is one-to-one and defines an order on [0,1)d
 maps onto a space-filling Hilbert curve

Points within a small region of the curve are likely
to have been mapped from vectors that are close in d-space.

How to generate multiple Cis from a single vector:

 reorder elements in vector, translate individual elements



Data Structures for CurveIx44/65

Derived from data structures for the QBIC system:


 Ci 
 a mapping function for each of the m
	different space-filling curves (i=1..m) 


 Db 
 a database of r d-dimensional feature vectors;
	each entry is a pair (imageId, vector) where
	imageId forms a primary key 


 Index 
 a database of rm curve points; each point is represented
	by a tuple (curveId, point, imageId);
	the pair (curveId, point) forms a primary key with an
	ordering, where point = CcurveId(vimageId) 


 vq 
 feature vector for query q 



... Data Structures for CurveIx45/65






Database Construction46/65

For each image obj, insertion requires:
id = identifier for obj
for (i in 1..m) {
     p = Ci(vobj)
     insert (i, p, id) into Index
}
insert (id, vobj) into Db



Example: Search with 5 Curves47/65







Finding k-NN in CurveIx48/65

Given:   vq ,  
C1 .. Cm ,  
Index ,   Db
for (i in 1..m) {
    p = Ci(vq)
    lookup (i,p) in Index
    fetch (i,p1,j)
    while (p1 "close to" p on curve i) {
        collect j as candidate
        fetch next (i,p1,j)
}   }
for each candidate j {
     lookup j in Db
     fetch (j,vj)
     d = D(vj , vq)
     include j in k-NN if d small enough
}



CurveIx vs. Linear Scan49/65

Linear scan: 
  Cost  =  read all N vectors  +  compute D for each

CurveIx:
  Cost   =   m B-tree lookups  +  compute D   fN times

Some observations ...

 we can't afford to have too many curves   (m < 10?)
 CurveIx has to do effective filtering    (f ≪ 1)

Difficult to formally analyse further, so we implemented the system to see
how it performed ...


Ci Values as Keys50/65

One problem raised early in the implementation:


	the Hilbert numbers produced by Ci
	could be expanded to arbitrary precision in order
	to distinguish between
	Ci(v1 )
	and
	Ci(v2 )

To use such values as database keys, we 
need them as fixed precision, so we limit
expansion (to 4 levels).

Problems:

 gives keys that are 96 bytes long 
	  (producing very large Index files)
 different vj can be mapped to same point
	  (candidate set size artificially inflated)


Solution:

 use a "prefix" index structure (something like a trie)



Performance Analysis51/65

Since CurveIx does not guarantee to deliver the k-NN
among its candidates, we set an "acceptable" accuracy level of 90%.

In other words, CurveIx must deliver 0.9 k nearest-neighbours
to be considered useful.

The initial experiments aimed to answer the questions:


How many curves are needed to achieve 90% accuracy?

How many curve-neighbours do we need to examine?

Can all of this be done reasonably efficiently?



Experiments52/65

Measures for accuracy:

 Acc1   average top-10 entries in CurveIx top-10
 Acc2   how frequently CurveIx gives 10 out of 10

Measures for efficiency:

 Size   size of Db file + Index file
 Dist   number of distance calculations required
 IO     total amount of I/O performed



... Experiments53/65

To determine how these measures vary:

 built databases of size 5K, 10K, 15K, 20K   (supersets)
 for each database, ran 25 query "benchmark" set
 for each query, ran for 3,5,10,20,30,40 curve-neighbours 
	(but because of curve-mapping problem, only got 20,30,40)
 for each query, ran for 20,40,60,80,100 curves

Also implemented a linear scan version for comparison and
to collect the exact answer sets.


Sample Comparison54/65



CurveIx Indexing
Linear Scan



QUERY: img00102
0.000000  img00102
0.005571  img00713
0.008826  img00268
0.011413  img05054
0.011811  img00631
0.014259  img04042
0.027598  img00203
0.037471  img00287
0.063639  img00244
0.067583  img00306

# dist calcs: 524
1.67user 0.23system ...



QUERY: img00102
0.000000  img00102
0.005571  img00713
0.008826  img00268
0.011413  img05054
0.011811  img00631
0.014259  img04042
0.027598  img00203
0.037471  img00287
0.063639  img00244
0.067583  img00306

# dist calcs: 20000
1.93user 1.12system ...






Experimental Results55/65

For fixed database (20K), effect of varying Range, Ncurves


  #Curves     Range     Acc1     Acc2     #Dist  

 20  20  6.72  0.20  426 
 20  30  7.28  0.28  695 
 20  40  7.68  0.36  874 
 40  30  8.16  0.40  1301 
 40  40  8.60  0.44  1703 
 60  30  8.40  0.44  1905 
 60  40  8.60  0.48  2413 
 80  30  8.87  0.58  2485 
 80  40  9.20  0.72  3381 
 100  30  9.10  0.70  3061 
 100  40  9.28  0.72  4156 



Results: Size vs. Accuracy56/65

For fixed CurveIx parameters (80 curves, 30 range), effect of varying
database size:

  #Images     Acc1     Acc2  
 5K  9.72  0.76 
 10K  9.44  0.80 
 15K  9.16  0.70 
 20K  9.04  0.64 



iDistance



iDistance58/65

iDistance   (Jagadish, Ooi, Tan, Yu, Zhang)

 adaptive B-tree based indexing method
 aimed at handling kNN queries in high-d spaces

The basic idea:

 determine a set of reference points Oj 
	(reference points partition the data space)
 build index on (pi, D(pi)) 
	(distance of each point pi to its nearest reference point Oj)
 use index to quickly find pi likely to be close to q



... iDistance59/65

Computing the iDistance:   D(p) = j.c + dist(p,Oj)






where c is a constant to "spread" the partitions over the iDistance line



Searching with iDistance60/65

The approach:

 start with query point q
 choose a radius r around query, giving hypersphere S
 for all partitions intersected by S

 determine   a = min(dist(S,Oj))  
	and   b = max(dist(S,Oj))
 find all data points p with D(p) in range j.c+a .. j.c+b
 maintain a sorted list of (p,dist(p,q)) pairs

 increase radius by delta-r and repeat above steps
 continue until kNN are found



... Searching with iDistance61/65

First iteration of search (small r):






... Searching with iDistance62/65

Later iteration of search (larger r):






... Searching with iDistance63/65

Details of search method:
//Inputs:
// q = query point, k = # nearest-neighbours
// O[m] = set of reference points
// rad[m] = radius of partition around O[i]
// deltaR = search radius increase on each step
// maxR   = maximum search radius
int r = 0;  // search radius around q
int stop = 0;  // flag for when to halt
int seen[];  // partition i already searched?
Obj S[];  // result set, k nearest neighbours
Obj lp[], rp[];  // lists of limits with partitions

while (!stop) {
   r = r + deltaR
   stop = SearchRefs(q,r)
}
...



... Searching with iDistance64/65

int SearchRefs(q,r) {
   f = furthest(S,q)
   stop = (dist(f,q) < r && |S| == k)
   for (i = 0; i < m; i++) {
      d = dist(O[i],q)
      if (!seen[i]) {
         hs[i] = sphere(O[i],rad[i])
         if (hs[i] contains q) {
            seen[i] = 1
            leaf = BtreeSearch(i*c+d)
            lp[i] = SearchInward(leaf, i*c+d-r)
            rp[i] = SearchOutward(leaf, i*c+d+r)
         }
         elseif (hs[i] intersects sphere(q,r)) {
            seen[i] = 1
            leaf = BtreeSearch(rad[i])
            lp[i] = SearchInward(leaf, i*c+d-r)
         }
      }
      else {
         if (lp[i] != null)
            lp[i] = SearchInward(left(lp[i]), i*c+d-r)
         if (rp[i] != null)
            rp[i] = SearchOutward(right(rp[i]), i*c+d+r)
      }
   }
   return stop
}



... Searching with iDistance65/65

Cost depends on data distribution and position of Oi

See analysis in ACM Trans on DB Systems, v.30, Jun 2005, p.364

Determining the set of reference points:

 space-partitioning ... divide space up using geometric partitions
 data-partitioning ... try to have equal # points in each partition

Determining the size of deltaR:

 too small ... more interations, more access to B-tree index
 too large ... "overshoot" and fetch unnecessary pages (in last iteration)


Produced: 15 May 2016</p><h3 >字段7</h3><p>Implementing Join


Join1/87

DBMSs are engines to store, combine and filter information.

Filtering is achieved via selection and projection.

The join operation (⋈) is the primary means of
combining information.

Because join is

 such an important operation in database applications/systems
 potentially very expensive to execute

many methods have been developed for its implementation.

(We use a running example to compare costs of the various join processing methods)


... Join2/87

Types of join:

 simple equijoin   (single-equality condition)
select * from R,S where R.i = S.j

 partial-match join   (conjunction of equality conditions)
select * from R,S where R.a = S.b and R.c = S.d ...

 theta join   (arbitrary expression as condition)
select * from R,S where R.a < S.b and R.c <> S.d ...


Focus on simple equijoin, since common in practice (R.pk=S.fk)


Join Example3/87

Consider a university database with the schema:
create table Student(
   id     integer primary key,
   name   text,  ...
);
create table Enrolled(
   stude  integer references Student(id),
   subj   text references Subject(code),  ...
);
create table Subject(
   code   text primary key,
   title  text,  ...
);


And the following request on this database:

List names of students in all subjects, arranged by subject.



... Join Example4/87

The result of this request would look like:
Subj      Name
--------  -----------------
COMP1011  Chen Hwee Ling
COMP1011  John Smith
COMP1011  Ravi Shastri
...
COMP1021  David Jones
COMP1021  Stephen Mao
...
COMP3311  Dean Jones
COMP3311  Mark Taylor
COMP3311  Sashin Tendulkar



... Join Example5/87

An SQL query to provide this information:
select E.subj, S.name
from   Student S, Enrolled E
where  S.id = E.stude
order  by E.subj, S.name;

And its relational algebra equivalent:

Sort[subj] ( Project[subj,name] ( Join[id=stude](Student,Enrolled) ) )


The core of the query is the join Join[id=stude](Student,Enrolled)

To simplify writing of formulae, S = Student, E = Enrolled.


... Join Example6/87

Some database statistics:




 Sym 
 Meaning 
 Value 


 rS 
 # student records 
 20,000 


 rE 
 # enrollment records 
 80,000 


 CS 
 Student records/page 
 20 


 CE 
 Enrolled records/page 
 40 


 bS 
 # data pages in Student 
 1,000 


 bE 
 # data pages in Enrolled 
 2,000 




Also, in cost analyses below, N = number of memory buffers.


... Join Example7/87

Out = Student ⋈ Enrolled relation statistics:




 Sym 
 Meaning 
 Value 


 rOut 
 # tuples in result 
 80,000 


 COut 
 result records/page 
 80 


 bOut 
 # data pages in result 
 1,000 




Notes:

 rOut ... one result tuple for each Enrolled tuple
 COut ... result tuples have only subj and name



Join via Cross-product8/87

Join can be defined as a cross-product followed by selection:


Join[Cond](R,S)   =   Select[Cond]( R × S )


For the example query, could implement


Join[id=stude](Student,Enrolled)

as

Select[id=stude](Student × Enrolled)


Cross-product contains 20,000 × 80,000 = 1,600,000,000 tuples.


... Join via Cross-product9/87

For Temp = (Student × Enrolled)

I/O costs:

 size of Temp relation r = 16 × 108 records
 assuming CTemp=16, then bTemp = 108
 Temp is written once, then scanned once
 total I/O = 108.(Tw+Tr)

Assuming Tw=Tr=0.01s, this will take around 500 hours!


... Join via Cross-product10/87

Because

 cross-products are infrequent in practice
	  (except to describe join)
 cross-products are large 
          (typically much larger than the final join result)

DBMSs do not implement join via cross-product.

DBMSs implement only join and provide cross-product as:


R × S  =  Join[true](R,S)

or, in SQL
select * from R,S



Nested-Loop Join



Nested Loop Join12/87

The simplest join algorithm:

 iteratively generates the cross-product 
 checks join condition on each tuple

Algorithm to compute   Join[Cond](R,S):
for each tuple r in R {
    for each tuple s in S {
        if ((r,s) satisfies join condition) {
            add (r,s) to result
}   }   }

R is the outer relation; S is the inner relation.


... Nested Loop Join13/87

Requires (at least) three memory buffers (2 input, 1 output).





... Nested Loop Join14/87

Abstract algorithm for Join[Cond](R,S) (with 3 memory buffers):
for each page of relation R {
    read into buffer rBuf
    for each page of relation S {
        read into buffer sBuf
	for each record r in rBuf {
            for each record s in sBuf {
	        if ((r,s) satisfies Cond) {
		    add combined(r,s) to OutBuf
	            write Outbuf when full
}   }   }   }   }



... Nested Loop Join15/87

Detailed algorithm for Join[Cond](R,S) (with 3 memory buffers):
// rf: file for R, sf: file for S, of: output file
outp = 0; clearBuf(oBuf);
for (rp = 0; rp < nPages(rf); rp++) {
   readPage(rf, rp, rBuf);
   for (sp = 0; sp < nPages(sf); sp++) {
      readPage(sf, sp, sBuf);
      for (i = 0; i < nTuples(rBuf); i++) {
         rTup = getTuple(rBuf, i);
         for (j = 0; j < nTuples(sBuf); j++) {
            sTup = getTuple(sBuf, j);
            if (satisfies(rTup,sTup,Cond)) {
            rsTup = combine(rTup,sTup);
            addTuple(oBuf, rsTup);
            if (isFull(oBuf)) {
               writePage(of, outp++, oBuf);
               clearBuf(oBuf);
}   }   }   }   }   }



... Nested Loop Join16/87

The three-memory-buffer nested loop join requires:

 read all bR pages of R once
 for each of page of R, read bS pages of S

Cost   =   bR + bR bS

If we use S as the outer relation in the join

Cost   =   bS + bS bR

It is (slightly) better to use smaller relation as outer relation.


Nested Loop Join on Example17/87

If Student is outer relation and Enrolled is inner:




 Cost 
 = 
 bS + bS bE 


 
 = 
 1,000 + 1,000 × 2,000 = 2,001,000 




If Enrolled is outer relation and Student is inner:




 Cost 
 = 
 bE + bE bS 


 
 = 
 2,000 + 2,000 × 1,000 = 2,002,000 




Cost of nested-loop join is too high
	  (5 hours, if Tr=0.01 sec)


Implementing Join Better18/87

Aims of effective join computation:

 generate only relevant tuples from the cross-product
 generate these tuples with minimal disk I/O


Range of costs for Join(R,S)

 worst case cost =   bR + bRbS
	   (nested loop join)
 best case cost =   bR + bS
	   (read each page once)



Block Nested Loop Join19/87

If at least bR+2 memory buffers available:

 read the entire R relation into memory
 for each S page, check join condition on all (r,s) pairs






... Block Nested Loop Join20/87

Algorithm for nested loop join with bR+2 memory buffers:
read all of R's pages into memory buffers
for each page of relation S {
    read page into S's input buffer
    for each tuple s in S's buffer {
        for each tuple r in R's memory buffers {
            if ((r,s) satisfies JoinCond)) {
                add (r,s) to output buffer
                write output buffer when full
}   }   }   } 

Note that R effectively becomes the inner relation in this scheme.


... Block Nested Loop Join21/87

This method requires:

 read bR pages of relation R into buffers
 while R is buffered, read bS pages of S

Cost   =  
	bR + bS

Notes:

 minimal I/O cost, but considers all (r,s) pairs
 thus, requires rR.rS checks of the join condition



... Block Nested Loop Join22/87

Further performance improvements:

 must reduce number of R tuples matched against each S tuple
 use access method to find small set of R tuples matching S tuple

Example:

 each S joins with k ≪ rR tuples of R
 R tuples are stored in sorted array of memory buffers
 for each S tuple, use binary search to find matching buffer
 scan around that buffer to find all matching (R,S) pairs
 requires approx CR.rS checks of join condition



Block Nested Loop Join on Example23/87

If ≥ 1002 memory buffers are available:

 read Student relation into memory
 scan Enrolled relation, computing join





 Cost 
 = 
 bS + bE 


 
 = 
 1,000 + 2,000 = 3,000 




This is considerably better than 106
	   (30 secs vs 5 hours).

But what if we have only N memory buffers, where N < bR ,  N < bS?


... Block Nested Loop Join on Example24/87

In general case, read outer relation in runs of N-2 pages
for each run of N-2 pages from R {
    read N-2 of R's pages into memory buffers
    for each page of relation S {
        read page into S's input buffer
        for each tuple s in S's buffer do
            for each tuple r in R's memory buffers {
                if ((r,s) satisfies JoinCond)) {
                    add (r,s) to output buffer
                    write output buffer when full
}   }   }   }   }



... Block Nested Loop Join on Example25/87

Block nested loop join requires

 read   ⌈bR/N-2⌉   runs from R
 for each run, scan bS pages of S

Cost   =  
bR + bS .
	 ⌈ bR/N-2 ⌉

Notes:

 the final run will typically be "short"   (i.e. < N-2 pages)
 unless index/hash is used, we still do rR.rS tuple comparisons



... Block Nested Loop Join on Example26/87

Costs for various buffer pool sizes:


 N 
 Inner 
 Outer 
 #runs 
 Cost 


 22 
 Student 
 Enrolled 
 50 
 101,000 


 52 
 Student 
 Enrolled 
 20 
 41,000 


 102 
 Student 
 Enrolled 
 10 
 21,000 


 1002 
 Student 
 Enrolled 
 1 
 3,000 


 22 
 Enrolled 
 Student 
 100 
 102,000 


 52 
 Enrolled 
 Student 
 40 
 42,000 


 102 
 Enrolled 
 Student 
 20 
 22,000 


 1002 
 Enrolled 
 Student 
 2 
 4,000 




Block Nested Loop Join in Practice27/87

Why block nested loop join is very useful in practice ...

Many queries have the form
select * from R,S where r.i=s.j and r.x=k

This would typically be evaluated as


Join [i=j] ((Sel[r.x=k](R)), S)


If |Sel[r.x=k](R)| is small ⇒ may fit in memory
	(in small #buffers)


Join Conditions and Methods28/87

Nested loop join makes no assumptions about join conditions.
for each pair of tuples (r,s) {
    check join condition on (r,s)
    if satisfied, add to results
}

To improve join:

 reduce the number of tuple pairs considered
 but not easy to do for arbitrary join condition

As noted above, simple equijoin is a common join condition.

Thus, a range of other join algorithms has been developed
specifically for equality join conditions.


Index Nested Loop Join29/87

Most joins considered so far have a common problem:

 repeated scans of entire inner relation S are required

If there is an index on S, we can avoid such repeated scanning.

Consider Join[R.i=S.j](R,S):
for each tuple r in relation R {
    use index to select tuples
        from S where s.j = r.i
    for each selected tuple s from S {
        add (r,s) to result
}   }



(For ordered indexes (e.g. Btree), this also assists join
conditions like R.i<S.j)



... Index Nested Loop Join30/87

This method requires:

 one scan of R relation (bR)

 only one buffer needed, since we use R tuple-at-a-time

 for each tuple in R (rR), one index lookup on S

 cost depends on type of index and number of results
 best case is when each R.i matches few S tuples


Cost   =   bR + rR.SelS
	   (SelS is the cost of performing a select on S).


... Index Nested Loop Join31/87

For index lookup:

 cost of locating first matching tuple

 for B+ trees, typically 2-4 page reads
 for hashing, typically 1-2 page reads

 cost of finding other matching tuples

 if clustered, typically 1-2 page reads
 if unclustered, up to bq page reads


 

Note: building an index "on the fly" to perform a join can be very cost-effective.



Index Nested Loop Join on Example32/87

Case 1: Join[id=stude](Student,Enrolled)

 Student is outer and Enrolled is inner
 Enrolled has a clustered B+ tree index on stude field
 B+ tree has depth 3 (root + internal + leaf)
 most of the time, the four matching records are in a single page

 


 Cost 
 = 
 bS + rS btreeE 




 
 = 
 1,000 + 20,000 × (3+1.01) = 80,000 




... Index Nested Loop Join on Example33/87

Case 2: Join[id=stude](Student,Enrolled)

 Student is outer and Enrolled is inner
 Enrolled has an unclustered B+ tree index on stude field
 B+ tree has depth 3 (root + internal + leaf)
 assume worst case; matching records are all on different pages

 


 Cost 
 = 
 bS + rS btreeE 




 
 = 
 1,000 + 20,000 × (3+4) = 150,000 




... Index Nested Loop Join on Example34/87

Case 3: Join[id=stude](Student,Enrolled)

 Enrolled is outer and Student is inner
 Student is hashed on id field (e.g. linear hashing)
 there may be (short) overflow chains (e.g. 1.1 page reads/bucket)

 


 Cost 
 = 
 bE + rE hashS 




 
 = 
 2,000 + 80,000 × 1.1 = 90,000 


 




Optimised Index Nested Loop Join35/87

Consider the following scenario for Join[R.i=S.j](R,S):

 R.i is not a primary key
	(so many tuples have same R.i value)
 R is sorted on R.i
	(or could be efficiently sorted on R.i)
 each R.i value does not match very many tuples

Could save repeated index scans with the same R.i value

 cache results of index scan for R.i=k in buffer
 if next R tuple also has R.i=k, re-use scan results



... Optimised Index Nested Loop Join36/87

Abstract algorithm for optimised index nested loop join:
for each tuple r in relation R {
   if (prev == r.i)
      use selected tuples in buffer(s)
   else {
      use index to select tuples
         from S where s.j = r.i
      store selected tuples in buffer(s)
   }
   for each selected tuple s from S
      add (r,s) to result
   prev = r.i
}


Cost savings depend on repetition factor, #buffers, size of index scans



Sort-Merge Join



Sort-Merge Join38/87

Basic approach:

 sort both relations on join attribute
	  (reminder: Join[R.i=S.j](R,S))
 scan together using merge to form result (r,s) tuples

Advantages:

 no need to deal with "entire" S relation for each r tuple
 deal with runs of matching R and S tuples

Disadvantages:

 cost of sorting both relations
         (relations may be sorted on join key?)
 some rescanning required when long runs of S tuples



... Sort-Merge Join39/87

Method requires several cursors to scan sorted relations:

 r = current record in R relation
 s = start of current run in S relation
 ss = current record in current run in S relation






... Sort-Merge Join40/87

Abstract algorithm for merge phase of Join[R.i=S.j](R,S):

r = first tuple in R
s = first tuple in S
while (r != eof and s != eof) {
    // align cursors to start of next common run
    while (r != eof and r.i < s.j) { r = next tuple in R }
    while (s != eof and r.i > s.j) { s = next tuple in S }
    // scan common run, generating result tuples
    while (r != eof and r.i == s.j) {
        ss = s   // set to start of run
        while (ss != eof and ss.j == r.i) {
            add (r,s) to result
            ss = next tuple in S
        }
        r = next tuple in R
    }
    s = ss   // start search for next run
}



Sidetrack: Iterators41/87

Sort-merge join implementation is simplified by use of iterators.

 iterators give the appearance of tuple-at-a-time
 even when the underlying data is page-by-page
 and even in the pesence of auxiliary index structures

Typical usage of iterator:
Iterator iter; Tuple tup;
iter = startScan("Rel","i=5");
while ((tup = nextTuple(iter)) != NULL) {
    process(tuple);
}
endScan(iter);



... Sidetrack: Iterators42/87

typedef struct {
    File   inf;  // input file
    Buffer buf;  // buffer holding current page
    int    curp; // current page during scan
    int    curr; // index of current record in page
} Iterator;

// simple linear scan; no condition
Iterator *startScan(char *relName) {
    Iterator *iter = malloc(sizeof(Iterator));
    iter->inf  = openFile(fileName(relName),READ);
    iter->curp = 0;
    iter->curr = -1;
    readPage(iter->inf, iter->curp, iter->buf);
}



... Sidetrack: Iterators43/87

Tuple nextTuple(Iterator *iter) {
    // check if reached end of current page
    if (iter->curr == nTuples(iter->buf)-1) {
        // check if reached end of data file
	if (iter->curp == nPages(iter->inf)-1)
	    return NULL;
	iter->curp++;
        iter->buf = readPage(iter->inf, iter->curp);
        iter->curr = -1;
    }
    iter->curr++;
    return getTuple(iter->buf, iter->curr);
}
// curp and curr hold indexes of most recently read page/record



... Sidetrack: Iterators44/87

TupleID scanCurrent(Iterator *iter) {
    // form TupleID for current record
    return iter->curp + iter->curr;
}

void setScan(Iterator *iter, int page, int rec) {
    assert(page >= 0 && page < nPages(iter->inf));
    if (iter->curp != page) {
        iter->curp = page;
	readPage(iter->inf, iter->curp, iter->buf);
    }
    assert(rec >= 0 && rec < nTuples(iter->buf));
    iter->curr = rec;
}

void endScan(Iterator *iter) {
    closeFile(iter->buf);
    free(iter);
}



Sort-Merge Join45/87

Concrete algorithm using iterators:
Iterator *ri, *si;  Tuple rup, stup;

ri = startScan("SortedR");
si = startScan("SortedS");
while ((rtup = nextTuple(ri)) != NULL
       && (stup = nextTuple(si)) != NULL) {
    // align cursors to start of next common run
    while (rtup != NULL && rtup.i < stup.j)
           rtup = nextTuple(ri);
    if (rtup == NULL) break;
    while (stup != NULL && rtup.i > stup.j)
           stup = nextTuple(si);
    if (stup == NULL) break;
	// must have (r.i == s.j) here
...



... Sort-Merge Join46/87

...
    // remember start of current run in S
    TupleID startRun = scanCurrent(si);
    // scan common run, generating result tuples
    while (rtup != NULL && rtup.i == stup.j) {
        while (stup != NULL and stup.j == rtup.i) {
            addTuple(outbuf, combine(rtup,stup));
            if (isFull(outbuf)) {
                writePage(outf, outp++, outbuf);
                clearBuf(outbuf);
            }
            stup = nextTuple(si);
        }
        rtup = nextTuple(ri);
        setScan(si, startRun);
    }
}



... Sort-Merge Join47/87

Buffer requirements:

 for sort phase:

 as many as possible (remembering that cost is O(log#Bufs) )
 if insufficient buffers, sorting cost can dominate

 for merge phase:

 one output buffer for result
 one input buffer for relation R
 (preferably) enough buffers for longest run in S




... Sort-Merge Join48/87

Cost of sort-merge join.

Step 1: sort each relation   (if not already sorted):

 Cost =
2.bR (1 + logN-1(bR /N))  + 
2.bS (1 + logN-1(bS /N))

            (where N = number of memory buffers)

Step 2: merge sorted relations:

 if every run of values in S fits completely in buffers, 
	merge requires single scan,
	  Cost = bR + bS
 if some runs in of values in S are larger than buffers, 
	need to re-scan run for each corresponding value from R



Sort-Merge Join on Example49/87

Case 1:   Join[id=stude](Student,Enrolled)

 Student and Enrolled already sorted on id#
 memory buffers N=4; all runs are of length < 2

 
Cost  =  bS + bE  =  3,000    (i.e. minimal cost)


... Sort-Merge Join on Example50/87

Case 2:   Join[id=stude](Student,Enrolled)

 relations are not sorted on id#
 memory buffers N=32; all runs are of length < 30

 


 Cost 
 = 
 sort(S) + sort(E) + bS + bE 


 
 = 
 bS ⌈ log30 bS ⌉ + bE ⌈ log30 bE ⌉ + bS + bE 


 
 = 
 1,000 × 3 + 2,000 × 3 + 1,000 + 2,000 


 
 = 
 12,000 




... Sort-Merge Join on Example51/87


Case 3:   Join[id=stude](Student,Enrolled)

 Student and Enrolled already sorted on id#
 memory buffers N=3 (S input, E input, output)
 one-quarter of the "runs" in E span two pages
 there are no "runs" in S, since id# is a primary key

 
Cost depends on which relation is outer and which is inner.


... Sort-Merge Join on Example52/87

Case 3 (continued) ...

If E is outer relation:

 Cost  =  bE + bS  =  3,000

If S is outer relation:

 one-quarter of E runs require two page reads
 each E run is processed once for matching S.id value
 Cost  =  bS + bE + rS/4  =  8,000



Sidetrack 2: More on Iterators53/87

Above description of iterators:

 involved simple scan of a single table
 with no condition to select tuples

In the general case, an iterator involves:

 one (selection) or two (join) tables
 with a condition to determine relevant tuples

A typical SQL query involves many iterators

 one for each relational operator in query plan
 connected in a demand-driven network of query nodes



... Sidetrack 2: More on Iterators54/87

Requires a more general definition of execution state:
typedef struct {
    Oper   op;    // operation (sel,sort,join,...)
    Reln   r1;    // first relation
    Reln   r2;    // second relation (if any)
    Buffer *bufs; // buffers used by operation
    int    curp1; // index of current page for r1
    int    curr1; // index of current record in page
    int    curp2; // index of current page for r2
    int    curr2; // index of current record in page
    Cond   cond;  // condition for choosing tuple(s)
} Iterator;

For PostgreSQL details, see include/nodes/execnodes.h


Hash Join



Hash Join56/87

Basic idea:

 use hashing as a technique to partition relations
 to avoid having to consider all pairs of tuples

Requires sufficent memory buffers

 to hold substantial portions of partitions
 (preferably) to hold largest partition of outer relation

Other issues:

 works only for equijoin   R.i=S.j   (but this is a common case)
 susceptible to data skew   (or poor hash function)

Variations:   simple,   grace,   hybrid.


Simple Hash Join57/87

Basic approach:

 hash the inner relation R into memory buffers (build)
 scan the outer relation S, using hash to search (probe)

 if R.i=S.j, then h(R.i)=h(S.j)   (hash to same buffer)
 only need to check one memory buffer for each S tuple


Makes the assumption: whole of S hashes into memory

 requires R to be smaller than memory buffers
 requires a uniform hash function   (no overflows)



... Simple Hash Join58/87

Data flow:






... Simple Hash Join59/87

Algorithm for ideal simple hash join Join[R.i=S.j](R,S):
for each tuple r in relation R
   { insert r into buffer[h(R.i)] }
for each tuple s in relation S {
   for each tuple r in buffer[h(S.j)] {
      if ((r,s) satisfies join condition) {
         add (r,s) to result
      }
   }
}

Cost = bR + bS    (minimum possible cost)


... Simple Hash Join60/87

Consider that we have N buffers available.

If bR ≤ N-2 buffers, no need to hash   (use nested loop).

In practice, size of hash table bhR > bR
	  (e.g. data skew) 
⇒ hash table for R is even less likely to fit in memory

Can be handled by a variation on above algorithm:

 scan R, making hash table with N-2 buffers
 once hash table built, scan S   (standard probe phase)
 if more R tuples, build new table and repeat



... Simple Hash Join61/87

Algorithm for realistic simple hash join Join[R.i=S.j](R,S):
for each tuple r in relation R {
   if (buffer[h(R.i)] is full) {
      for each tuple s in relation S {
         for each tuple rr in buffer[h(S.j)] {
            if ((rr,s) satisfies join condition) {
               add (rr,s) to result
            }
         }
      }
      clear all hash table buffers
   }
   insert r into buffer[h(R.i)]
}

Note: requires multiple passes over the S relation.


... Simple Hash Join62/87

Cost depends on N and on properties of data/hash.

Worst case:

 h(i)=k so read only CR tuples before hash table "full"
 each hash table for R occupies one buffer with CR tuples
 degenerates to nested-loop-with-3-buffers case
	⇒ bR + bRbS

Best case:

 perfect uniform distribution of hash values
 each hash table of R holds (N-2)CR tuples from N-2 pages
 number of hash tables built = nhR = ⌈bR / (N-2)⌉
 read all of S for each hash table
	⇒ bR + nhR.bS



Grace Hash Join63/87

Basic approach:

 partition both relations on join attribute using hashing
 scan through corresponding pairs of partitions to form results


Similar approach to sort-merge join, except:

 sort-merge: partitioning achieved by sorting (runs)
 hash: partitioning achieved by hashing


Requires enough buffer space to hold largest partition
of inner relation.


... Grace Hash Join64/87

Partition phase:




This is applied to each relation R and S.


... Grace Hash Join65/87

Probe/join phase:




The second hash function (h2) simply speeds up the matching process.

Without it, would need to scan entire R partition for each record in S partition.



... Grace Hash Join66/87

Abstract algorithm for Join[R.i=S.j](R,S):
// assume h(val) generates [0..N-2]
// assume h2(val) generates [0..N-3]

// Partition phase (each relation -> N-1 partitions)
// 1 input buffer, N-1 output buffers

for each tuple r in relation R 
    add r to partition h(r.i) in output file R'
for each tuple s in relation S
    add s to partition h(s.j) in output file S'
...



... Grace Hash Join67/87

Abstract algorithm for Join[R.i=S.j](R,S) (cont.)
// Probe/join phase
// 1 input buffer for S, 1 output buffer
// N-2 buffers to build hash table for R partition

for each partition p = 0 .. N-2 {
    // Build in-memory hash table for partition p of R'
    for each tuple r in partition p of R'
        insert r into buffer h2(r.i)
    
    // Scan partition p of S', probing for matching tuples
    for each tuple s in partition p of S' {
        b = h2(s.j)
        for all matching tuples r in buffer b
            add (r,s) to result
}   }



... Grace Hash Join68/87

Concrete algorithm for partitioning:
Buffer iBuf, oBuf[N-1];
File inf, outf[N-1]; char rel[100];
int i, r, h, ip, op[N-1]; Tuple tup;
for (i = 0; i < N-1; i++) {
    clearBuf(oBuf[i]);   op[i] = 0;
    rel = sprintf("%s%d","Rel",i);
    outf[i] = openFile(fileName(rel),WRITE));
}
inf = openFile(fileName("Rel"),READ);
for (ip = 0; ip < nPages(inf); ip++) {
    iBuf = readPage(inf, ip);
    for (r = 0; r < nTuples(iBuf); r++) {
        tup = getTuple(iBuf, r);
        h = hash(tup.i, N-1);
        addTuple(oBuf[h], tup);
        if (isFull(oBuf[h])) {
            writePage(outf[h], op[h]++, oBuf[h]);
            clearBuf(oBuf[h]);
}   }   }



... Grace Hash Join69/87

Cost of grace hash join:

 #pages in all partition files of Rel ≅ bRel
	  (maybe slightly more)
 partition relation R ...
	  Cost  = 
	bR.Tr + bR.Tw
	 =  2bR
 partition relation S ...
	  Cost  = 
	bS.Tr + bS.Tw
	 =  2bS
 probe/join requires one scan of each (partitioned) relation 
	Cost  =  bR + bS
 all hashing and comparison occurs in memory   ⇒   ≅0 cost

Total Cost   =   3 (bR + bS)


... Grace Hash Join70/87

The above cost analysis assumes:

 every partition of R fits in memory buffers at once

We achieve this situation if:

 data has uniform distribution
 hash function gives uniform distribution 
	  (all partitions are similar size)
 we have N-1 ≥ ⌈ √b ⌉ memory buffers 
	  (giving N-1 partitions, each with ≅ bR/(N-1) pages)



... Grace Hash Join71/87

Possibilities for dealing with "over-long" partitions of R

 handle each over-long partition via scanning

 requires over-long partitions to be scanned multiple times
 essentially, such partitions are treated via nested loop join

 apply hash join recursively to over-long partitions

 increases i/o by needing to partition parts of file multiple times

 use a different hash function with better distribution properties

 but difficult to find such hash functions "on the fly"

 use the relation with the best partitioning as the "outer" relation



Grace Hash Join on Example72/87

For the example Join[id=stude](Student,Enrolled):

 assume that we have a good hash function and N = √1000 = 32



 Cost 
 = 
 3 (bS + bE) 


 
 = 
 3 (1,000 + 2,000) = 9,000 




Hybrid Hash Join73/87

An optimisation if we have √bR < N < bR+2

 create k partitions using N buffers where k ≪ N
 with grace join, would use k output buffers   (one per partition)
 what to do with N-k remaining buffers?   (ignore input buffer)
 use them to hold m partitions of R in memory   (no disk writes)
 other partitions are handled as before   (using k-m output buffers)

When we come to scan and partition S relation

 any tuple with hash in range 0..m-1 can be resolved
 other tuples are written to one of k-m partition files for S

Final phase is same as grace join, but with only k-m partitions.


... Hybrid Hash Join74/87

Some observations:

 for k partitions, each partition has expected size ceil(bR/k)
 holding m partitions in memory needs m×ceil(bR/k) buffers
 since we have k-m output buffers, we must have mbR/k +(k-m) ≤ N
 for every partition/block held in memory, we save on disk i/o
 saving is m/k × 2(bR+ bS)

Other notes:

 if N = bR+2, using block nested loop join is simpler
 cost depends on N (but less than grace hash join)



... Hybrid Hash Join75/87

Need to choose appropriate m and k to minimise cost

 base cost: 3 × (bR+ bS)   (grace join)
 i/o saving: m/k × 2(bR+ bS)
 constraint: mbR/k +(k-m) ≤ N

Approach to maximise saving:

 have one large in-memory partition   (m = 1)
 use as many as possible of N buffers for partition
 use as few output buffers as possible   (minimise k)



... Hybrid Hash Join76/87

Data flow for hybrid hash join (partitioning R):






... Hybrid Hash Join77/87

Data flow for hybrid hash join (partitioning S):




After this, proceed as for grace hash join.


... Hybrid Hash Join78/87

Cost of hybrid hash join:

 assume: large N total buffers, m partitions in memory, k partitions on disk
 read both tables: bR + bS
 total partitions for each table: m+k
 assuming uniform hashing, #pages in each R partition PR = ceil(bR/(m+k))
 assuming uniform hashing, #pages in each S partition PS = ceil(bS/(m+k))
 in Pass 1, k*PR + k*PS pages written to disk partitions
 all joining of m in-memory partitions is handled in memory
 in Pass2, k*PR + k*PS pages read back from disk partitions

Cost  =  bR + bS + k*PR + k*PS + k*PR + k*PS 
         =  bR + bS + 2 * k * (PR + PS)
         =  bR + bS + 2 * k * (ceil(bR/(m+k)) + ceil(bS/(m+k)) )


How to determine k:

 set m=1 and so size of partition ≅ N ⇒ k ≅ bR/N
 need to ensure that ⌈bR/k⌉ + k ≤ N   (allowing for input buffer)
 choose k close to bR/N but satisfying constraint



Hybrid Hash Join on Example79/87

Case 1:   N = 100 buffers, bR = 1000

 k = 10 ⇒ 1000/10 + 10 = 110 buffers; not less than 100
 k = 12 ⇒ 1000/12 + 12 = 96 buffers
 Cost = (3-2/12).(1000+2000) = 8500

Case 2:   N = 200 buffers, bR = 1000

 k = 5 ⇒ 1000/5 + 5 = 205 buffers; not less than 200
 k = 6 ⇒ 1000/6 + 6 = 173 buffers
 Cost = (3-2/6).(1000+2000) = 8000

Case 3:   N = 502 buffers, bR = 1000

 k = 2 ⇒ 1000/2 + 2 = 502 buffers
 Cost = (3-2/2).(1000+2000) = 6000



Pointer-based Join80/87

Conventional join algorithms set up R ↔ S connections via attribute values.

Join could be performed faster if direct connections already existed.

 in OODBMSs, they generally already exist in the form of object references (oids)
 in RDBMSs, they could be introduced via extra rid attributes

Such a modification to conventional RDBMS structure would be worthwhile:

 if we know in advance what kind of joins will be required
 adding the extra rid attributes into tuples is feasible 



... Pointer-based Join81/87

The basic idea for pointer-based join is:
for each tuple r in relation R {
    for each rid associated with r {
        fetch tuple s from S via rid
        add (r,s) to result relation
    }
}

Often, each R tuple is associated with only one rid, so the inner loop is not needed.


... Pointer-based Join82/87

The advantage over value-based joins:

 rather than find S tuples via value-based lookup
	  (e.g. hashing, index)
 we find S tuples by direct fetch with rid
	  (much faster per tuple)
 requires no assumption about sorted-ness of relations
 does not require large numbers of buffers

The (potential) disadvantages:

 every fetch goes to a different page of S 
	(this essentially returns us to the worst-case scenario for nested-loop join)
 the join only works in "one direction" (from R to S)
 requires additional data for each different join type
 requires tuples to be larger ⇒ bR is larger



General Join Conditions83/87

Above examples all used simple equijoin
	e.g. Join[i=j](R,S).

For theta-join e.g Join[i<j](R,S):

 index nested loop join:   need B+ tree index on inner relation
 sort-merge join can be adapted, but is not very effective
 hash join is inapplicable
 other methods are essentially unchanged



... General Join Conditions84/87

For multi-equality (pmr) join e.g. Join[i=j ∧ k=l](R,S)

 index nested loop join:

 build index on all join fields of inner relation
 e.g. if S is inner, build index on (S.j,S.l)

 sort-merge join:

 sort both relations on combined join fields
 e.g. sort R on (R.i,R.k), sort S on (S.j,S.l)

 hash-join:

 use multi-attribute hashing on combined join fields

 other methods are essentially unchanged



Join Summary85/87

No single join algorithm is superior in some overall sense.

Which algorithm is best for a given query depends on:

 sizes of relations being joined,   size of buffer pool
 any indexing on relations,   whether relations are sorted
 which attributes and operations are used in the query
 number of tuples in S matching each tuple in R

Choosing the "best" join algorithm is critical because the
cost difference between best and worst case can be very large.

E.g.   Join[id=stude](Student,Enrolled):   3,000 ... 2,000,000

In some cases, it may be worth modifying access methods "on the fly"
(e.g. add index) to enable an efficient join algorithm.


... Join Summary86/87

Comparison of join costs   (from Zeller/Gray VLDB90,
	assumes bR = bS = b)





Join in PostgreSQL87/87

Join implementations are under: src/backend/executor

PostgreSQL suports three kinds of join:

 nested loop join (nodeNestloop.c)
 sort-merge join   (nodeMergejoin.c)
 hash join   (nodeHashjoin.c)   (hybrid hash join)

Query optimiser chooses appropriate join, by considering

 physical characteristics of tables being joined
 estimated selectivity (likely number of result tuples)


Produced: 3 Nov 2018</p><h3 >字段8</h3><p>Query Processing


Query Processing Overview



(Translation, Optimisation, Execution)



Query Evaluation2/154






... Query Evaluation3/154

The most common form of interaction with a DBMS involves:

 supplying some input in the form of an SQL query
 getting back the results as a set of tuples






Overall approach clearly applies to select ... but also to delete and update.



... Query Evaluation4/154

A query in SQL:

 states what answers are required (declaratively)
 does not say how they should be computed

A query evaluator/processor :

 takes declarative description of query 
	  (in SQL)
 determines plan for answering query
	  (expressed as DBMS ops)
 executes plan via DBMS engine 
	  (to produce result tuples)

Typically: translate → plan → execute ... in a pipeline.

Some DBMSs can save query plans for later re-use 
	(because the planning step is potentially quite expensive).


... Query Evaluation5/154

Internals of the query evaluation "black-box":





... Query Evaluation6/154

Three phases of query evaluation:

 parsing/compilation 
	- input: SQL query, catalog 
	- using: parsing techniques, mapping rules 
	- output: relational algebra (RA) expression
 query optimisation 
	- input: RA expression, DB statistics 
	- using: cost models, search strategy 
	- output: query execution plan   (DB engine ops)
 query execution 
	- input: query execution plan 
	- using: database engine 
	- output: tuples that satisfy query

We ignore the "display" step; simply maps result tuples into appropriate format


Intermediate Representations7/154

SQL query text is not easy to manipulate/transform.
 
Need a query representation formalism that ...

 is powerful enough to express all queries
 has a well-defined, formal basis
 simplifies the query transformation process

Relational algebra (RA) expressions:

 are easy to transform and have a procedural interpretation

Thus, RA typically used as "target language" for SQL compilation.


... Intermediate Representations8/154

In principle, could base RA engine on 
{ σ, π, ∪, -, ×}   (completeness).

In practice, having only these operators makes execution inefficient.

The standard set of RA operations in a DBMS includes:

 filtering and combining   (select, project, join (inner,outer))
 set operations   (union, intersection, difference)

Other operations typically provided in extended RA engines:

 grouping (group by) and group-based selection (having)
 aggregates (count, sum, avg, max, min)
 sorting (order by),   uniq (distinct, sets)

RA rename operator is "hidden" in table/tuple representation.


... Intermediate Representations9/154

In practice, DBMSs provide several versions of each RA operation.

For example:

 several "versions" of selection (σ) are available
 each version is effective for a particular kind of selection, e.g
select * from R where id = 100  -- hashing
select * from S                 -- Btree index
where age > 18 and age < 35
select * from T                 -- grid file
where a = 1 and b = 'a' and c = 1.4



Similarly, π and ⋈ have versions to match
specific query types.


... Intermediate Representations10/154

We call these specialised version of RA operations RelOps.

One major task of the query processor:

 given a set of RA operations to be executed
 find a combination of RelOps to do this efficiently

Requires the query translator/optimiser to consider

 information about relations (e.g. sizes, primary keys, ...)
 information about operations (e.g. select reduces size)

RelOps are realised at execution time

 as a collection of inter-communicating nodes
 communicating either via pipelines or temporary relations



... Intermediate Representations11/154

Terminology variations ...

Relational algebra expression of SQL query

 intermediate query representation
 logical query plan

Execution plan as collection of RelOps

 query evaluation plan
 query execution plan
 physical query plan



Query Translation



... Intermediate Representations13/154

Query translation:   SQL statement text → RA expression





Query Translation14/154

Translation step is a mapping

 from the text of an SQL statement
 to a useful internal representation for optimisation

Standard internal representation is relational algebra (RA).

Mapping from SQL to RA may including some optimisations.

Mapping processes:  lexer/parser,  mapping rules,  rewriting rules.

Example:
SQL: select name from Students where id=7654321;
-- is translated to
RA:  Proj[name](Sel[id=7654321]Students)



Parsing SQL15/154

Parsing task is similar to that for programming languages.

Language elements:

 keywords:   create,   select,   from,   where,   ...
 identifiers:   Students,   name,   id,   CoursCode,   ...
 operators:   +,   -,   =,   <,   >,   AND,   OR,   NOT,   IN,   ...
 constants:   'a string',   123,   19.99,   '01-jan-1970'


One difference to parsing PLs:

 PLs define objects as part of program definition (e.g. in *.h)
 SQL references tables/types/functions stored in the DBMS

Therefore, SQL parser needs access to catalog for definitions.


... Parsing SQL16/154

PostgreSQL dialect of SQL ...

 large set of keywords (> 700 of them)   (e.g. select)
 parser is implemented via lex/yacc (src/backend/parser)
 handles multiple languages   (i.e. Unicode strings)
 maps all identifiers to lower-case   (A-Z → a-z)
 needs to handle user-extendable operator set
 makes extensive use of catalog   (src/backend/catalog)

 pg_class hold pre- and user-defined tables
 pg_type hold pre- and user-defined types
 pg_proc hold pre- and user-defined functions




Catalog and Parsing17/154

Consider the following simple University schema:
Staff(id, name, position, office, phone, ...)
Students(id, name, birthday, degree, avg, ...)
Subjects(id, title, prereqs, syllabus, ...)
Enrolments(sid, subj, session, mark, ...)

DBMS catalog stores following kinds of information:

 Staff, Students, ... are relations owned by some user
 id, name ... are fields of the Staff relation
 id has type INTEGER and contains a unique value
 there are 1000 tuples in Staff, 20000 tuples in Students, ...
 Enrolments.sid is a foreign key referencing Students.id
 a Student is associated to <40 Subjects via Enrolments(?)



... Catalog and Parsing18/154

The schema/type information is used for

 checking that the named tables and attributes exist
 resolving attribute references
	(e.g. is id from Students or Subjects?)
 checking that attrs are used appropriately
	(e.g. not id='John')

The statistical information is used for

 choosing appropriate operators for query execution plans

Examples:

 a query with exactly one solution: 
	select * from Students where id=12345
 a query with thousands of solutions: 
	select * from Students where degree='BSc'



Query Blocks19/154

A query block is an SQL query with

 no nesting
 exactly one SELECT, FROM clause
 at most one WHERE, GROUP-BY, HAVING clause

Query optimisers typically deal with one query block at a time ...

⇒ SQL compilers need to decompose queries into blocks

Interesting kinds of blocks:

 non-correlated query returning a set of tuples/values
 non-correlated query returning a single result   (treat as constant)
 correlated sub-query   (query changes for each outer tuple)



... Query Blocks20/154

Consider the following example query:
SELECT s.name FROM Students s
WHERE  s.avg = (SELECT MAX(avg) FROM Students)

which consists of two blocks:
Block1: SELECT MAX(avg) FROM Students
Block2: SELECT s.name FROM Students s
        WHERE  s.avg = <<Block1>>>

Query processor arranges execution of each block separately
and transfers result of Block1 to Block2.


Mapping SQL to Relational Algebra21/154

A naive query compiler might use the following translation scheme:

 SELECT clause   →   projection
 FROM clause   →   product
 WHERE clause   →   selection

Example:
SELECT s.name, e.subj
FROM   Students s, Enrolments e
WHERE  s.id = e.sid AND e.mark > 50;

is translated to


 πs.name,e.subj( σs.id=e.sid ∧ e.mark>50 ( Students × Enrolments ) )



... Mapping SQL to Relational Algebra22/154

A better translation scheme would be something like:

 SELECT clause   →   projection
 WHERE clause on single relation   →   selection
 WHERE clause on two relations   →   join

Example:
SELECT s.name, e.subj
FROM   Students s, Enrolments e
WHERE  s.id = e.sid AND e.mark > 50;

is translated to


 πs.name,e.subj( σe.mark>50 ( Students ⋈s.id=e.sid Enrolments ) ) )



... Mapping SQL to Relational Algebra23/154

In fact, many SQL compilers ...

 produce the cross-product version as an intermediate result
 map it into the join version by applying rewriting rules

This is one instance of a general query translation process

 expression rewriting via algebraic laws on RA expressions

Aim of rewriting: convert a given RA expression

 into an equivalent RA expression
 that is guaranteed/likely to be more efficient

(Rewriting is discussed later)


Mapping Rules24/154

Mapping from an SQL query to an RA expression requires:

 a collection of templates for particular kinds of queries
 a matching process to ...

 determe what kind of query we have (i.e. choose a template)
 bind components of actual query to slots in the template

 mapping rules to ...

 convert the matched query into relational algebra
 filling slots in RA expression from matched components


May need to use multiple templates to map whole SQL statement.


... Mapping Rules25/154

Projection:

 SELECT   f1, f2, ... fn   FROM   ...  

⇒    Project[f1, f2, ... fn](...) 

SQL projection extends RA projection with renaming and assignment:

 SELECT   a+b AS x, c AS y   FROM   R ...  

⇒    Project[x ← a+b, y ← c](R)


... Mapping Rules26/154

Join:   (e.g. on R(a,b,c,d) and S(c,d,e))

 SELECT  ...  FROM  ... R, S ...  WHERE  ... R.a  op  S.e ... ,    or

 SELECT  ...  FROM  ... R  JOIN  S  ON  (R.a  op  S.e) ...  WHERE  ...

 ⇒    Join[R.a op S.e](R,S)

 SELECT  ...  FROM  ... R  NATURAL JOIN  S,    or

 SELECT  ...  FROM  ... R  JOIN  S  USING  (c,d) ...  WHERE  ...

 ⇒    Proj[a,b,e](Join[R.c=S.c ∧ R.d=S.d](R,S))


... Mapping Rules27/154

Selection:

 SELECT  ...  FROM  ...  R  ...  WHERE  ... R.f  op  val ... 

 ⇒    Select[R.f op val](R) 

 SELECT  ...  FROM  ... R ...  WHERE  ... Cond1,R  AND  Cond2,R ... 

 ⇒    Select[Cond1,R ∧ Cond2,R](R) 
or 
 ⇒    Select[Cond1,R](Select[Cond2,R](R))
or 
 ⇒    Select[Cond2,R](Select[Cond1,R](R))


... Mapping Rules28/154

Aggregation operators (e.g. MAX, SUM, ...):

 add as new operators in extended RA

e.g. SELECT MAX(age) FROM ...   ⇒   max(Project[age](...)) 
 incorporate into projection operator

e.g. SELECT MAX(age) FROM ...   ⇒   Project[age](max,...) 
 add new projection operators

e.g. SELECT MAX(age) FROM ...   ⇒   ProjectMax[age](...) 



... Mapping Rules29/154

Sorting (ORDER BY):

 add Sort operator into extended RA

Duplicate elimination (DISTINCT):

 add Uniq operator into extended RA
	  (e.g. Uniq(Project(...)))
 or, extend RA ops with uniq parameter
	  (e.g. Project(uniq,...)))

Grouping (GROUP BY, HAVING):

 add operators into extended RA
	  (e.g. GroupBy, GroupSelect )



... Mapping Rules30/154

View definitions produce:

 mapping of view statement to RA expression
 association between view name and RA expression

Example: assuming Employee(id,name,birthdate,salary)
create view OldEmps as
select * from Employees
where birthdate < '01-01-1960';

yields

OldEmps  =  Select[birthdate<'01-01-1960'](Employees)


... Mapping Rules31/154

General case:

CREATE VIEW  V   AS   SQLstatement

 ⇒    V  =  mappingOf(SQLstatement)

Special case: views with attribute renaming:

 CREATE VIEW  V(a,b,c)  AS   SELECT  h,j,k  FROM  R  WHERE  C  ... 

 ⇒    V  =  Proj[a ← h, b ← j, c ← k](Select[C](R))


... Mapping Rules32/154

Views used in queries:

 references to views are replaced by the view definition
 introduces a new subexpression into the RA expression
 may require renaming of some attributes

Example:

select name from OldEmps;
	  -- using OldEmps as defined above

 ⇒    Projname(mappingOf(OldEmps)) 

 ⇒    Projname(Select[birthdate<'01-01-1960'](Employees)) 


Mapping Example33/154

The query


List the names of all subjects with more than 100 students in them


can be expressed in SQL as
select   distinct s.code
from     Course c, Subject s, Enrolment e
where    c.id = e.course and c.subject = s.id
group by s.id
having   count(*) > 100;



... Mapping Example34/154

In the SQL compiler, the query
select   distinct s.code
from     Course c, Subject s, Enrolment e
where    c.id = e.course and c.subject = s.id
group by s.id
having   count(*) > 100;

might be translated to the relational algebra expression

Uniq(Project[code](
    GroupSelect[groupSize>100](
        GroupBy[id] (
            Enrolment ⋈ Course ⋈ Subjects
))))



... Mapping Example35/154

The join operations could be done in two different ways:




The query optimiser determines which has lower cost.

Note: for a join involving n tables, there are O(n!) possible trees.


Expression Rewriting Rules36/154

Since RA is a well-defined formal system

 there exist many algebraic laws on RA expressions
 which can be used as a basis for expression rewriting
 in order to produce equivalent (more-efficient) expressions

Expression transformation based on such rules can be used

 to simplify/improve SQL→RA mapping results
 to generate new plan variations during query optimisation



Relational Algebra Laws37/154

Commutative and Associative Laws:

 R × S  ↔  S × R,    (R × S) × T  ↔  R × (S × T)
 R ⋈ S  ↔  S ⋈ R,    (R ⋈ S) ⋈ T  ↔  R ⋈ (S ⋈ T)    (natural join)
 R ∪ S  ↔  S ∪ R,    (R ∪ S) ∪ T  ↔  R ∪ (S ∪ T)
 R ∩ S  ↔  S ∩ R,    (R ∩ S) ∩ T  ↔  R ∩ (S ∩ T)
 R ⋈Cond S  ↔  S ⋈Cond R    (theta join)

But it is not true in general that

 (R ⋈Cond1 S) ⋈Cond2 T  ↔  R ⋈Cond1 (S ⋈Cond2 T)

Example:   R(a,b),   S(b,c),   T(c,d),   (R Join[R.b>S.b] S) Join[a<d] T

Cannot rewrite as R Join[R.b>S.b] (S Join[a<d] T) because neither S.a nor T.a exists.


... Relational Algebra Laws38/154

Selection commutativity   (where c is a condition):

  σc ( σd (R))   ↔   σd ( σc (R)) 

Selection splitting (where c and d are conditions):

  σc ∧ d(R)   ↔   σc ( σd (R)) 
  σc ∨ d(R)   ↔   σc(R)  ∪  σd(R) 
	  (but only if R is a set)

Selection pushing   ( σc(R op S) ):

 σc(R ∪ S)  ↔  σcR ∪ σcS
(must be pushed into both arguments of union)
 σc(R - S)  ↔  σcR - S,   
	σc(R - S)  ↔  σcR - σcS
(must be pushed into left branch of difference, may be pushed to right branch)



... Relational Algebra Laws39/154

Selection pushing with join, cross-product and intersection ...

If c refers only to attributes from R:

  σc (R ⋈ S)   ↔   σc(R)  ⋈  S
	  (similarly for × and ∩)

If c refers only to attributes from S:

  σc (R ⋈ S)   ↔   R  ⋈  σc(S)
	  (similarly for × and ∩)

If c refers to attributes from both R and S:

  σc (R ⋈ S)   ↔   σc(R)  ⋈  σc(S)
 above is always true for ∩   (union-compatible)



... Relational Algebra Laws40/154

Rewrite rules for projection ...

All but last projection can be ignored:

  πL1 ( πL2 ( ... πLn (R)))   ↔   πL1 (R) 


Projections can be pushed into joins:

  πL (R  ⋈c S)   ↔   πL ( πM(R)  ⋈c  πN(S) ) 

where 

 M and N must contain all attributes needed for c
 M and N must contain all attributes used in L
	  (L ⊂ M∪N)



Mapping Subqueries41/154

Two varieties of sub-query:    (sample schema R(a,b), S(c,d))

 independent: sub-query makes no reference to outer query
select * from R
where a in (select c from S where d>5)

select * from R
where a = (select max(c) from S)

 correlated: sub-query depends on data from outer query
select * from R
where a in (select c from S where d=R.b)


Standard strategy: convert to a join.


... Mapping Subqueries42/154

Example of mapping independent subquery ...
select * from R
where a in (select c from S where d>5)

is mapped as

⇒   Sel[a in Proj[c](Sel[d>5]S)](R)

⇒   Sel[a=c](R × Proj[c](Sel[d>5](S)))

⇒   R  Join[a=c]  Proj[c](Sel[d>5](S))


Query Optimisation



Query Optimisation44/154

Query optimiser:   RA expression → efficient evaluation plan





... Query Optimisation45/154

Query optimisation is a critical step in query evaluation.

The query optimiser

 takes a relational algebra expression from SQL compiler
 produces a sequence of RelOps to evaluate the expression
 the query execution plan yields an efficient evaluation


"Optimisation" is necessary because there can be enormous differences
in costs between different plans for evaluating a given RA expression.

E.g.   tmp ← A × B;
	  res ← σx(tmp)     vs    
	res ← A ⋈x B


Query Evaluation Example46/154

Example database schema (multi-campus university):
Employee(eid, ename, status, city)
Department(dname, city, address)
Subject(sid, sname, syllabus)
Lecture(subj, dept, empl, time)

Example database instance statistics:




 Relation 
 r 
 R 
 C 
 b 


 Employee 
 1000 
 100 
 10 
 100 


 Department 
 100 
 200 
 5 
 20 


 Subject 
 500 
 95 
 10 
 100 


 Lecture 
 2000 
 100 
 10 
 200 





... Query Evaluation Example47/154

Query: Which depts in Sydney offer database subjects?


select dname
from   Department D, Subject S, Lecture L
where  D.city = 'Sydney' and S.sname like '%Database%'
       and D.dname = L.dept and S.sid = L.subj

Additional information (needed to determine query costs):

 5 departments are located in Sydney 
 80 subjects are about databases
 300 lectures are on databases
 100 lectures are in Sydney
 3 of these are on databases



... Query Evaluation Example48/154

Consider total I/O costs for five evaluation strategies.

Assumptions in computing costs:

 database tables are unsorted and have no indexes
 intermediate tables are written to disk and then re-read

These are worst-case scenarios and could be improved by

 having indexes on database tables
 writing intermediate results as e.g. sorted/hashed



... Query Evaluation Example49/154

Strategy #1 for answering query:

 TMP1    ←    Subject × Lecture
 TMP2    ←    TMP1 × Department
 TMP3    ←    Select[check](TMP2)

	 check =
	(city='Sydney' & sname='Databases' & dname=dept & sid=subj)
	
 RESULT    ←   Project[dname](TMP3)



... Query Evaluation Example50/154

Costs involved in using strategy #1:




 Reln 
 r 
 R 
 C 
 b 


 TMP1 
 1000000 
 195 
 5 
 20000 


 TMP2 
 100000000 
 395 
 2 
 50000000 


 TMP3 
 3 
 395 
 2 
 2 


 RESULT 
 3 
 100 
 10 
 1 




Total I/Os

= CostStep1 + CostStep2 + CostStep3 + CostStep4

= (100+100*200+20000) + (20+20*20000+5*106) + (5*106+2) + 2

= 100,440,124


... Query Evaluation Example51/154

Strategy #2 for answering query:

 TMP1    ←    Join[sid=subj](Subject,Lecture)
 TMP2    ←    Join[dept=dname](TMP1,Department)
 TMP3    ←    Select[city='Sydney' & sname='Databases'](TMP2)
 RESULT    ←    Project[dname](TMP3)



... Query Evaluation Example52/154

Costs involved in using strategy #2:




 Reln 
 r 
 R 
 C 
 b 


 TMP1 
 2000 
 195 
 5 
 400 


 TMP2 
 2000 
 395 
 2 
 1000 


 TMP3 
 3 
 395 
 2 
 2 


 RESULT 
 3 
 100 
 10 
 1 




Total I/Os

= CostStep1 + CostStep2 + CostStep3 + CostStep4

= (100+100*200+400) + (20+20*400+1000) + (1000+2) + 2

= 30,524


... Query Evaluation Example53/154

Strategy #3 for answering query:

 TMP1    ←    Join[sid=subj](Subject,Lecture)
 TMP2    ←    Select[sname='Databases'](TMP1)
 TMP3    ←    Join[dept=dname](TMP2,Department)
 TMP4    ←    Select[city='Sydney'](TMP3)
 RESULT    ←    Project[dname](TMP4)



... Query Evaluation Example54/154

Costs involved in using strategy #3:




 Reln 
 r 
 R 
 C 
 b 


 TMP1 
 2000 
 195 
 5 
 400 


 TMP2 
 20 
 195 
 5 
 4 


 TMP3 
 20 
 395 
 2 
 10 


 TMP4 
 3 
 395 
 2 
 2 


 RESULT 
 3 
 100 
 10 
 1 




Total I/Os

= CostStep1 + CostStep2 + CostStep3 + CostStep4 + CostStep5

= (100+100*200+400) + (400+4) + (4+4*20+10) + (10+2) + 1

= 21,011


... Query Evaluation Example55/154

Strategy #4 for answering query:

 TMP1    ←    Select[sname='Databases'](Subject)
 TMP2    ←    Select[city='Sydney'](Department)
 TMP3    ←    Join[sid=subj](TMP1,Lecture)
 TMP4    ←    Join[dept=dname](TMP3,TMP2)
 RESULT    ←    project[dname](TMP4)



... Query Evaluation Example56/154

Costs involved in using strategy #4:




 Reln 
 r 
 R 
 C 
 b 


 TMP1 
 80 
 95 
 5 
 16 


 TMP2 
 5 
 200 
 5 
 1 


 TMP3 
 300 
 195 
 5 
 60 


 TMP4 
 3 
 395 
 2 
 2 


 RESULT 
 3 
 100 
 10 
 1 



Total I/Os

= CostStep1 + CostStep2 + CostStep3 + CostStep4 + CostStep5

= (100+16) + (20+1) + (16+16*200+60) + (1+1*60+2) + 2

= 3478


... Query Evaluation Example57/154

Strategy #5 for answering query:

 TMP1    ←    Select[sname='Databases'](Subject)
 TMP2    ←    Select[city='Sydney'](Department)
 TMP3    ←    Join[dept=dname](TMP2,Lecture)
 TMP4    ←    Join[sid=subj](TMP3,TMP1)
 RESULT    ←    project[dname](TMP4)



... Query Evaluation Example58/154

Costs involved in using strategy #5:




 Reln 
 r 
 R 
 C 
 b 


 TMP1 
 80 
 95 
 5 
 16 


 TMP2 
 5 
 200 
 5 
 1 


 TMP3 
 100 
 295 
 3 
 34 


 TMP4 
 3 
 395 
 2 
 2 


 RESULT 
 3 
 100 
 10 
 1 



Total I/Os

= CostStep1 + CostStep2 + CostStep3 + CostStep4 + CostStep5

= (100+16) + (20+1) + (1+1*200+34) + (16+16*34+2) + 2

= 936


Query Optimisation Problem59/154

Given:

 a query Q,
	  a database D,
	  a database "engine" E

Determine a sequence of relational algebra operations that:

 produces the answer to Q in D
 executes Q efficiently on E   (minimal I/O)

The term "query optimisation" is a misnomer:

 not just for queries    (e.g. also updates)
 not necessarily optimal    ("reasonably efficient")



... Query Optimisation Problem60/154

Why do we not generate optimal query execution plans?

Finding an optimal query plan ...

 requires an exhaustive search of a space of possible plans
 for each possible plan, need to estimate cost
	(not cheap)

Even for a small query (e.g. 4-5 joins, 4-5 selects)

 the space of possible query plans is very large 
	(choices: order of operations, access methods, intermediate results, etc.)
 cost of searching plan space ≅ cost of executing query

Compromise:

 do limited searching of query plan space
	  (guided by heuristics)
 quickly choose a reasonably efficient execution plan



Cost Models and Analysis61/154

The cost of evaluating a query is determined by:

 size of relations   (database relations and temporary relations)
 access mechanisms   (indexing, hashing, sorting, join algorithms)
 size/number of main memory buffers   (and replacement strategy)

Analysis of costs involves estimating:

 size of intermediate results
 number of secondary storage accesses



Approaches to Optimisation62/154

Three main classes of techniques developed:

 algebraic     (equivalences, rewriting, heuristics)
 physical      (execution costs, search-based)
 semantic     (application properties, heuristics)

All driven by aim of minimising (or at least reducing) "cost".

Real query optimisers use a combination of algrebraic+physical.

Semantic QO is good idea, but expensive/difficult to implement.


Optimisation Process63/154

Start with RA tree representation of query, then ...

 apply algebraic query transformations


 giving standardised, simpler, more efficient RA tree


 generate possible access plans (physical)


 replace each RA operation by specific access method to implement it
 consider possible orders of join operations (left-deep trees)


 analyse cost of generated plans and select cheapest

Result: tree of DBMS operations to answer query efficiently.


... Optimisation Process64/154

Example of optimisation transformations:




For join, may also consider sort/merge join and hash join.


Algebraic Optimisation65/154

Make use of algebraic equivalences:

 examine query expression
 search for applicable transformation rules (heuristics)
 generate equivalent (and "better") algebraic expressions

Most commonly used heuristic:


Apply Select and Project before Join


Rationale: minimises size of intermediate relations.

Can potentially be done during the SQL→RA mapping phase.

Algebraic optimisation cannot assist with finding good join order.


Physical Optimisation66/154

Makes use of execution cost analysis:

 examine query evaluation plan
 determine efficient join sequences
 select access method for each operation
	  (e.g. index for select)
 for distributed DB, select best sites
	  (closest, best bandwidth)
 determine total cost for evaluation plan
 repeat for all possible plans and choose best

Physical optimisation is also called query evaluation plan generation.


Semantic Optimisation67/154

Make use of application-specific properties:

 functional dependencies
 attributes constraints
 tuple constraints
 database constraints

Can be applied in algebraic or physical optimisation phase.

Basis: exploit meta-data and other semantic info about relations.

(E.g. this field is a primary key, so we know there'll only be one
matching tuple)


Stages in Algebraic Optimisation68/154

Start with relational algebra (RA) expression.

 Standardise

 construct normal form of expression
	  (boolean algebra laws)

 Simplify

 transform to eliminate redundancy
	  (boolean algebra and RA laws)

 Ameliorate

 transform to improve efficiency
	  (RA laws and heuristics)


Result: an RA expression equivalent to the input, but more efficient.


Standardisation69/154

Many query optimisers assume RA expression is in a "standard form".

E.g. select conditions may be stated in

 conjunctive normal form 
(A1 AND ... AND An) OR ... OR (Z1 AND ... AND Zm)
 disjunctive normal form 
(A1 OR ... OR An) AND ... AND (Z1 OR ... OR Zm)

RA expression is first transformed into one of these forms.


Simplification70/154

Main aim of simplification is to reduce redundancy.

Makes use of tranformation rules based on laws of boolean algebra.




 A AND B 
 ↔ 
 B AND A 


 
A OR (B AND C) 
 ↔ 
 (A OR B) AND (A OR C) 


 
A AND A 
 ↔ 
 A 


 
A OR NOT(A) 
 ↔ 
 True 


 
A AND NOT(A) 
 ↔ 
 False 


 
A OR (A AND B) 
 ↔ 
 A 


 
NOT(A AND B) 
 ↔ 
 NOT(A) OR NOT(B) 


 
NOT(NOT(A)) 
 ↔ 
 A 




(Programmers don't produce redundant expressions; but much SQL is produced by programs)


Simplification Example71/154

Consider the query:
select Title from Employee
where  (not (Title = 'Programmer')
        and (Title = 'Programmer'
             or Title = 'Analyst')
        and not (Title = 'Analyst'))
       or Name = 'Joe'

Denote the atomic conditions as follows:


P = (Title = 'Programmer') 
A = (Title = 'Analyst') 
J = (Name = 'Joe')



... Simplification Example72/154

Selection condition can then be simplified via:




 Cond 
 = 
 ( P  ∧  (P  ∨  A)  ∧   A)  ∨  J 



 = 
 (( P  ∧   A)  ∧ (P  ∨  A))  ∨  J 



 = 
 ( (P  ∨  A)  ∧  (P  ∨  A))  ∨  J 



 = 
 False  ∨  J 



 = 
 J 




Giving the equivalent simplified query:
select Title from Employee where Name = "Joe"



Amelioration73/154

Aim of amelioration is to improve efficiency.

Transform query to equivalent form which is known to
be more efficient.

Achieve this via:

 relational algebra laws/transformations
 heuristics to choose which to apply

Often describe RA expression as a tree and
RA transformations as tree transformations.


Amelioration Process74/154


 break up σa ∧ b ∧ c ... into ``cascade'' of σ 
	(gives more flexibility for later transformations)
 move σ as far down as possible 
	(reduces size of intermediate results)
 move most restrictive σ down/left 
	(reduces size of intermediate results)
 move π as far down as possible 
	(reduces size of intermediate results)
 replace × then σ by equivalent ⋈ 
	(reduces computation/size of intermediate results)
 replace subexpressions by single operation 
	(e.g. opposite of 1.) (reduces computation overhead)



Amelioration Example75/154

Consider school information database:
CSG(Course, StudentId, grade)
SNAP(StudentId, name, address, phone)
CDH(Course, Day, Hour)
CR(Course, Room)

And the query:


Where is Brown at 9am on Monday morning?



... Amelioration Example76/154

An obvious translation of the query into SQL
select Room from CSG,SNAP,CDH,CR
where  name='Brown' and day='Mon' and hour='9am'

and an obvious mapping of the SQL into relational algebra gives


 πRoom ( σN&D&H ( CSG ⋈ SNAP ⋈ CDH ⋈ CR)) 



  N  is  name='Brown',   D  is  Day='Mon',   H  is  Hour='9am' 



... Amelioration Example77/154



 πRoom ( σN&D&H ( CSG ⋈ SNAP ⋈ CDH ⋈ CR)) 


Push selection:


 πRoom ( σN&D&H ( CSG ⋈ SNAP ⋈ CDH) ⋈ CR) 


Split selection:


 πRoom ( σN ( σD&H ( CSG ⋈ SNAP ⋈ CDH)) ⋈ CR) 


Push two selections:


 πRoom ( ( σN (CSG ⋈ SNAP) ⋈ σD&H (CDH)) ⋈ CR) 


Push selection:


 πRoom ( ((CSG ⋈ σN (SNAP)) ⋈ σD&H (CDH)) ⋈ CR) 


Can we show that the final version is more efficient?


... Amelioration Example78/154

Could use an argument based on size of intermediate results, as above.

But run into problems with expressions like:


 CSG ⋈ SNAP ⋈ CDH ⋈ CR 


Order of joins can make a significant difference?

How to decide "best" order?

Need understanding of physical characteristics of relations 
(for example, selectivity and likelihood of join matches across tables)


Physical (Cost-Based) Optimisation79/154

Need to determine:

 order in which operations applied
	  (execution plan)
 precisely how each operation done
	  (map to DBMS ops)
 size of intermediate results
	  (need to estimate these)

From these, estimate overall evaluation cost.

Do this for all possible execution plans.

Choose cheapest plan.


Execution Plans80/154

Consider query execution plans for the RA expression:


 σc (R  ⋈d  S  ⋈e  T) 


Plan #1
tmp1   :=  HashJoin[d](R,S)
tmp2   :=  SortMergeJoin[e](tmp1,T)
result :=  BinarySearch[c](tmp2)



... Execution Plans81/154

Plan #2
tmp1   := SortMergeJoin[e](S,T)
tmp2   := HashJoin[d](R,tmp1)
result := LinearSearch[c](tmp2)

Plan #3
tmp1   := BtreeSearch[c](R)
tmp2   := HashJoin[d](tmp1,S)
result := SortMergeJoin[e](tmp2)

All plans produce same result, but have quite different costs.


Cost-based Query Optimiser82/154

Overview of cost-based query optimisation process:

 start with RA tree from compilation of SQL query
 use RA laws as rewrite rules to generate new RA trees
 for each node in RA tree, choose specific access method

For a given SQL query, there are

 very many possible RA trees 
	  (large choice of re-write rules)
 many possible execution plans
	  (choice of access methods, params)

Too many combinations to enumerate all; prune via heuristics.


... Cost-based Query Optimiser83/154

Approximate algorithm for cost-based optimisation:
translate SQL query to RAexp
for all transformations RA' of RAexp {
    for each node e of RA' (recursively) {
        select access method for e
        plan[i++] = access method for e
    }
    cost = 0
    for each op in plan[]
        { cost += Cost(op) }
    if (cost < MinCost) {
        MinCost = cost
        BestPlan = plan
}   }


As noted above, we do not consider *all* transformations. 
Heuristics: push selections down, consider only left-deep join trees.



Choosing Access Methods84/154

Inputs:

 a single RA operation (σ,   π,   ⋈)
 information about file organisation, data distribution, ...
 list of operations available in the database engine

Output:

 calls to database operations to implement this RA operation



... Choosing Access Methods85/154

Example:

 RA operation: σname='John' ∧ age>21(Student)
 Student relation has B-tree index on name
 database engine (obviously) has B-tree search method

giving
tmp[i]   := BtreeSearch[name='John'](Student)
tmp[i+1] := LinearSearch[age>21](tmp[i])

Where possible, use pipelining to avoid storing tmp[i] on disk.


... Choosing Access Methods86/154

Rules for choosing σ access methods:

 σA=c(R) and R has index on A
	  ⇒   indexSearch[A=c](R)
 σA=c(R) and R is hashed on A
	  ⇒   hashSearch[A=c](R)
 σA=c(R) and R is sorted on A
	  ⇒   binarySearch[A=c](R)
 σA ≥ c(R) and R has clustered index on A
	  ⇒   indexSearch[A=c+1](R) then scan
 σA ≥ c(R) and R is hashed on A
	  ⇒   linearSearch[A>=c](R)



... Choosing Access Methods87/154

Rules for choosing ⋈ access methods:

 R ⋈ S and R fits in memory buffers
	  ⇒   bnlJoin(R,S)
 R ⋈ S and R,S sorted on join attr
	  ⇒   smJoin(R,S)   (merge only)
 R ⋈ S and R has index on join attr
	  ⇒   inlJoin(S,R)
 R ⋈ S and no indexes, no sorting, |R| ≪ |S|
	  ⇒   hashJoin(R,S)

 

(bnl = block nested loop;
  inl = index nested loop;
  sm = sort merge)



Example Plan Enumeration88/154

Consider the query:
select name
from   Student s, Enrol e
where  s.sid = e.sid AND e.cid = 'COMP9315';

where




 Relation 
 r 
 b 
 PrimKey 
 Clustered 
 Indexes 


 Student 
 10000 
 500 
 sid 
 No 
 B-tree on sid 


 Enrol 
 40000 
 400 
 cid 
 Yes 
 B-tree on cid 


 S ⋈ E 
 40000 
 800 
 sid,cid 
 No 
 - 





... Example Plan Enumeration89/154

First step is to map SQL to RA expression


 σ9315 ( Student   ⋈sid   Enrol ) 


then devise an execution plan based on this expression

 index on Student.sid
	   ⇒   index nested loop join
 join result not sorted on Enrol.cid
	   ⇒   linear scan

giving
tmp1   := inlJoin[s.sid=e.sid](Enrol,Student)
result := LinearSearch[e.id='COMP9315'](tmp1)



... Example Plan Enumeration90/154

Estimated cost for this plan:



 Cost 
 = 
 inl-join on (Enrol,Student) + linear scan of tmp1 


 
 = 
 Tr(bS + rS.btreeE) + Twbtmp1 + Trbtmp1  


 
 = 
 500 + 10,000.4 + 800 + 800 


 
 = 
 42,100 



Notes:

 if we assume pipelining on tmp1, can remove (800+800) term
 if we make Enrol the outer relation, join cost is 400 + 40,000.3



... Example Plan Enumeration91/154

Apply rule for pushing select into join:


 σ9315 ( Student   ⋈sid   Enrol ) 

giving

 Student   ⋈sid   σ9315 ( Enrol ) 


and devise a plan based on

 index on Enrol.cid
	   ⇒   B-tree lookup
 σ9315 output small
	   ⇒   buffered nested loop join

giving
tmp1   := BtreeSearch[e.cid='COMP9315'](Enrol)
result := bnlJoin[s.sid=e.sid](tmp1,Student)



... Example Plan Enumeration92/154

Estimated cost for this plan:



 Cost 
 = 
 btree-search on Enrol + bnl-join of (tmp1,Student) 


 
 = 
 search+scan on Enrol + Tr(btmp1 + bS)  


 
 = 
 3Tr + 3Tr + Tr(1 + 500) 


 
 = 
 3 + 3 + 1 + 500   =   507 



Notes:

 assumes 200 students enrolled in COMP9315
 with pipelining 1Tr would vanish
 if Enrol no clustered, 3Tr scan would increase



Cost Estimation93/154

Without actually executing it, cannot always know the cost of plan precisely.

(E.g. how big is the select result that feeds into the join?)

Thus, query optimisers need to estimate costs.

Two aspects to cost estimation:

 cost of performing operation 
	  (dealt with extensively in earlier lectures)
 size of result
	  (which affects cost of performing next operation)



Cost Estimation Information94/154

Cost estimation uses statistical information about relations:




 rS 
 cardinality of relation S 


 RS    
 avg size of tuple in relation S 


 V(A,S) 
 # distinct values of attribute A 


 min(A,S) 
 min value of attribute A 


 max(A,S) 
 max value of attribute A 





Estimating Projection Result Size95/154

Straightforward, since we know:

 all tuples from input table are included in result
 all required attributes (and their sizes) ⇒ Rout

#bytes in result = rS × Rout

If pipelining through buffers of size B, then

#buffers-full = rS × ⌈B/Rout⌉


Estimating Selection Result Size96/154

Selectivity factor = fraction of tuples expected to satisfy a condition.

Common assumption: attribute values uniformly distributed.

Example:
Consider the query
select * from Parts where colour='Red'

and assume that there are only four possible colours.

If we have 1000 Parts, we would expect 250 of them to be Red.

In other words,


 V(colour,R)=4,   rR=1000  ⇒  |σcolour=K(R)|=250 


In general, |σA=c(R)|  ≅ 
	 rR/V(A,R)


... Estimating Selection Result Size97/154

Estimating size of result for e.g. 
select * from Enrolment
where  year > 2003;

Could estimate by using:

 uniform distribution assumption, r, min/max years

Assume: min(year)=1996, max(year)=2005, |Enrolment|=105

 105 from 1996-2005 means approx 10000 enrolments/year
 this suggests 20000 enrolments since 2003 


Of course, we know that enrolments grow continually, so this underestimates.


Simpler heuristic used by some systems: selected tuples = r/3


... Estimating Selection Result Size98/154

Estimating size of result for e.g.
select * from Enrolment
where  course <> 'COMP9315';

Could estimate by using:

 uniform distribution assumption, r, #courses

Alternative, simpler way to estimate:

 assume that most tuples are not equal to chosen value
 # selected tuples = r



... Estimating Selection Result Size99/154

Uniform distribution assumption is convenient, but often not realistic.

How to handle non-uniform attribute value distributions?

 collect statistics about the values stored in the attribute/relation
 store these as e.g. a histogram in the meta-data for the relation

So, for the part colour example, we might have a distribution like:


White: 35%  
Red: 30%  
Blue: 25%  
Silver: 10%


Use histogram as basis for determining # selected tuples.

Disadvantage: cost of storing/maintaining histograms.


Estimating Join Result Size100/154

Analysis is not as simple as select.

Relies on semantic knowledge about data/relations.

Consider equijoin on common attr: R ⋈A S

Case 1: |R ⋈A S| = 0

Useful if we know that dom(A,R) ∩ dom(A,S) = {}

Case 2: A is unique key in both R and S 
⇒ can't be more tuples in join than in smaller
of R and S.


 max(|R ⋈A S|)  =  min(|R|, |S|) 



... Estimating Join Result Size101/154

Case 3: A is primary key in R, foreign key in S 
⇒ every S tuple has at most one matching R tuple.


 max(|R ⋈A S|)  =  |S| 


Example:
select name from Students s, Enrol e
where  e.cid = 'COMP9315' AND e.sid = s.sid


 |Students ⋈ σ9315(Enrol)|  =  |σ9315(Enrol)| 



Cost Estimation: Postscript102/154

Inaccurate cost estimation can lead to poor evaluation plans.

Above methods can (sometimes) give inaccurate estimates.

To get more accurate estimates costs:

 more time ... complex computation of selectivity
 more space ... storage for histograms of data values

Either way, optimisation process costs more
	(more than query?)

Trade-off between optimiser performance and query performance.


Semantic Query Optimisation (SQO)103/154

Makes use of semantics of data to assist query optimisation process.

The discussion of join cost estimation above gives an example of this.

Kinds of information used:

 knowledge about relations
 nature of data
 constraints within/between attributes/relations

SQO uses this to simplify queries and reduce search space.

Two examples of semantic transformation operations:

 restriction elimination,   index introduction



Semantic Equivalence104/154

Semantic equivalence does not require syntactic equivalence.

Consider the two queries:
select * from Emp where sal > 80K

select * from Emp where sal > 80K and job = 'Prof'

They are equivalent under the semantic rule:


Emp.job = 'Prof'  ↔  Emp.sal > 80K


(i.e. Profs are the only people earning more than $80,000)

Could use this to transform query to exploit index on salary.


Restriction Elimination105/154

Query:


List all the departments that store benzene in quantities of more than 400

select dept from  Storage
where material = 'Benzene' and qty > 400

Use rule: material = 'Benzene'  ⇒  qty > 500
select dept from Storage where material = 'Benzene'

Result: Unnecessary restriction on the attribute qty is eliminated.


Index Introduction106/154

Query: Find all the employees who make more than 42K
select name from Employees where salary > 42K

Use rule:  salary > 42K  ⇒  job = 'manager'
select name from Employees
where salary > 42K and job = 'manager'

Result: A new constraint is obtained on the 
indexed attribute job.


Query Execution



Query Execution108/154

Query execution:   applies evaluation plan → set of result tuples





... Query Execution109/154

Query execution

 applies a query execution plan
 and produces a set of result tuples



... Query Execution110/154

Example of query translation:
select s.name, s.id, e.course, e.mark
from   Student s, Enrolment e
where  e.student = s.id and e.semester = '05s2';

maps to


πname,id,course,mark(Stu ⋈e.student=s.id (σsemester=05s2Enr))


maps to
Temp1  = BtreeSelect[semester=05s2](Enr)
Temp2  = HashJoin[e.student=s.id](Stu,Temp1)
Result = Project[name,id,course,mark](Temp2)



... Query Execution111/154

A query execution plan:

 consists of a sequence of operations
 each operation is a relational algebra operator 
	(a specific implementation with particular performance characteristics)

Results may be passed from one operator to the next:

 materialization ... writing results to disk and reading them back
 pipelining ... generating and passing results one-at-a-time



... Query Execution112/154

Two ways of communicating results between query blocks ...

Materialization

 first block writes all results to disk
 next block reads tuples from disk to process
 advantage: can exploit file structures (e.g. hashing)

Pipelining

 blocks execute "concurrently" as producer/consumer pairs
 structured as interacting iterators (open; while(next); close)
 advantage: no requirement for disk access



Materialization113/154

Steps in materialization between two operators

 first operator reads input(s) and writes results to disk
 next operator treats tuples results on disk as its input

Advantage:

 intermediate results can be placed in a file structure 
	(which can be chosen to speed up execution of subsequent operators)

Disadvantage:

 requires disk space/writes for intermediate results
 requires disk access to read intermediate results



... Materialization114/154

Example:
select s.name, s.id, e.course, e.mark
from   Student s, Enrolment e
where  e.student = s.id and
       e.semester = '05s2' and s.name = 'John';

might be executed as
Temp1  = BtreeSelect[semester=05s2](Enrolment)
Temp2  = BtreeSelect[name=John](Student)
         -- indexes on name and semester
	 -- produce sorted Temp1 and Temp2
Temp3  = SortMergeJoin[e.student=s.id](Temp1,Temp2)
         -- SMJoin especially effective, since
         -- Temp1 and Temp2 are already sorted
Result = Project[name,id,course,mark](Temp3)



Pipelining115/154

How pipelining is organised between two operators:

 blocks execute "concurrently" as producer/consumer pairs
 first operator acts as producer; second as consumer
 structured as interacting iterators (open; while(next); close)

Advantage:

 no requirement for disk access
	(results passed via memory buffers)

Disadvantage:

 each operator accesses inputs via linear scan



Iterators (reminder)116/154

Iterators provide a "stream" of results:

 iter = open(params)

 set up data structures for iterator
	  (create state, open files, ...)
 params are specific to operator
	 (e.g. reln, condition, #buffers, ...)

 tuple = next(iter)

 get the next tuple in the iteration; return null if no more

 close(iter)

 clean up data structures for iterator


Other possible operations: reset to specific point, restart, ...


... Iterators (reminder)117/154

Implementation of single-relation selection iterator:
typedef struct {
    File   inf;  // input file
    Cond   cond; // selection condition
    Buffer buf;  // buffer holding current page
    int    curp; // current page during scan
    int    curr; // index of current record in page
} Iterator;

Iterator structure contains information:

 related to operation being performed
	  (e.g. cond)
 information giving current execution state
	  (e.g. curp, curr)



... Iterators (reminder)118/154

Implementation of single-relation selection iterator (cont):
Iterator *open(char *relName, Condition cond) {
    Iterator *iter = malloc(sizeof(Iterator));
    iter->inf  = openFile(fileName(relName),READ);
    iter->cond = cond;
    iter->curp = 0;
    iter->curr = -1;
    readBlock(iter->inf, iter->curp, iter->buf);
    return iter;
}
void close(Iterator *iter) {
    closeFile(iter->inf);
    free(iter);
}



... Iterators (reminder)119/154

Implementation of single-relation selection iterator (cont):
Tuple next(Iterator *iter) {
    Tuple rec;
    do {
        // check if reached end of current page
        if (iter->curr == nRecs(iter->buf)-1) {
            // check if reached end of data file
    	    if (iter->curp == nBlocks(iter->inf)-1)
    	        return NULL;
    	    iter->curp++;
            iter->buf = readBlock(iter->inf, iter->curp);
            iter->curr = -1;
        }
        iter->curr++;
        rec = getRec(iter->buf, iter->curr);
    } while (!matches(rec, iter->cond));
    return rec;
}
// curp and curr hold indexes of most recently read page/record



Pipelining Example120/154

Consider the query:
select s.id, e.course, e.mark
from   Student s, Enrolment e
where  e.student = s.id and
       e.semester = '05s2' and s.name = 'John';

which maps to the RA expression


Proj[id,course,mark](Join[student=id](Sel[05s2](Enr),Sel[John](Stu)))


which could represented by the RA expression tree





... Pipelining Example121/154

Modelled as communication between RA tree nodes:





... Pipelining Example122/154

This query might be executed as
System:
    iter0 = open(Result)
    while (Tup = next(iter0)) { display Tup }
    close(iter0)
Result:
    iter1 = open(Join)
    while (T = next(iter1))
        { T' = project(T); return T' }
    close(iter1)
Sel1:
    iter4 = open(Btree(Enrolment,'semester=05s2'))
    while (A = next(iter4)) { return A }
    close(iter4)
...



... Pipelining Example123/154

...
Join: -- nested-loop join
    iter2 = open(Sel1)
    while (R = next(iter2) {
        iter3 = open(Sel2)
        while (S = next(iter3))
            { if (matches(R,S) return (RS) }
        close(iter3) // better to reset(iter3)
    }
    close(iter2)
Sel2:
    iter5 = open(Btree(Student,'name=John'))
    while (B = next(iter5)) { return B }
    close(iter5)



Pipeline Execution124/154

Piplines can be executed as ...

Demand-driven

 producers wait until consumers request tuples

Producer-driven

 producers generate tuples until output buffer full, then wait

In both cases, top-level driver is request for result tuples.

In parallel-processing systems, iterators could run concurrently.


Disk Accesses125/154

Pipelining cannot avoid all disk accesses.

Some operations use multiple passes (e.g. merge-sort, hash-join).

 data is written by one pass, read by subsequent passes

Thus ...

 within an operation, disk reads/writes are possible
 between operations, no disk reads/writes are needed



... Disk Accesses126/154

Sophisticated query optimisers might realise e.g.


if operation X writes its results to a file with structure S, 
the subsequent operation Y will proceed much faster 
than if Y reads X's output tuple-at-a-time


In this case, it could materialize X's output in an S-file.

Produces a pipeline/materialization hybrid query execution.

Example:

 selection writes output into an indexed file (Btree)
 later join can then be implemented as efficient index-join



... Disk Accesses127/154

Example: (pipeline/materialization hybrid)
select s.id, e.course, e.mark
from   Student s, Enrolment e
where  e.student = s.id and
       e.semester = '05s2' and s.name = 'John';

might be executed as
System:
    exec(Sel2)  -- creates Temp1
    iter0 = open(Result)
    while (Tup = next(iter0)) { display Tup }
    close(iter0)
Result:
    iter1 = open(Join)
    while (T = next(iter1))
        { T' = project(T); return T' }
    close(iter1)
...



... Disk Accesses128/154

...
Join: -- index join
    iter2 = open(Sel1)
    while (R = next(iter2) {
        iter3 = open(Btree(Temp1,'id=R.student'))
        while (S = next(iter3)) { return (RS) }
        close(iter3)
    }
    close(iter2)
Sel1:
    iter4 = open(Btree(Enrolment,'semester=05s2'))
    while (A = next(iter4)) { return A }
    close(iter4)
Sel2:
    iter5 = open(Btree(Student,'name=John'))
    createBtree(Temp1,'id')
    while (B = next(iter5)) { insert(B,Temp1) }
    close(iter5)



PostgreSQL Execution129/154

Defs: src/include/executor and src/include/nodes

Code: src/backend/executor

PostgreSQL uses pipelining ...

 query plan is a tree of Plan nodes
 each type of node implements one kind of RA operation 
	(node implements specific access methods and provides iterator interface)
 node types e.g. Scan, Group, Indexscan, Sort, HashJoin
 execution is managed via a tree of PlanState nodes 
	(mirrors the structure of the tree of Plan nodes; holds execution state)



... PostgreSQL Execution130/154

Modules in src/backend/executor fall into two groups:

execXXX (e.g. execMain, execProcnode, execScan)

 implement generic control of plan evaluation (execution)
 provide overall plan execution and dispatch to node iterators

nodeXXX  
	(e.g. nodeSeqscan, nodeNestloop, nodeGroup)

 implement iterators for specific types of RA operators
 typically contains ExecInitXXX, ExecXXX, ExecEndXXX


The "style" is OO (e.g. specialisations of Nodes),
but implementation in C masks this



... PostgreSQL Execution131/154

Top-level data/functions for executor ...

QueryDesc

 contains plan and state information
	(e.g. pointer to root of plan tree)

ExecutorStart(QueryDesc *, ...)

 initialises all dynamic state information
	(e.g. iterators)

ExecutorRun(QueryDesc *, ScanDirection, ...)

 implements "get next result tuple" (via iterator tree)

ExecutorEnd(QueryDesc *)

 cleans up all iterator and state information



... PostgreSQL Execution132/154

Overview of query processing:
CreateQueryDesc

ExecutorStart
    CreateExecutorState -- creates per-query context
    switch to per-query context to run ExecInitNode
    ExecInitNode -- recursively scans plan tree
        CreateExprContext -- creates per-tuple context
        ExecInitExpr

ExecutorRun
    ExecutePlan -- invoke iterators from root
        ExecProcNode -- recursively called in per-query context
            ExecEvalExpr -- called in per-tuple context
            ResetExprContext -- to free memory

ExecutorEnd
    ExecEndNode -- recursively releases resources
    FreeExecutorState -- frees per-query and child contexts

FreeQueryDesc



... PostgreSQL Execution133/154

More detailed view of plan execution (but still much simplified)
ExecutePlan(execState, planStateNode, ...) {
    process "before each statement" triggers
    for (;;) {
        tuple = ExecProcNode(planStateNode)
        check tuple validity // MVCC
        if (got a tuple) break
    }
    process "after each statement" triggers
    return tuple
}
ExecProcNode(node) {
    switch (nodeType(node)) {
    case SeqScan:
        result = ExecSeqScan(node); break;
    case NestLoop:
        result = ExecNestLoop(node); break;
    ...
    }
    return result;
}



... PostgreSQL Execution134/154

Generic iterator interface is provided by ...

ExecInitNode

 initialize a plan node and its subplans

ExecProcNode

 get a tuple by executing the plan node

ExecEndNode

 shut down a plan node and its subplans

Each calls corresponding function for specific node type 
(e.g. for nested loop join ExecInitNestLoop(), ExecNestLoop(), ExecEndNestLoop())


Example PostgreSQL Execution135/154

Consider the query:
-- get manager's age and # employees in Shoe department
select e.age, d.nemps
from   Departments d, Employees e
where  e.name = d.manager and d.name ='Shoe'

and its execution plan tree





... Example PostgreSQL Execution136/154

This produces a tree with three nodes:

 NestedLoop with join condition (Outer.manager = Inner.name)
 IndexScan on Departments with selection (name = 'Shoe')
 SeqScan on Employees

We ignore the top-level node here 
	(it handles the projection via attrList)


... Example PostgreSQL Execution137/154

Initially InitPlan() invokes ExecInitNode() on plan tree root.

ExecInitNode() sees a NestedLoop node ... 
    so dispatches to ExecInitNestLoop() to set up iterator
    and then invokes ExecInitNode() on left and right sub-plans
        in left subPlan, ExecInitNode() sees an IndexScan node
            so dispatches to ExecInitIndexScan() to set up iterator
        in right sub-plan, ExecInitNode() sees aSeqScan node
            so dispatches to ExecInitSeqScan() to set up iterator

Result: a plan state tree with same structure as plan tree.


... Example PostgreSQL Execution138/154

Execution: ExecutePlan() repeatedly invokes ExecProcNode().

ExecProcNode() sees a NestedLoop node ...
    so dispatches to ExecNestedLoop() to get next tuple
    which invokes ExecProcNode() on its sub-plans
        in the left sub-plan, ExecProcNode() sees an IndexScan node
            so dispatches to ExecIndexScan() to get next tuple
            if no more tuples, return END
            for this tuple, invoke ExecProcNode() on right sub-plan
                ExecProcNode() sees a SeqScan node
                    so dispatches to ExecSeqScan() to get next tuple
                check for match and return joined tuples if found
            reset right sub-plan iterator

Result: stream of result tuples returned via ExecutePlan()


Performance Tuning



Performance Tuning140/154

Schema design:

 devise data structures to represent application information

Performance tuning:

 devise data structures to achieve good performance

Good performance may involve any/all of:

 making applications run faster
 lowering response time of queries/transactions
 improving overall transaction throughput



... Performance Tuning141/154

Tuning requires us to consider the following:

 which queries and transactions will be used? 
	  (e.g. check balance for payment, display recent transaction history)
 how frequently does each query/transaction occur? 
	  (e.g. 90% withdrawals; 10% deposits; 50% balance check)
 are there time constraints on queries/transactions? 
	  (e.g. EFTPOS payments must be approved within 7 seconds)
 are there uniqueness constraints on any attributes? 
	  (define indexes on attributes to speed up insertion uniqueness check)
 how frequently do updates occur? 
	  (indexes slow down updates, because must update table and index)



... Performance Tuning142/154

Performance can be considered at two times:

 during schema design

 typically towards the end of schema design process
 requires schema transformations such as denormalisation

 outside schema design

 typically after application has been deployed/used
 requires adding/modifying data structures such as indexes




Denormalisation143/154

Normalisation minimises storage redundancy.

 achieves this by "breaking up" data into logical chunks
 requires minimal "maintenance" to ensure consistency

Problem: queries that need to put data back together.

 need to use a (potentially expensive) join operation
 if an expensive join is frequent, performance suffers

Solution: store some data redundantly

 benefit: queries no longer need expensive joins
 trade-off: extra maintenance effort to keep consistency
 worthwhile if joins are frequent and updates are rare



... Denormalisation144/154

Example ... consider the following normalised schema:
create table Subject (
   id        serial primary key,
   code      char(8), -- e.g. COMP9315
   title     varchar(60),
   syllabus  text, ... );
create table Term (
   id        serial primary key,
   name      char(4), -- e.g. 09s2
   starting  date, ... );
create table Course (
   subject   integer references Subject(id),
   term      integer references Term(id),
   lic       integer references Staff(id), ... );



... Denormalisation145/154

Example: Courses = Course ⋈ Subject ⋈ Term

If we often need to refer to "standard" name (e.g. COMP9315 09s2)

 add extra courseName column into Course table
 cost: trigger before insert on Course to construct name
 trade-off likely to be worthwhile: Course insertions infrequent


-- can now replace a query like:
select s.code||' '||t.name, avg(e.mark)
from   Course c, Subject s, Term t
where  c.subject = s.id and c.term = t.id
       and s.code='COMP9315' and t.name='09s2'
-- by a query like:
select c.courseName, e.grade, e.mark
from   Course c
where  c.courseName = 'COMP9315 09s2'



Indexes146/154

Indexes provide efficient content-based access to tuples.

Can build indexes on any (combination of) attributes.

Definining indexes:
CREATE INDEX name ON table ( attr1, attr2, ... )

attri can be an arbitrary expression (e.g. upper(name)).

CREATE INDEX also allows us to specify

 that the index is on UNIQUE values
 an access method (USING btree, hash, gist, gin)



... Indexes147/154

Indexes can significantly improve query costs.

Considerations in applying indexes:

 is an attribute used in frequent/expensive queries? 
	  (note that some kinds of queries can be answered from index alone)
 should we create an index on a collection of attributes? 
	  (yes, if the collection is used in a frequent/expensive query)
 is the table containing attribute frequently updated?
 should we use B-tree or Hash index?

-- use hashing for (unique) attributes in equality tests, e.g.
select * from Employee where id = 12345
-- use B-tree for attributes in range tests, e.g.
select * from Employee where age > 60




Query Tuning148/154

Sometimes, a query can be re-phrased to affect performance:

 by helping the optimiser to make use of indexes
 by avoiding unnecessary/expensive operations

Examples which may prevent optimiser from using indexes:
select name from Employee where salary/365 > 100
       -- fix by re-phrasing condition to (salary > 36500)
select name from Employee where name like '%ith%'
select name from Employee where birthday is null
       -- above two are difficult to "fix"
select name from Employee
where  dept in (select id from Dept where ...)
       -- fix by using Employee join Dept on (e.dept=d.id)



... Query Tuning149/154

Other factors to consider in query tuning:

 select distinct requires a sort; is distinct necessary?
 if multiple join conditions are available ... 
	choose join attributes that are indexed, avoid joins on strings

select ... Employee join Customer on (s.name = p.name)
vs
select ... Employee join Customer on (s.ssn = p.ssn)

 sometimes or in condition prevents index from being used ... 
	replace the or condition by a union of non-or clauses

select name from Employee where dept=1 or dept=2
vs
(select name from Employee where dept=1)
union
(select name from Employee where dept=2)




PostgreSQL Query Tuning150/154

PostgreSQL provides the explain statement to

 give a representation of the query execution plan
 with information that may help to tune query performance

Usage:
EXPLAIN [ANALYZE] Query

Without ANALYZE, EXPLAIN shows plan with estimated costs.

With ANALYZE, EXPLAIN executes query and prints real costs.


Note that runtimes may show considerable variation due to buffering.



EXPLAIN Examples151/154

Example: Select on indexed attribute

ass2=# explain select * from Students where id=100250;
                            QUERY PLAN
-----------------------------------------------------------------
 Index Scan using student_pkey on student
                  (cost=0.00..5.94 rows=1 width=17)
   Index Cond: (id = 100250)

ass2=# explain analyze select * from Students where id=100250;
                            QUERY PLAN
-----------------------------------------------------------------
 Index Scan using student_pkey on student
                  (cost=0.00..5.94 rows=1 width=17)
                  (actual time=31.209..31.212 rows=1 loops=1)
   Index Cond: (id = 100250)
 Total runtime: 31.252 ms




... EXPLAIN Examples152/154

Example: Select on non-indexed attribute

ass2=# explain select * from Students where stype='local';
                        QUERY PLAN
----------------------------------------------------------
 Seq Scan on student  (cost=0.00..70.33 rows=18 width=17)
   Filter: ((stype)::text = 'local'::text)

ass2=# explain analyze select * from Students
ass2-#                          where stype='local';
                         QUERY PLAN
---------------------------------------------------------------
 Seq Scan on student  (cost=0.00..70.33 rows=18 width=17)
             (actual time=0.061..4.784 rows=2512 loops=1)
   Filter: ((stype)::text = 'local'::text)
 Total runtime: 7.554 ms



... EXPLAIN Examples153/154

Example: Join on a primary key (indexed) attribute

ass2=# explain
ass2-# select s.sid,p.name
ass2-# from Students s, People p where s.id=p.id;
                        QUERY PLAN
-----------------------------------------------------------
 Hash Join  (cost=70.33..305.86 rows=3626 width=52)
   Hash Cond: ("outer".id = "inner".id)
   -> Seq Scan on person p
               (cost=0.00..153.01 rows=3701 width=52)
   -> Hash  (cost=61.26..61.26 rows=3626 width=8)
       -> Seq Scan on student s
                   (cost=0.00..61.26 rows=3626 width=8)



... EXPLAIN Examples154/154

Example: Join on a non-indexed attribute

ass3=> explain select s1.code, s2.code
ass2-# from Subjects s1, Subjects s2 where s1.offerer=s2.offerer;
                        QUERY PLAN
----------------------------------------------------------------
 Merge Join  (cost=2744.12..18397.14 rows=1100342 width=18)
   Merge Cond: (s1.offerer = s2.offerer)
   ->  Sort  (cost=1372.06..1398.33 rows=10509 width=13)
         Sort Key: s1.offerer
         ->  Seq Scan on subjects s1
                      (cost=0.00..670.09 rows=10509 width=13)
   ->  Sort  (cost=1372.06..1398.33 rows=10509 width=13)
         Sort Key: s2.offerer
         ->  Seq Scan on subjects s2
                      (cost=0.00..670.09 rows=10509 width=13)
(8 rows)


Produced: 15 May 2016</p><h3 >字段9</h3><p>Transaction Processing


Transaction Processing1/146

Where transaction processing fits in the DBMS:





Transactions2/146

A transaction is:

 a unit of processing corresponding to a DB state-change

Transactions occur naturally in context of DB applications, e.g.

 booking an airline or concert ticket
 transferring funds between bank accounts
 updating stock levels via point-of-sale terminal
 enrolling in a course or class

In order to achieve satisfactory performance (throughput):

 DBMSs allow multiple transactions to execute concurrently


(Notation: we use the abbreviation "tx" as a synonym for "transaction")



... Transactions3/146

A transaction

 represents an operational unit at the application level
 but typically comprises multiple operations in the DBMS

E.g.   select ... update ... insert ... select ... insert ...

A transaction can end in two possible ways:

 commit ... effects of all DBMS operations in tx are visible
 abort ... effects of no DBMS operations in tx are visible

Above is essentially the atomicity requirement in ACID (see later).


... Transactions4/146

A transaction is a DB state-change operation.





Assume that the code of the transaction 

 is correct with respect to its own specification
 performs a mapping that maintains all DB constraints

Above is essentially the consistency requirement in ACID (see later).


... Transactions5/146

Transactions execute on a collection of data that is

 shared - concurrent access by multiple users
 unstable - potential for hardware/software failure

Transactions need an environment that is

 unshared - their work is not inadvertantly affected by others
 stable - their updates survive even in the face of system failure

Goal: data integrity should be maintained at all times
	(for each tx)


... Transactions6/146

If a transaction commits, must ensure that

 effects of all operations persist permanently
 changes are visible to all subsequent transactions

Part-way through a transaction, must ensure that

 other txs don't see results of partly-complete computations

If a transaction aborts, must ensure that

 effects of any DB operations are "cleaned up" (rolled-back)

If there is a system failure, must ensure that on restart

 the database is restored to a consistent state
 all partly-complete transactions are rolled-back



Concurrency in DBMSs7/146

Concurrency in a multi-user DBMS like PostgreSQL:





ACID Properties8/146

Data integrity is assured if transactions satisfy the following:

Atomicity 


 Either all operations of a transaction are reflected in database
or none are.


Consistency 


 Execution of a transaction in isolation preserves data consistency.


Isolation 


 Each transaction is "unaware" of other transactions executing
concurrently.


Durability 


 If a transaction commits, its changes persist even after later system failure.




... ACID Properties9/146

Atomicity can be represented by state-transitions:




COMMIT ⇒ all changes preserved,
	  ABORT ⇒ database unchanged


... ACID Properties10/146

Transaction processing:

 the study of techniques for realising ACID properties

Consistency is the property mentioned earlier:

 a tx is correct with respect to its own specification
 a tx performs a mapping that maintains all DB constraints

Ensuring this must be left to application programmers.

Our discussion thus focusses on:
	Atomicity, Durability, Isolation


... ACID Properties11/146

Atomicity is handled by the commit and abort mechanisms

 commit ends tx and ensures all changes are saved
 abort ends tx and undoes changes already made


Durability is handled by implementing stable storage, via

 redundancy, to deal with hardware failures
 logging/checkpoint mechanisms, to recover state

Isolation is handled by concurrency control mechanisms

 two possibilities: lock-based, timestamp-based
 various levels of isolation are possible (e.g. serializable)



Review of Transaction Terminology12/146

To describe transaction effects, we consider:

 READ - transfer data from disk to memory
 WRITE - transfer data from memory to disk
 ABORT - terminate transaction, unsuccessfully
 COMMIT - terminate transaction, successfully

Relationship between the above operations and SQL:

 SELECT produces READ operations on the database
 UPDATE and DELETE produce READ then WRITE operations
 INSERT produces WRITE operations



... Review of Transaction Terminology13/146

More on transactions and SQL

 BEGIN starts a transaction


 the begin keyword in PLpgSQL is not the same thing


 COMMIT commits and ends the current transaction


 some DBMSs e.g. PostgreSQL also provide END as a synonym
 the end keyword in PLpgSQL is not the same thing


 ROLLBACK aborts the current transaction, undoing any changes


 some DBMSs e.g. PostgreSQL also provide ABORT as a synonym




In PostgreSQL, tx's cannot be defined inside stored procedures (e.g. PLpgSQL)



... Review of Transaction Terminology14/146

The READ, WRITE, ABORT, COMMIT operations:

 occur in the context of some transaction T
 involve manipulation of data items X, Y, ...
	  (READ and WRITE)

The operations are typically denoted as:


 RT(X) 
 read item X in transaction T 


 WT(X) 
 write item X in transaction T 


 AT 
 abort transaction T 


 CT 
 commit transaction T 




Schedules15/146

A schedule gives the sequence of operations that occur

 when a set of tx's T1 .. Tnrun concurrently
 operations from invidual Ti's are interleaved

E.g.    RT1(A)   RT2(B)   W T1(A)   WT3(C)   RT2(A)   WT3(B)   ... 

For a single tx, there is a single schedule
	  (its operations in order).

For a group of tx's, there are very many possible schedules.


... Schedules16/146

Consider a simple banking transaction, expressed in "PLpgSQL":

create function
   withdraw(Acct integer, Required float) returns text
as $$
declare
   Bal float;
begin
   select balance into Bal from Accounts where id=Acct;  R
   Bal := Bal - Required;
   update Accounts set balance=Bal where id=Acct;        W
   if (Bal < 0) then
      rollback; return 'Insufficient funds';             A
   else
      commit; return 'New balance: '||Bal::text;         C
   end if;
end;
$$ language plpgsql;


Notes:

 a better way to implement this would be to check before updating
 you can't embed tx-type commands inside PLpgSQL functions
 the begin at the start of the function does not begin a tx




... Schedules17/146

If tx T = withdraw(A,R)
it has two possible schedules

 Fail:    RT(A)   WT(A)   AT
 OK:    RT(A)   WT(A)   CT


If tx T = withdraw(A,R1)
and tx S = withdraw(B,R2) 
some possible schedules are:

 RT(A)   WT(A)   AT   RS(B)   WS(B)   CS    (serial schedule)
 RT(A)   WT(A)   RS(B)   WS(B)   AT   CS
 RT(A)   RS(B)   WS(B)   WT(A)   AT   CS
 RS(B)   WS(B)   CS   RT(A)   WT(A)   AT    (serial schedule)



... Schedules18/146

Serial schedules have no interleave of operations from different tx's.

Why serial schedules are good:

 each transaction is correct (consistency) 
	(leaves the database in a consistent state if run to completion individually)
 the database starts in a consistent state
 the first transaction completes, leaving the DB consistent
 the next transaction completes, leaving the DB consistent

As would occur e.g. in a single-user database system.


... Schedules19/146

With different-ordered serial executions, tx's may get different results.

I.e. StateAfter(T1;T2) = StateAfter(T2;T1) is not generally true.

Consider the following two transactions:
T1 : select sum(salary)
     from Employee where dept='Sales'

T2 : insert into Employee
     values (....,'Sales',...)

If we execute T1 then T2
we get a smaller salary total
than if we execute T2 then T1.

In both cases, however, the salary total is consistent with the state
of the database at the time the transaction is executed.


... Schedules20/146

A serial execution of consistent transactions is always consistent.

If transactions execute under a concurrent (nonserial) schedule,
the potential exists for conflict among their effects.

In the worst case, the effect of executing the transactions ...

 is to leave the database in an inconsistent state
 even though each transaction, by itself, is consistent

So why don't we observe such problems in real DBMSs? ...

 concurrency control mechanisms handle them
	  (see later).



... Schedules21/146

Not all concurrent executions cause problems.

For example, the schedules
T1: R(X) W(X)           R(Y) W(Y)
T2:           R(X) W(X)

or
T1: R(X) W(X)      R(Y)      W(Y)
T2:           R(X)      W(X)

or ...

leave the database in a consistent state.


Example Transaction #122/146

Problem: Allocate a seat on a plane flight

Implement as a function returning success status:
function allocSeat(paxID    integer,
                   flightID integer,
                   seatNum  string)
         returning boolean
{
    check whether seat currently occupied
    if (already occupied)
        tell pax seat taken; return !ok
    else
        assign pax to seat; return ok
}

Assume schema:

Flight(flightID, flightNum, flightTime, ...)
SeatingAlloc(flightID, seatNum, paxID, ...)



... Example Transaction #123/146

PLpgSQL implementation for seat allocation:

create or replace function
    allocSeat(paxID int, fltID int, seat text) returns boolean
as $$
declare
    pid int;
begin
    select paxID into pid from SeatingAlloc
    where  flightID = fltID and seatNum = seat;
    if (pid is not null) then
        return false;  -- someone else already has seat
    else
        update SeatingAlloc set pax = paxID
        where  flightID = fltID and seatNum = seat;
        commit;
        return true;
    end if;
end;
$$ langauge plpgsql;



... Example Transaction #124/146

If customer Cust1 executes allocSeat(Cust1,f,23B)

 Cust1 sees that seat 23B is available
 Cust1 allocates seat 23B for themselves

If customer Cust2 then executes allocSeat(Cust2,f,23B)

 Cust2 sees that seat 23B is already booked
 allocSeat() returns with failure; no change to DB

The system behaves as required ⇒ tx is consistent.


... Example Transaction #125/146

Consider two customers trying allocSeat(?,f,23B) simultaneously.

A possible order of operations ...

 Cust1 sees that seat 23B is available
 Cust2 sees that seat 23B is available
 Cust1 allocates seat 23B for themselves
 Cust2 allocates seat 23B for themselves

Cause of problem: unfortunate interleaving of operations within concurrent transactions.

Serial execution (e.g. enforced by locks) could solve these kinds
of problem.


Example Transaction #226/146

Problem: transfer funds between two accounts in same bank.

Implement as a function returning success status:
function transfer(sourceAcct integer,
                  destAcct   integer,
                  amount     real)
         returning boolean
{
    check whether sourceAcct is valid
    check whether destAcct is valid
    check whether sourceAcct has
          sufficient funds (>= amount)
    if (all ok) {
        withdraw money from sourceAcct
        deposit money into destAcct
}   }



... Example Transaction #227/146

PLpgSQL for funds transfer between accounts:

create or replace function
    transfer(source int, dest int, amount float)
    returns boolean
as $$
declare
    ok   boolean := true;
    acct Accounts%rowtype;
begin
    select * into acct
    from   Accounts where id=source;
    if (not found) then
        raise warning 'Invalid Withdrawal Account';
        ok := false;
    end if;
    select * from Accounts where id=dest;
    if (not found) then
        raise warning 'Invalid Deposit Account';
        ok := false;
    end if;
    if (acct.balance < amount) then
        raise warning 'Insufficient funds';
        ok := false;
    end if;
    if (not ok) then rollback; return false; end if;
    update Accounts
    set    balance := balance - amount
    where  id = source;
    update Accounts
    set    balance := balance + amount
    where  id = dest;
    commit;
    return true;
end;
$$ language plpgsql;



... Example Transaction #228/146

If customer transfers $1000 from Acct1 to Acct2

 Acct1 and Acct2 both exist; Acct1 has > $1000
 remove $1000 from Acct1; add $1000 to Acct2

But if Cust1 and Cust2 both transfer from Acct1 together

 all accounts exist, and Acct1 contains $1500
 Cust1 checks Acct1; has sufficient funds
 Cust2 checks Acct1; has sufficient funds
 Cust1 removes money from Acct1; adds to Acct2
 Cust2 removes money from Acct1; adds to Acct2

But account ran out of money after Cust1 took their cash.

Similar to earlier problem; could be fixed by serialization.


... Example Transaction #229/146

Consider customer transfers $1000 from Acct1 to Acct2

 Acct1 and Acct2 both exist; Acct1 has > $1000
 remove $1000 from Acct1; then system failure

If transactions are not atomic/durable:

 money removed from Acct1, no money added to Acct2
 customer is not happy



Transaction Anomalies30/146

What problems can occur with concurrent transactions?

The set of phenomena can be characterised broadly under:

 dirty read: 
	reading data item currently in use by another tx
 nonrepeateable read: 
	re-reading data item, since changed by another tx
 phantom read: 
	re-reading result set, since changed by another tx



... Transaction Anomalies31/146

Dirty read: a transaction reads data written by a
concurrent uncommitted transaction

Example:
     Transaction T1       Transaction T2
(1)  select a into X
     from R where id=1
(2)                       select a into Y
                          from R where id=1
(3)  update R set a=X+1
     where id=1
(4)  commit
(5)                       update R set a=Y+1
                          where id=1
(6)                       commit

Effect: T1's update on R.a is lost.


... Transaction Anomalies32/146

Nonrepeatable read: 
a transaction re-reads data it has previously read and finds that
data has been modified by another transaction
(that committed since the initial read)

Example:
     Transaction T1    Transaction T2
(1)  select * from R
     where id=5
(2)                    update R set a=8
                       where id=5
(3)                    commit
(4)  select * from R
     where id=5

Effect: T1 runs same query twice; sees different data


... Transaction Anomalies33/146

Phantom read: 
a transaction re-executes a query returning a set of rows that
satisfy a search condition and finds that the set of rows
satisfying the condition has changed due to
another recently-committed transaction

Example:
     Transaction T1    Transaction T2
(1)  select count(*)
     from R where a=5
(2)                    insert into R(id,a,b)
                              values (2,5,8)
(3)                    commit
(4)  select count(*)
     from R where a=5

Effect: T1 runs same query twice; sees different result set


Example of Transaction Failure34/146

The above examples generally assumed that transactions committed.

Additional problems can arise when transactions abort.

We give examples using the following two transactions:
T1: read(X)           T2: read(X)
    X := X + N            X := X + M
    write(X)              write(X)
    read(Y)
    Y := Y - N
    write(Y)

and initial DB state X=100, Y=50, N=5, M=8.


... Example of Transaction Failure35/146

Consider the following schedule where one transaction fails:
T1: R(X) W(X) A
T2:             R(X) W(X)

Transaction T1 aborts after writing X.

The abort will rollback the changes to X,
but where the undo occurs can affect the results.

Consider three places where rollback might occur:
T1: R(X) W(X) A [1]     [2]     [3]
T2:                 R(X)    W(X)



Transaction Failure - Case 136/146

This scenario is ok.   T1's effects have been eliminated.

Database   Transaction T1       Transaction T2
---------  ------------------   --------------
X    Y               X    Y               X
100  50              ?    ?               ?
           read(X)   100
           X:=X+N    105
105        write(X)
           abort
100        rollback
                                read(X)   100
                                X:=X+M    108
108                             write(X)
---------
108  50



Transaction Failure - Case 237/146

In this scenario, some of T1's effects have been retained.

Database   Transaction T1       Transaction T2
---------  ------------------   --------------
X    Y               X    Y               X
100  50              ?    ?               ?
           read(X)   100
           X:=X+N    105
105        write(X)
           abort
                                read(X)   105
                                X:=X+M    113
100        rollback
113                             write(X)
---------
113  50



Transaction Failure - Case 338/146

In this scenario, T2's effects have been lost, even after commit.

Database   Transaction T1       Transaction T2
---------  ------------------   --------------
X    Y               X    Y               X
100  50              ?    ?               ?
           read(X)   100
           X:=X+N    105
105        write(X)
           abort
                                read(X)   105
                                X:=X+M    113
113                             write(X)
100        rollback
---------
100  50



Transaction Isolation



Transaction Isolation40/146

If transactions always run "single-user":

 they are easier to code 
	(programmers concentrate on getting application semantics correct)
 there is no danger of "interference" 
	(correct transactions won't interact to produce an incorrect result)

Simplest form of isolation: serial execution
	  (T1 ; T2 ; T3 ; ...)

 transaction T1 maps DB from valid state D0 to valid state D1
 transaction T2 maps DB from valid state D1 to valid state D2
 etc. etc.



... Transaction Isolation41/146

In practice, serial execution gives poor performance.

We need approaches that allow "safe" concurrency

 to improve overall system performance (throughput)
 to avoid the concurrency problems mentioned earlier

The remainder of this discussion involves

 what, exactly, do we mean by "safe" concurrency?
 DBMS mechanisms that can achieve it in practice



DBMS Transaction Management42/146

Abstract view of DBMS concurrency mechanisms:



The Scheduler

 collects arbitrarily interleaved requests from tx's
 orders their execution to avoid concurrency problems



Serializability43/146

Consider two schedules S1 and S2 produced by

 executing the same set of transactions T1..Tn concurrently
 but with a different interleaving of R/W operations

S1 and S2 are equivalent if

 StateAfter(S1)  =  StateAfter(S2) 
(i.e. the final state yielded by S1 is the same as the final state yielded by S2)

A schedule S for a set of concurrent tx's
	T1 ..Tn is serializable if

 S is equivalent to some serial schedule Ss of T1 ..Tn

Under these circumstances, consistency is guaranteed.


... Serializability44/146

Two formulations of serializability:

 conflict serializibility

 i.e. conflicting read/write operations occur in the "right" order
 checked by looking for absence of cycles in precedence graph

 view serializibility

 i.e. read operations see the correct version of data
 checked via VS conditions on likely equivalent schedules


View serializability is strictly weaker than conflict serializability.

i.e. there are VS schedules that are not CS, but not vice versa


Isolation and Concurrency Control45/146

It is not useful to

 execute a set of tx's T1 .. Tn to obtain a schedule
 then check whether the schedule produced was serializable

The goal is to devise concurrency control schemes

 which arrange the sequence of actions from T1 .. Tn
 such that we obtain a schedule equivalent to some serial schedule

Serializability tests are used in proving properties of these schemes.


Other Properties of Schedules46/146

Above discussion explicitly assumes that all transactions commit.

What happens if some transaction aborts?

Under serial execution, there is no problem:

 use the log to undo any changes made by Ti
 database is reset to valid state before next Tj starts

With concurrent execution, there may be problems:

 unfortunate interactions with the behaviour of redo/undo log
 inability to recover, even though all schedules are serializable



Recoverability47/146

Consider the serializable schedule:
T1:        R(X)  W(Y)  C
T2:  W(X)                 A

(where the final value of Y is dependent in the X value)

Notes:

 the final value of X is valid (change from T2 rolled back)
 T1 reads/uses an X value that is eventually rolled-back
 even though T2 is correctly aborted, it has produced an effect

The schedule produces an invalid database state, even though serializable.


... Recoverability48/146

Recoverable schedules avoid these kinds of problems.

For a schedule to be recoverable, we require additional constraints

 all tx's Ti that wrote values used by Tj
 must have committed before Tj commits

and this property must hold for all transactions Tj

Note that recoverability does not prevent "dirty reads".

In order to make schedules recoverable in the presence of dirty reads and
aborts, may need to abort multiple transactions.


Cascading Aborts49/146

Recall the earlier non-recoverable schedule:
T1:        R(X)  W(Y)  C
T2:  W(X)                 A

To make it recoverable requires:

 delaying T1's commit until T2 commits
 if T2 aborts, cannot allow T1 to commit

T1:        R(X)  W(Y)  ...   A
T2:  W(X)                 A

Known as cascading aborts (or cascading rollback).


... Cascading Aborts50/146

Example: T3 aborts, causing T2 to abort, causing T1 to abort
T1:                    R(Y)  W(Z)        A
T2:        R(X)  W(Y)                 A
T3:  W(X)                          A

Even though T1 has no direct connection with T3
(i.e. no shared data).

This kind of problem ...

 can potentially affect very many concurrent transactions
 could have a significant impact on system throughput



... Cascading Aborts51/146

Cascading aborts can be avoided if

 transactions can only read values written by committed transactions


(alternative formulation: no tx can read data items written by an uncommitted tx)


Effectively: eliminate the possibility of reading dirty data.

Downside: reduces opportunity for concurrency.

GUW call these ACR (avoid cascading rollback) schedules.

All ACR schedules are also recoverable.


Strictness52/146

Strict schedules also eliminate the chance of writing dirty data.

A schedule is strict if

 no tx can read values written by another uncommitted tx   (ACR)
 no tx can write a data item written by another uncommitted tx

Strict schedules simplify the task of rolling back after aborts.


... Strictness53/146

Example: non-strict schedule
T1:  W(X)        A
T2:        W(X)     A

Problems with handling rollback after aborts:

 when T1 aborts, don't rollback
	  (need to retain value written by T2)
 when T2 aborts, need to rollback to pre-T1
	  (not just pre-T2)



Schedule Properties54/146

Relationship between various classes of schedules:





Schedules ought to be serializable and strict.

But more serializable/strict ⇒ less concurrency.

DBMSs allow users to trade off "safety" against performance.


Transaction Isolation Levels55/146

Previous examples showed:

 if we allow uncontrolled concurrent access to shared data
 the consistency of database may be compromised
 even though individual transactions are consistent

Safest approach ...

 force serial equivalent execution by appropriate locking
 but this costs performance (less concurrency)

Other approaches are weaker ...

 may allow schedules that are not safe
 but provide more opportunity for concurrency

Is a trade-off useful?


... Transaction Isolation Levels56/146

In many real applications, there is either

 no chance for conflict between concurrent transactions 
	(e.g. they act on different data or they don't modify the data)
 only a small chance for conflict between transactions 
	(in which case it is feasible to abort one and then re-execute it)

This leads to a trade-off between performance and isolation

 determined by programmer on application-by-application basis
 if low concurrency problems, less isolation, better performance
 if significant potential for concurrency problems, more isolation



... Transaction Isolation Levels57/146

SQL provides a mechanism for database programmers to specify
how much isolation to apply to transactions
set transaction
    read only  -- so weaker isolation may be ok
    read write -- suggests stronger isolation needed
isolation level
    -- weakest isolation, maximum concurrency
    read uncommitted
    read committed
    repeatable read
    serializable
    -- strongest isolation, minimum concurrency



... Transaction Isolation Levels58/146

Meaning of transaction isolation levels:

Isolation          Dirty          Nonrepeatable   Phantom
Level              Read           Read            Read

Read uncommitted   Possible       Possible        Possible
 
Read committed     Not possible   Possible        Possible

Repeatable read    Not possible   Not possible    Possible

Serializable       Not possible   Not possible    Not possible



... Transaction Isolation Levels59/146

For transaction isolation, PostgreSQL

 provides syntax for all four levels
 treats read uncommitted as read committed
 repeatable read behaves like serializable
 default level is read committed

Note: cannot implement read uncommitted because of MVCC


... Transaction Isolation Levels60/146

A PostgreSQL tx consists of a sequence of SQL statements:
BEGIN S1; S2; ... Sn; COMMIT;

Isolation levels affect view of DB provided to each Si:

 in read committed ...

 each Si sees snapshot of DB at start of Si

 in repeatable read and serializable ...

 each Si sees snapshot of DB at start of tx
 serializable checks for extra conditions




... Transaction Isolation Levels61/146

Using PostgreSQL's serializable isolation level, a select:

 sees only data committed before the transaction began
 never sees changes made by concurrent transactions 

Using the serializable isolation level, an update fails:

 if it tries to modify an "active" data item 
(active = affected by some other transaction, either committed or uncommitted)

The transaction containing the update must then rollback and re-start.


Implementing Concurrency Control



Concurrency Control63/146

Approaches to concurrency control:

 Lock-based

Synchronise transaction execution via locks on relevant part of DB.
 Version-based

Allow multiple consistent versions of the data to exist. 
Each transaction has exclusive access to one version (the one when tx started).
 Validation-based   (optimistic concurrency control)

Execute all transactions; check for validity problems just before commit.
 Timestamp-based

Organise transaction execution via timestamps assigned to actions.



Lock-based Concurrency Control64/146

Locks introduce additional mechanisms in DBMS:



The Scheduler

 collects arbitrarily interleaved requests from tx's
 uses locks to delay tx's, if necessary, to avoid unsafe schedules



... Lock-based Concurrency Control65/146

Lock table entries contain:

 object being locked
 type of lock: read/shared, write/exclusive
 FIFO queue of tx's requesting this lock
 count of tx's currently holding lock
	  (max 1 for write locks)

Lock and unlock operations must be atomic.

Lock upgrade:

 if a tx holds a read lock, and it is the only tx holding that lock
 then the lock can be converted into a write lock



... Lock-based Concurrency Control66/146

Synchronise access to shared data items via following rules:

 before reading X, get read (shared) lock on X
 before writing X, get write (exclusive) lock on X
 a tx attempting to get a read lock on X is blocked
	if another transaction already has write lock on X
 a tx attempting to get an write lock on X is blocked
	if another transaction has any kind of lock on X

These rules alone do not guarantee serializability.


... Lock-based Concurrency Control67/146

Consider the following schedule, using locks:
T1: Lr(Y)     R(Y)               U(Y)         Lw(X) W(X) U(X)
T2:      Lr(X)    R(X) U(X) Lw(Y)....W(Y) U(Y)

(where Lr = read-lock, Lw = write-lock, U = unlock)

Locks correctly ensure controlled access to shared objects (X, Y).

Despite this, the schedule is not serializable.


Two-Phase Locking68/146

To guarantee serializability, we require an additional constraint
on how locks are applied:

 in every tx, all lock requests precede unlock requests

Each transaction is then structured as:

 growing phase where locks are acquired
 action phase where "real work" is done
 shrinking phase where locks are released



... Two-Phase Locking69/146

Consider the following two transactions:
T1: Lw(A) R(A) F1 W(A) Lw(B) U(A) R(B) G1 W(B) U(B)
T2: Lw(A) R(A) F2 W(A) Lw(B) U(A) R(B) G2 W(B) U(B)

They follow 2PL protocol, inducing a schedule like:
T1(a): Lw(A)      R(A) F1 W(A) Lw(B) U(A)
T2(a):      Lw(A) ...................... R(A) F2 W(A)

T1(b): R(B)      G1 W(B) U(B)
T2(b):     Lw(B) ............ U(A) R(B) G2 W(B) U(B)



Problems with Locking70/146

Appropriate locking can guarantee correctness.

However, it also introduces potential undesirable effects:

 Deadlock

No transactions can proceed; each waiting on lock held by another.
 Starvation

One transaction is permanently "frozen out" of access to data.
 Reduced performance

Locking introduces delays while waiting for locks to be released.



Deadlock71/146


Deadlock occurs when two transactions are waiting
for a lock on an item held by the other.

Example:
T1: Lw(A) R(A)            Lw(B) ......
T2:            Lw(B) R(B)       Lw(A) .....

How to deal with deadlock?

 prevent it happening in the first place
 let it happen, detect it, recover from it



... Deadlock72/146

Handling deadlock involves forcing a transaction to "back off".

 select process to "back off"


 choose on basis of how far transaction has progressed, # locks held, ...


 roll back the selected process


 how far does this it need to be rolled back? (less roll-back is better)
 worst-case scenario: abort one transaction


 prevent starvation


 need methods to ensure that same transaction isn't always chosen





... Deadlock73/146

Simple approach: timeout

 set maximum time limit for execution of transaction
 if transaction exceeds limit, abort it
 if caused by deadlock, all its resources are freed

Better approach: waits-for graph

 one node per transaction Ti
 a directed edge from Tj to Tk, if

 Tj is waiting on a lock held by Tk




... Deadlock74/146

A cycle in the waits-for graph indicates a deadlock.

Could prevent deadlock by

 each time Ti delays trying to get lock
	(i.e. add new edge to graph)
 check if this would add a cycle to the graph ⇒ abort Ti

Could detect deadlock by

 periodically check for cycles in the waits-for graph
 choose one Ti from the cycle and abort it



... Deadlock75/146

Alternative deadlock handling: timestamps.

Each tx is permanently allocated a unique timestamp (e.g. start-time).

When Tj tries to get lock held by Tk

 compare timestamps of two transactions
 use a fixed policy to decide what to do
 two possible policies: wait-die or wound-wait


Both schemes prevent waits-for cycles by imposing order/priority on tx's.



... Deadlock76/146

Tj tries to get lock held by Tk ...

Wait-die scheme:

 if Tj is older than Tk,
	Tj is allowed to wait
 otherwise, Tj aborts (i.e. is rolled back)

Wound-wait schema:

 if Tj is older than Tk,
	it "wounds" Tk 
	(Tk must roll back and give Tj any locks it needs )
 otherwise, Tj is allowed to wait



... Deadlock77/146

Properties of deadlock handling methods:

 both wait-die and wound-wait are fair
 wait-die tends to


 roll back tx's that have done little work
 but rolls back tx's more often


 wound-wait tends to


 roll back tx's that may have done significant work
 but rolls back tx's less often


 timestamp-based are easier to implement than waits-for graph
 waits-for minimises roll backs because of deadlock



Starvation78/146

Starvation occurs when one transaction

 is "trapped" waiting on a lock indefinitely
 while other transactions continue normally

Whether it occurs depends on the lock wait/release strategy.

Multiple locks ⇒ need to decide which to release first.

Solutions:

 implement a fair wait/release strategy
	(e.g. FIFO)
 use priority deadlock prevention schemes
	(e.g. wait-die)



Locking Granularity79/146

Locking typically reduces concurrency ⇒ reduces throughput.

Granularity of locking can impact performance:

+
lock a small item ⇒ more of database accessible

+
lock a small item ⇒ quick update ⇒ quick lock release

-
lock small items ⇒ more locks ⇒ more lock management

Granularity levels: field, row (tuple), table, whole database


... Locking Granularity80/146

Multiple-granularity locking protocol:

 adds new "intention" locks (LIR, LIW)
 before locking a low-level item (e.g. record), acquire
	intention locks on its ancestors
 unlock from lowest-level to higher-level items

Example: T1 scans table R and updates some tuples

 gets R+IW lock on R, then gets R lock on each tuple
 occasionally, upgrades to W lock on a tuple

Example: T2 uses an index to read part of R

 gets IR lock on R, then gets R lock on each tuple



Locking in B-trees81/146

How to lock a B-tree leaf node?

One possibility:

 lock root, then each node down path
 finally, lock the leaf node

If for searching (select), locks would be read locks.

If for insert/delete, locks would be write locks.

This approach gives poor performance
	(lock on root is bottleneck).


... Locking in B-trees82/146

Approach for searching (select) ...

 acquire read lock on each node
 release lock when child has been locked
 repeat down to leaf node
	(which is only lock still held)

Approach for insert/delete ...

 traverse from root, getting write lock on each node
 at root, check whether any propagation might occur 
	(i.e. inserting and node is full, deleteing and node is half full)
 if "safe", release locks on all ancestors
 maintain lock on leaf node and update appropriately



Optimistic Concurrency Control83/146

Locking is a pessimistic approach to concurrency control:

 limit concurrency to ensure that conflicts don't occur

Costs: lock management, deadlock handling, contention.

In systems where read:write ratio is very high

 don't lock (allow arbitrary interleaving of operations)
 check just before commit that no conflicts occurred
 if problems, roll back conflicting transactions



... Optimistic Concurrency Control84/146

Transactions have three distinct phases:

 Reading: read from database, modify local copies of data
 Validation: check for conflicts in updates
 Writing: commit local copies of data to database

Timestamps are recorded at points noted:





... Optimistic Concurrency Control85/146

Data structures needed for validation:

 Act set: txs that are reading data and computing results
 Val set: txs that have reached validation (not yet committed)
 Fin set: txs that have finished (committed data to storage)
 for each Ti,  timestamps for when it reached A, V, F
 R(Ti) set of all data items read by Ti
 W(Ti) set of all data items to be written by Ti

Use the V timestamps as ordering for transactions

 assume serial tx order based on ordering of V(Ti)'s



... Optimistic Concurrency Control86/146

Validation check for transaction T

 for all transactions Ti  ≠  T

 if V(Ti) < A(T) < F(Ti),
	then W(Ti) ∩ R(T) is empty
 if V(Ti) < V(T) < F(Ti),
	then W(Ti) ∩ W(T) is empty


If this test fails for any Ti, then T is rolled back.

What this prevents ...

 T reading dirty data (i.e. data later changed by Ti)
 T overwriting changes made by Ti



... Optimistic Concurrency Control87/146

Problems with optimistic concurrency control:

 if validation fails, "complete" tx's are rolled back
 costs of maintaining the sets of tx's and the R/W-sets

Notes:

 validation test must be atomic ⇒ some locking needed
 T remains in Fin until there are no Ti satisfying A(Ti) < F(T)



Multi-version Concurrency Control88/146

Multi-version concurrency control (MVCC) aims to

 retain benefits of locking, but get more concurrency
 provide multiple (consistent) versions of the database

Achieves this by

 readers access an "appropriate" version of each data item
 writers make new versions of the data items they modify

Main difference between MVCC and standard locking:

 read locks do not conflict with write locks ⇒
 reading never blocks writing, writing never blocks reading



... Multi-version Concurrency Control89/146

Each transaction is

 associated with a time-stamp (TS)
 declared as a reader or a writer 
	(writers may modify data items; readers are guaranteed not to)


Each record in the database is

 tagged with timestamp of tx that wrote it (WTS)
 tagged with timestamp of tx that read it last (RTS)
 chained to older versions of itself
 discarded when it is too old to be "of interest" (vacuum) 
	(newer versions exist; all tx's started after the subsequent version)



... Multi-version Concurrency Control90/146

When a reader Ti is accessing the database

 ignore any data item created after Ti started (WTS > TS(Ti))
 use only newest version V satisfying
	 WTS(V) < TS(Ti)

When a writer Tj changes a data item

 find newest version V satsifying WTS(V) < TS(Tj)
 if RTS(V) < TS(Tj), create new version of data item
 if no such version available, reject the write



Concurrency Control in SQL91/146

Transactions in SQL are specified by

 BEGIN ... start a transaction
 COMMIT ... successfully complete a transaction
 ROLLBACK ... undo changes made by transaction + abort

In PostgreSQL, other actions that cause rollback:

 raise exception during execution of a function
 returning null from a before trigger



... Concurrency Control in SQL92/146

More fine-grained control of "undo" via savepoints:

 SAVEPOINT ... marks point in transaction
 ROLLBACK TO SAVEPOINT ... undo changes, continue transaction

Example:
begin;
  insert into numbersTable values (1);
  savepoint my_savepoint;
  insert into numbersTable values (2);
  rollback to savepoint my_savepoint;
  insert into numbersTable values (3);
commit;

will insert 1 and 3 into the table, but not 2.


... Concurrency Control in SQL93/146

SQL standard defines four levels of transaction isolation.

 serializable - strongest isolation, most locking
 repeatable read
 read committed
 read uncommitted - weakest isolation, less locking

The weakest level allows dirty reads, phantom reads, etc.


PostgreSQL implements:
repeatable-read = serializable,
read-uncommitted = read-committed



... Concurrency Control in SQL94/146

Using the serializable isolation level, a select:

 sees only data committed before the transaction began
 never sees changes made by concurrent transactions 

Using the serializable isolation level, an update fails:

 if it tries to modify an "active" data item 
(active = affected by some other transaction, either committed or uncommitted)

The transaction containing the failed update will rollback and re-start.


... Concurrency Control in SQL95/146

Explicit control of concurrent access is available, e.g.

Table-level locking: LOCK TABLE

 various kinds of shared/exclusive locks are available

 access share allows others to read, and some writes
 exclusive allows others to read, but not to write
 access exclusive blocks all other access to table

 SQL commands automatically acquire appropriate locks

 e.g. ALTER TABLE acquires an access exclusive lock



Row-level locking: SELECT FOR UPDATE, DELETE

 allows others to read, but blocks write on selected rows

All locks are released at end of transaction (no explicit unlock)


Concurrency Control in PostgreSQL96/146

PostgreSQL uses two styles of concurrency control:

 multi-version concurrency control (MVCC) 
	(used in the implementation of SQL DML statements (e.g. select))
 two-phase locking (2PL) 
	(used in the implementation of SQL DDL statements (e.g. create table))

From the SQL (PLpgSQL) level:

 can let the lock/MVCC system handle concurrency
 can handle it explicitly via LOCK statements



... Concurrency Control in PostgreSQL97/146

Implementing MVCC in PostgreSQL requires:

 a log file to maintain current status of each Tj
 in every tuple:

 xmin ID of the tx that created the tuple
 xmax ID of the tx that replaced/deleted the tuple (if any)
 xnew link to newer versions of tuple (if any)

 for each transaction Tj:

 a transaction ID (timestamp) 
 SnapshotData: list of active tx's when Tj started




... Concurrency Control in PostgreSQL98/146

Rules for a tuple to be visible to Tj:

 the xmin (creation transaction) value must

 be committed in the log file
 have started before Tj's start time
 not be active at Tj's start time

 the xmax (delete/replace transaction) value must

 be blank or refer to an aborted tx, or
 have started after Tj's start time, or
 have been active at SnapshotData time




... Concurrency Control in PostgreSQL99/146

A transaction sees a consistent view of the database, but may
not see the "current" view of the database.


E.g. T1 does a select and then concurrent T2 deletes some of T1's selected tuples


This is OK unless tx's communicate outside the database system.

For applications that require that every transaction accesses the
current consistent version of the data, explicit locks are available.

Locks are available at various granluarities:

 LOCK TABLE locks an entire table
 SELECT FOR UPDATE locks only the selected rows



Implementing Atomicity/Durability



Atomicity/Durability101/146

Reminder:

Transactions are atomic

 if a tx commits, all of its changes occur in DB
 if a tx aborts, none of its changes occur in DB

Transaction effects are durable

 if a tx commits, its effects persist 
	(even in the event of subsequent (catastrophic) system failures)

Implementation of atomicity/durability is intertwined.


Durability102/146

What kinds of "system failures" do we need to deal with?

 single-bit inversion during transfer mem-to-disk
 decay of storage medium on disk (some data changed)
 failure of entire disk device (no longer accessible)
 failure of DBMS processes (e.g. postgres crashes)
 operating system crash, power failure to computer room
 complete destruction of computer system running DBMS


The last requires off-site backup; all others should be locally recoverable.



... Durability103/146

Consider following scenario:



Desired behaviour after system restart:

 all effects of T1, T2 persist
 as if T3, T4 were aborted (no effects remain)



... Durability104/146

Durabilty begins with a stable disk storage subsystem.

Operations:

 putBlock(Buffer *b) ... writes data from buffer to disk
 getBlock(Buffer *b) ... reads data from disk into buffer

Each call to putBlock must ensure that

 data is transferred from buffer to disk verbatim (unchanged)

Subsequent getBlock on same disk sector must ensure that

 same data that was last written is transferred back to buffer



... Durability105/146

Implementation of transaction operations R(V) and W(V)
Value R(Object V) {
    B = getBuf(blockContaining(V))
    return value of V from B
}
void W(Object V, value k) {
    B = getBuf(blockContaining(V))
    set value of V in B
}

Note:

 W does not actually do output; happens when buffer replaced
 if tx terminates before buffer replaced, need to do putBuf()



... Durability106/146

getBuf() and putBuf() interface buffer pool with disk
Buffer getBuf(BlockAddr A) {
    if (!inBufferPool(A)) {
        B = availableBuffer(A);
        getBlock(B);
    }
    return bufferContaining(A);
}
void putBuf(BlockAddr A) {
    B = bufferContaining(A);
    putBlock(B);
}



Stable Store107/146

One simple strategy using redundancy: stable store.

Protects against all failures in a single disk sector.

Each logical disk page X is stored twice.

(Obvious disadvantage: disk capacity is halved)

X is stored in sector S on disk L and sector T on disk R

Assume that a sound parity-check is available


(i.e. can always recognise whether data has transferred mem↔disk correctly)



... Stable Store108/146

Low level sector i/o functions:
int writeSector(char *b, Disk d, Sector s) {
   int nTries = 0;
   do {
      nTries++;  write(d,s,b);
   } while (bad(parity) && nTries < MaxTries)
   return nTries;
}
int readSector(char *b, Disk d, Sector s) {
   int nTries = 0;
   do {
      nTries++;  read(d,s,b);
   } while (bad(parity) && nTries < MaxTries)
   return nTries;
}



... Stable Store109/146

Writing data to disk with stable store:
int writeBlock(Buffer *b, Disk d, Sector s) {
   int sec;
   for (;;) {
      sec = (s > 0) ? s : getUnusedSector(d);
      n = writeSector(b->data, d, sec);
      if (n == maxTries)
         mark s as unusable
      else
         return sec;
   }
}


 call writeBlock twice for each buffer b, on disks L and R
 remember association between b and (L,S) and (R,T)



... Stable Store110/146

Reading data from disk with stable store:
int readBlock(Buffer *b) {
   int n = readSector(b->data, b->diskL, b->sectorL);
   if (n == maxTries) {
      n = readSector(b->data, b->diskR, b->sectorR);
      if (n == maxTries) return -1; // read failure
   }
   return 0; // successful read
}


 if read from disk L succeeds, ignore disk R 
	(if sector (R,T) has failed, we discover failure on subsequent write)
 if read from disk L fails, get correct copy from disk R 
	(and, at the same time, can mark sector (L,S) as bad)
 the chances of both (L,S) and (R,T) failing is extremely small



... Stable Store111/146

Consider scenario where power fails during write operation:

 aim is to write new contents of buffer X to stable store
 will be done via writeBlock(X,L,S); writeBlock(X,R,T);
 on power-fail, intended new X is lost from memory
 partial write to sector will produce corrupted data
 power-fail happens at a point in time, so only one write fails

Wish to restore the system to a state where:

 sectors XL and XR are back "in sync"
 can recognize whether we have old/new version on disk



... Stable Store112/146

How stable storage handles failure during writing:

 failure occurs while writing XL ...

 sector XL will subsequently be seen as bad
 valid, but old, value of X is available in XR
 can repair XL by copying value from XR

 failure occurs while writing XR ...

 sector XR will subsequently be seen as bad
 new value of X is available in XL
 can repair XR by copying value from XL




RAID113/146

RAID gives techniques to achieve

 good read/write performance
 recovery from multiple simultaneous disk failures

Requires:

 multiple disks   (Redunant Array of Inexpensive Disks)
 arrangement of data across multiple disks
 use of error-correcting codes (ECCs) 

(See texts for further discussion on RAID)


Stable Storage Subsystem114/146

We can prevent/minimise loss/corruption of data due to:

 mem/disk transfer corruption: parity checking
 sector failure: mark "bad" blocks, stable storage
 disk failure: RAID (levels 4,5,6)
 destruction of computer system: 

- complete DB backups, stored away from computer system 
- on-line, distributed copies of database (consistency?)


If all of these implemented, assume stable storage subsystem.


Dealing with Transactions115/146

The remaining "failure modes" that we need to consider:

 failure of DBMS processes or operating system
 failure of transactions (ABORT)

Standard technique for managing these:

 keep a log of changes made to database
 use this log to restore state in case of failures



Architecture for Atomicity/Durability116/146

How does a DBMS provide for atomicity/durability?





Execution of Transactions117/146

Transactions deal with three address spaces:

 stored data on the disk
	  (representing DB state)
 data in memory buffers
	  (where held for sharing)
 data in their own local variables
	  (where manipulated)

Each of these may hold a different "version" of a DB object.


... Execution of Transactions118/146

Operations available for data transfer:

 INPUT(X) ... read page containing X into a buffer
 READ(X,v) ... copy value of X from buffer to local var v
 WRITE(X,v) ... copy value of local var v to X in buffer
 OUTPUT(X) ... write buffer containing X to disk

READ/WRITE are issued by transaction.

INPUT/OUTPUT are issued by buffer manager (and log manager).


... Execution of Transactions119/146

Example of transaction execution:
-- implements A = A*2; B = B+1;
BEGIN
READ(A,v); v = v*2; WRITE(A,v);
READ(B,v); v = v+1; WRITE(B,v);
COMMIT

READ accesses the buffer manager and may cause INPUT.

COMMIT needs to ensure that buffer contents go to disk.


... Execution of Transactions120/146

States as the transaction executes:

t   Action        v  Buf(A)  Buf(B)  Disk(A)  Disk(B)
-----------------------------------------------------
(0) BEGIN         .      .       .        8        5
(1) READ(A,v)     8      8       .        8        5
(2) v = v*2      16      8       .        8        5
(3) WRITE(A,v)   16     16       .        8        5
(4) READ(B,v)     5     16       5        8        5
(5) v = v+1       6     16       5        8        5
(6) WRITE(B,v)    6     16       6        8        5
(7) OUTPUT(A)     6     16       6       16        5
(8) OUTPUT(B)     6     16       6       16        6

After tx completes, we must have either 
Disk(A)=8,Disk(B)=5   or   Disk(A)=16,Disk(B)=6

If system crashes before (8), may need to undo disk changes.
If system crashes after (8), may need to redo disk changes.


Transactions and Buffer Pool121/146

Two issues arise w.r.t. buffers:

 forcing ... OUTPUT buffer on each WRITE

 ensures durability; disk always consistent with buffer pool
 poor performance; defeats purpose of having buffer pool

 stealing ... replace buffers of uncommitted tx's

 if we don't, poor throughput (tx's blocked on buffers)
 if we do, seems to cause atomicity problems?


Ideally, we want stealing and not forcing.


... Transactions and Buffer Pool122/146

Handling stealing:

 page P, held by tx T, is output to disk and replaced
 if T aborts, some of its changes are already "committed"
 must log changed values in P at "steal-time"
 use these to UNDO changes in case of failure of T

Handling no forcing:

 consider: transaction T commits, then system crashes
 but what if modified page P has not yet been output?
 must log changed values in P as soon as they change
 use these to support REDO to restore changes



Logging123/146

Three "styles" of logging

 undo changes by any uncommitted tx's
 redo changes by any committed tx's
 undo/redo ... combines aspects of both

All approaches require:

 a sequential file of log records
 records describing changes are written first
 actual changes to data are written later



Undo Logging124/146

Simple form of logging which ensures atomicity.

Log file consists of a sequence of small records:

 <START T> ... transaction T begins
 <COMMIT T> ... transaction T completes successfully
 <ABORT T> ... transaction T fails (no changes)
 <T,X,v> ... transaction T changed value of X from v

Notes:

 update log entry created for each WRITE (not OUTPUT)
 update log entry contains old value
	(new value is not recorded)



... Undo Logging125/146

Data must be written to disk in the following order:

 start transaction log record
 update log records indicating changes
 the changed data elements themselves
 the commit log record

Note: sufficient to have <T,X,v> output before X, for each X


... Undo Logging126/146

For the example transaction, we would get:

t    Action        v  B(A)  B(B)  D(A)  D(B)  Log
--------------------------------------------------------
(0)  BEGIN         .    .     .     8     5   <START T>
(1)  READ(A,v)     8    8     .     8     5
(2)  v = v*2      16    8     .     8     5
(3)  WRITE(A,v)   16   16     .     8     5   <T,A,8>
(4)  READ(B,v)     5   16     5     8     5
(5)  v = v+1       6   16     5     8     5
(6)  WRITE(B,v)    6   16     6     8     5   <T,B,5>
(7)  FlushLog
(8)  StartCommit
(9)  OUTPUT(A)     6   16     6    16     5
(10) OUTPUT(B)     6   16     6    16     6
(11) EndCommit                                <COMMIT T>
(12) FlushLog

Note that T is not regarded as committed until (11).


... Undo Logging127/146

Simplified view of recovery using UNDO logging:
committedTrans = abortedTrans = startedTrans = {}
for each log record from most recent to oldest {
    switch (log record) {
    <COMMIT T> : add T to committedTrans
    <ABORT T>  : add T to abortedTrans
    <START T>  : add T to startedTrans
    <T,X,v>    : if (T in committedTrans)
                       // don't undo committed changes
                 else
                     { WRITE(X,v); OUTPUT(X) }
}   }
for each T in startedTrans {
    if (T in committedTrans) ignore
    else if (T in abortedTrans) ignore
    else write <ABORT T> to log
}
flush log



... Undo Logging128/146

Recall example transaction and consider effects of system crash
at the following points:

Before (9) ... disk "restored" (unchanged); <ABORT T> written

(9)-(11) ... disk restored to original state; <ABORT T>  written

After (12) ... A and B left unchanged; T treated as committed

"Disk restored" means
WRITE(B,5); OUTPUT(B); WRITE(A,8); OUTPUT(A);



Checkpointing129/146

Previous view of recovery implied reading entire log file.

Since log file grows without bound, this is infeasible.

Eventually we can delete "old" section of log.

 i.e. where all prior transactions have committed

This point is called a checkpoint.

 all of log prior to checkpoint can be ignored for recovery



... Checkpointing130/146

Problem: many concurrent/overlapping transactions.

How to know that all have finished?

Simplistic approach

 stop accepting new transactions (block them)
 wait until all active tx's either commit or abort 
	(and have written a <COMMIT T> or <ABORT T> on the log)
 flush the log to disk
 write new log record <CHKPT>, and flush log again
 resume accepting new transactions

Known as quiescent checkpointing.


... Checkpointing131/146

Obvious problem with quiescent checkpointing

 DBMS effectively shut down until all existing tx's finish

Better strategy: nonquiescent checkpointing

 write log record <CHKPT (T1,..,Tk)> 
	(contains references to all active transactions ⇒ active tx table)
 continue normal processing (e.g. new tx's can start)
 when all of T1,..,Tk have completed, 
	write log record <ENDCHKPT> and flush log



... Checkpointing132/146

Recovery: scan backwards through log file processing as before.

Determining where to stop depends on ...

If we encounter <ENDCHKPT> first:

 we know that all incomplete tx's come after prev <CHKPT...>
 thus, can stop backward scan when we reach <CHKPT...>

If we encounter <CHKPT (T1,..,Tk)> first:

 crash occurred during the checkpoint period
 any of T1,..,Tk that committed before crash are done
 for uncommitted tx's, need to continue backward scan 
	(could simplify this task by chaining together log records for each tx)



Redo Logging133/146

Problem with UNDO logging:

 all changed data must be output to disk before committing
 conflicts with optimal use of the buffer pool

Alternative approach is redo logging:

 allow changes to remain only in buffers after commit
 write records to indicate what changes are "pending"
 after a crash, can apply changes during recovery



... Redo Logging134/146

Requirement for redo logging: write-ahead rule.

Data must be written to disk as follows:

 start transaction log record
 update log records indicating changes
 the commit log record
 the changed data elements themselves

Note that update log records now contain <T,X,v'>, 
where v' is the new value for X.


... Redo Logging135/146

For the example transaction, we would get:

t    Action        v  B(A)  B(B)  D(A)  D(B)  Log
--------------------------------------------------------
(0)  BEGIN         .    .     .     8     5   <START T>
(1)  READ(A,v)     8    8     .     8     5
(2)  v = v*2      16    8     .     8     5
(3)  WRITE(A,v)   16   16     .     8     5   <T,A,16>
(4)  READ(B,v)     5   16     5     8     5
(5)  v = v+1       6   16     5     8     5
(6)  WRITE(B,v)    6   16     6     8     5   <T,B,6>
(7)  COMMIT                                   <COMMIT T>
(8)  FlushLog
(9)  OUTPUT(A)     6   16     6    16     5
(10) OUTPUT(B)     6   16     6    16     6

Note that T is regarded as committed as soon as (8) completes.


... Redo Logging136/146

Simplified view of recovery using REDO logging:
set committedTrans // e.g. from tx table
for each log record from oldest to most recent {
    switch (log record) {
    <COMMIT T> : ignore // know these already
    <ABORT T>  : add T to abortedTrans
    <START T>  : add T to startedTrans
    <T,X,v'>   : if (T in committedTrans)
                       { WRITE(X,v'); OUTPUT(X) }
                 else
                     // nothing to do, no change on disk
}   }
for each T in startedTrans {
    if (T in committedTrans) ignore
    else if (T in abortedTrans) ignore
    else write <ABORT T> to log
}
flush log



... Redo Logging137/146

Data required for REDO logging checkpoints:

 must know which transactions have committed 
	(hold a list of completed tx's in tx table between checkpoints)
 must know which buffer pool pages are dirty 
	(buffer pool would normally record this information anyway)
 must know which tx's modified these pages 
	(one buffer pool page may have been written to by several tx's)



... Redo Logging138/146

Checkpoints in REDO logging use (as before):

 <CHKPT (T1,..,Tk)> to record active tx's at start of checkpoint
 <ENDCHKPT> to record end of checkpoint period

Steps in REDO log checkpointing

 write <CHKPT (T1,..,Tk)> to log and flush log
 output to disk all changed objects in buffer pool 
	which have not yet been written to disk 
	whose tx's had committed before <CHKPT...>
 write <ENDCHKPT> to log and flush log

Note that other tx's may continue between steps 2 and 3.


... Redo Logging139/146

Recovery with checkpointed REDO log.

As for UNDO logging, two cases ...

Last checkpoint record before crash is <ENDCHKPT>

 every tx that committed before prev <CHKPT...> is fine
 need to restore values for all T1,..,Tk
	(long backward search?)
 similarly for all Ti started after <CHKPT...>

Last checkpoint record before crash is <CHKPT (T1,..,Tk)>

 cannot be sure that committed tx's before here are done
 need to search back to most recent prior <ENDCHKPT>
 then proceed as for first case



Undo/Redo Logging140/146

UNDO logging and REDO logging are incompatible in

 order of outputting <COMMIT T> and changed data
 how data in buffers is handled during checkpoints

Undo/Redo logging combines aspects of both

 requires new kind of update log record 
	<T,X,v,v'> gives both old and new values for X
 removes incompatibilities between output orders



... Undo/Redo Logging141/146

Requirement for undo/redo logging: write-ahead.

Data must be written to disk as follows:

 start transaction log record
 update log records indicating changes
 the changed data elements themselves

Do not specify when the <COMMIT T> record is written.


... Undo/Redo Logging142/146

For the example transaction, we might get:

t    Action        v  B(A)  B(B)  D(A)  D(B)  Log
--------------------------------------------------------
(0)  BEGIN         .    .     .     8     5   <START T>
(1)  READ(A,v)     8    8     .     8     5
(2)  v = v*2      16    8     .     8     5
(3)  WRITE(A,v)   16   16     .     8     5   <T,A,8,16>
(4)  READ(B,v)     5   16     5     8     5
(5)  v = v+1       6   16     5     8     5
(6)  WRITE(B,v)    6   16     6     8     5   <T,B,5,6>
(7)  FlushLog
(8)  StartCommit
(9)  OUTPUT(A)     6   16     6    16     5
(10)                                          <COMMIT T>
(11) OUTPUT(B)     6   16     6    16     6

Note that T is regarded as committed as soon as (10) completes.


... Undo/Redo Logging143/146

Recovery using undo/redo logging:

 redo all committed tx's from earliest to latest
 undo all incomplete tx's from latest to earliest

Consider effects of system crash on example

Before (10) ... treat as incomplete; undo all changes

After (10) ... treat as complete; redo all changes


... Undo/Redo Logging144/146

Steps in UNDO/REDO log checkpointing

 write <CHKPT (T1,..,Tk)> to log and flush log
 output to disk all dirty memory buffers
 write <ENDCHKPT> to log and flush log

Note that other tx's may continue between steps 2 and 3.

A consequence of the above:

 tx's must not place changed data in buffers
	until they are certain that they will COMMIT


(If we allowed this, a buffer may contain changes from both committed and aborted tx's)



... Undo/Redo Logging145/146

The above description simplifies details of undo/redo logging.

Aries is a complete algorithm for undo/redo logging.

Differences to what we have described:

 log records contain a sequence numnber (LSN) 
 LSNs used in tx and buffer managers, and stored in data pages
 additional log record to mark <END>
	(of commit or abort)
 <CHKPT> contains only a timestamp
 <ENDCHKPT..> contains tx and dirty page info


(For more details consult any text or COMP9315 05s1 lecture notes)



Recovery in PostgreSQL146/146

PostgreSQL uses write-ahead undo/redo style logging.

However, it also uses multi-version concurrency control

 tags each record with a tx and update timestamp
 which simplifies aspects of undo/redo, e.g.


 some info required by logging is already held in each tuple
 no need to undo effects of aborted tx's; old versions still available



Transaction/logging code is distributed throughout backend source.

Core transaction code is in src/backend/access/transam.

Produced: 15 May 2016</p><h3 >字段10</h3><p>Non-classical DBMSs


Parallel and Distributed Databases



Parallel and Distributed Systems2/37

The discussion so far has revolved around systems

 with a single or small number of processors
 accessing a single memory space
 getting data from one or more disk devices






Parallel Architectures3/37

Types:   shared memory,   shared disk,   shared nothing

Example shared-nothing architecture:




Typically in the same room   (data transfer cost ~ 100's of μsecs)


... Parallel Architectures4/37

Hierarchical architectures are hybrid parallel ones



Typically on a local-area network   (data transfer cost ~ msecs)


Distributed Architectures5/37

Distributed architectures are ...

 effectively shared-nothing, on a global-scale network




Typically on the Internet   (data transfer cost ~ secs)


Parallel Databases (PDBs)6/37

Parallel databases provide various forms of parallelism ...

 processor parallelism can assist in speeding up memory ops
 processor parallelism introduces cache coherence issues
 disk parallelism can assist in overcoming latency
 disk parallelism can be used to improve fault-tolerance (RAID)
 one limiting factor is congestion on communication bus

PDBs typically run on closely-connected parallel architectures, 
so we focus on hybrid architectures on a LAN.


Data Storage in PDBs 7/37

Consider each table as a collection of pages ...

Page addressing: (Table, File, PageNum)

 Table maps to a set of files (e.g. named by tableID)
 File distinguishes primary/overflow files
 PageNum maps to an offset in a specific file

If all data for one table resides on one node

 the above addressing scheme is adequate
 Table can identify (Node, FileSet)



... Data Storage in PDBs 8/37

However, with multiple nodes, we could ...

 replicate tables across several node

 in which case, Table yields { (Node, FileSet) }

 partition pages for one table across several nodes

 in which case page addressing changes to include node
 (Node, Table, File, PageNum)


Could also have a combination of partitioning and replication


... Data Storage in PDBs 9/37

Data-partitioning example:






... Data Storage in PDBs 10/37

Data-partitioning strategies for one table:

 round-robin partitioning

 cycle through nodes, each new tuple is added on the "next" node

 hash partitioning

 use hash value to determine which processor and page

 range partitioning

 ranges of attr values are assigned to processors



Assume:   R(a,b,c,...),   D0 .. Dn-1 disks,   tup0 .. tupr-1 tuples


Storing data on many disks maximises chance for parallel data access



... Data Storage in PDBs 11/37

Round-robin partitioning:

 tuple ti sent to Dj, 
	tuple ti+1 sent to D(j+1)%n
 advantage: spreads data uniformly across disks
 disadvantage: doesn't partition data "usefully" (for queries)
 sequential scan can exploit parallelism

 read data from multiple disks simultaneously

 index-based scan can exploit limited parallelism

 index gives list of pages, potential parallel read

 provides no assistance for hash-based access



... Data Storage in PDBs 12/37

Hash partitioning

 hash functions:  
	hN(Ai) → NodeID,  
	hP(Ai) → PageNum
 well-designed hash functions can spread tuples uniformly
 hash-based access can work well

 all tuples matching query hash will be on one node

 sequential scan performance depends on uniform spread
 index-on-hash works as for round-robin
 provides no assistance for range queries



... Data Storage in PDBs 13/37

Range partitioning

 uses partitioning vector pv to determine node for tuple

 allocates range of partitioning attribute values to each node

 pv = [ (v0,Di), (v1,Dj), ... (vh,Dm) ]

 all tuples with Ai ≤ v0  go to Di
 all tuples with v0 < Ai ≤ v1  go to Dj,   etc.

 need to choose vi boundary points carefully

 to ensure reasonably uniform spread of data over disks




PostgreSQL and Parallelism14/37

PostgreSQL assumes

 shared memory space accesible to all back-ends
 files for one table are located on one disk

PostgreSQL allows

 data to be distributed across multiple disk devices

So could run on ...

 shared-memory, shared-disk architectures
 hierarchical architectures with distributed virtual memory



... PostgreSQL and Parallelism15/37

PostgreSQL can provide

 multiple servers running on separate nodes
 application #1: high availability

 "standby" server takes over if primary server fails

 application #2: load balancing

 several servers can be used to provide same data
 direct queries to least loaded server


Both need data synchronisation between servers

PostgreSQL uses notion of master and slave servers.


... PostgreSQL and Parallelism16/37

High availability ...

 updates occur on master, recorded in tx log
 tx logs shipped/streamed from master to slave(s)
 slave uses tx logs to maintain current state
 configuration controls frequency of log shipping
 bringing slave up-to-date is fast (~1-2secs)


Note: small window for data loss (committed tx log records not sent)


Distributed Databases17/37

Two kinds of distributed databases

 parallel database on a distributed architecture

 single schema/control, data distributed over network

 independent databases on a distributed architecture

 independent schemas/DBMSs, combined via global schema


The latter are also called federated databases

Distribution of data complicates tx processing ...

 potential for multiple copies of data to become inconsistent
 commit or abort must occur consistently on all nodes



... Distributed Databases18/37

Distributed tx processing handled by two-phase commit

 initiating site has transaction coordinator Ci ...

 waits for all other sites executing tx T to "complete"
 sends <prepare T> message to all other sites
 waits for <ready T> response from all other sites
 if not received (timeout), or <abort T> received, flag abort
 if all other sites respond <ready T>, flag commit
 write <commit T> or <abort T> to log
 send <commit T> or <abort T> to all other sites

 non-initiating sites write log entries before responding



... Distributed Databases19/37

Distributed query processing 

 may require query ops to be executed on different nodes

 node provides only source of some data
 some nodes may have limited set of operations

 needs to merge data received from different nodes

 may require data transformation (to fit schemas together)


Query optimisation in such contexts is difficult.


Non-classical DBMSs



Classical DBMSs21/37

Assumptions made in conventional DBMSs:

 data is sets of tuples; tuples are lists of atomic values
 data values can be compared precisely (via =, >, <, ...)
 filters can be described via boolean formulae
 SQL is a suitable langauage for all data management
 transaction-based consistency is critical
 data stored on disk, processed in memory
 data transferred in blocks of many tuples
 disk ↔ memory cost is most expensive in system
 disks are connected to processors via fast local bus



Modern DBMSs22/37

Demands from modern applications

 more flexible data structuring mechanisms
 very large data objects/values (e.g. music, video)
 alternative comparisons/filters   (e.g. similarity matching)
 massive amounts of data   (too much to store "locally")
 massive number of clients   (thousands tx's per second)
 solid-state storage   (minimal data latency)
 data required globally   (network latency)


Clearly, not all of these are relevant for every modern application.



... Modern DBMSs23/37

Some conclusions:

 relational model doesn't work for all applications
 SQL is not appropriate for all applications
 hard transactions not essential for all applications

Some "modernists" claim that

 "for all"  is really  "for any"
 ⇒ relational DBMSs and SQL are dinosaurs
 ⇒ NoSQL is the new way



... Modern DBMSs24/37

Some approaches:

 storage systems: Google FS, Hadoop DFS,   Amazon S3
 data structures: BigTable, HBase, Cassandra,   XML, RDF
 data structures: column-oriented DBMSs e.g. C-store
 data structures: graph databases e.g. Neo4j
 operations: multimedia similarity search e.g. Shazam
 operations: web search e.g. Google
 transactions: eventual consistency
 programming: object-relational mapping (ORM)
 programming: MapReduce
 languages: Sawzall, Pig, Hive,   SPARQL
 DB systems: CouchDB, MongoDB, F1, Cstore



Scale, Distribution, Replication25/37

Data for modern applications is very large (TB, PB, XB)

 not feasible to store on a single machine
 not feasible to store in a single location

Many systems opt for massive networks of simple nodes

 each node holds moderate amount of data
 each data item is replicated on several nodes
 nodes clustered in different geographic sites

Benefits:

 reliability, fault-tolerance, availability
 proximity ... use data closest to client
 scope for parallel execution/evaluation



Schema-free Data Models26/37

Many new DBMSs provide (key,value) stores

 key is a unique identifier (cf. URI)
 value is an arbitrarily complex "object"

 e.g. a text document  (often structured, e.g. Wiki, XML)
 e.g. a JSON object: (property,value) list
 e.g. an RDF triple  (e.g. <John,worksFor,UNSW>)

 objects may contain keys to link to other objects

Tables can be simulated by a collection of "similar" objects.


Eventual Consistency27/37

RDBMSs use a strong transactional/consistency model

 if a tx commits, changes take effect "instantly"
 all tx's have a strong guarantee about data integrity

Many new DBMSs applications do not need strong consistency

 e.g. doesn't matter if catalogue shows yesterday's price

Because of distribution/replication

 update is initiated on one node
 different nodes may have different versions of data
 after some time, updates propagate to all nodes



... Eventual Consistency28/37

If different nodes have different versions of data

 conflicts arise, and need to be resolved (when noticed)
 need to decide which node has "the right value"

Levels of consistency (from Cassandra system)

 ONE: at least one node has committed change   (weakest)
 QUORUM: at least half nodes holding data have committed
 ALL: changes propagated to all copies   (strongest)



MapReduce29/37

MapReduce is a programming model

 suited for use on large networks of computers
 processing large amounts of data with high parallelism
 originally developed by Google; Hadoop is open-source implementation

Computation is structured in two phases:

 Map phase:

 master node partitions work into sub-problems
 distributes them to worker nodes (who may further distribute)

 Reduce phase:

 master collects results of sub-problems from workers
 combines results to produce final answer




... MapReduce30/37

MapReduce makes use of (key,value) pairs

 key values identify parts of computation

Map(key1,val1) → list(key2,val2)

 applied in parallel to all (key1,val1) pairs
 results with common key2 are collected in group for "reduction"

Reduce(key2,list(val2)) → val3

 collects all values tagged with key2
 combines them to produce result(s) val3



... MapReduce31/37

"Classic" MapReduce example (word frequency in set of docs):
function map(String name, String document):
  // name: document name
  // document: document contents
  for each word w in document:
    emit (w, 1)
 
function reduce(String word, Iterator partialCounts):
  // word: a word
  // partialCounts: list of aggregated partial counts
  sum = 0
  for each c in partialCounts:
    sum += c
  emit (word, sum)



... MapReduce32/37

MapReduce as a "database language"

 some advocates of MapReduce have oversold it (replace SQL)
 DeWitt/Stonebraker criticised this

 return to low-level model of data access
 all done before in distributed DB research
 misses efficiency opportunities affored by DBMSs

 concensus is emerging

 SQL/MapReduce good for different kinds of task
 MapReduce as a basis for SQL-like languages (e.g. Apache HiveQL)




Modern vs Classical33/37

Some criticisms of the NoSQL approach:

 DeWitt/Stonebraker: MapReduce: A major step backwards

Online parody of noSQL advocates
 (strong language warning)






Hadoop DFS34/37

Storage system to support distributed, replicated data (Apache)

 provides a view of data as a collection of named files
 each HDFS cluster has a collection of DataNodes

 manages blocks of data, typically on commodity hardware
 each file is a collection of blocks, stored on multiple nodes
 individual blocks may be replicated across nodes
 DataNodes provide read/write operations to clients

 each HDFS cluster has a NameNode

 manages name space and access to data by clients
 determines mapping of data blocks to DataNodes
 provides file open/close/rename operations to clients




... Hadoop DFS35/37

Data replication in Hadoop

 organised by the NameNode
 all blocks in a file are same size
 determines placement of copies of each block
 uses HeartBeat and BlockReport info from DataNodes
 attempts to maximise reliability and performance

 e.g. keeps copies on separate DataNodes (obvious)
 e.g. assigns closest DataNode to client for read/write

 a complex optimisation problem, requires tuning



Cassandra36/37

Distributed NoSQL "database management system" (Apache)

 provides a hybrid (key,value)/row-oriented store
 a column is a (name,value) pair  (e.g. (Name,John)
 a supercolumn is also a (name,value) pair

 the value is a set of columns  (cf tuple)

 a columnfamily is a set of (name,value) pairs  (cf table)

 each value is effectively a supercolumn

 a keyspace is a namespace to hold objects  (cf database)

Tables are distributed on a Hadoop DFS  (DBA-specified replication)


... Cassandra37/37

CQL is the Cassandra query language  (cf SQL)
create keyspace UNSW;  use UNSW;
// don't need to specify all columns
create columnfamily Student (sid int primary key);
// columns are named as values are added
insert into Student (sid, name, degree)
values (12345, 'John Smith', 'BSc(CompSci)');
// familiar syntax ...
select * from Student where sid=12345;
// cannot reference non-family columns
select * from Student where degree='BSc(CompSci)';
// consistency can enter explicitly
update Student using consistency QUORUM
set degree = 'MIT' where sid = 12345;


Produced: 30 May 2016</p></div></body></html>
