{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "topic: header\n",
      "\n",
      "content:\n",
      " Week 09\n",
      " Week 09\n",
      " Assignment 2\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Assignment 2 2/55\n",
      "\n",
      "content:\n",
      " Implement a signature-based filtering scheme\n",
      " using superimposed codeword signatures\n",
      " three types: tuple-level, page-level, bit-sliced\n",
      " We give you the overall framework, you supply the details\n",
      " Once working, experimental analysis of signature performance\n",
      " create several database instances\n",
      " run benchmark PMR queries for each signature scheme\n",
      " measure costs (sigs read, pages read, tuple comparisons, false matches)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 3/55\n",
      "\n",
      "content:\n",
      " What's in the framework ...\n",
      " applications: create, insert, query, gendata, stats\n",
      " ADTs: hash, bits, tsig, psig, bsig, page, tuple, reln, query\n",
      " We deal with individual relations, where each relation ...\n",
      " is comprised of unique tuples, all the same size, e.g.\n",
      " 1000079,QHogGVRvjQRPMXbhKKfg,a3-25,a4-16\n",
      " with schema  (id:int/unique, name:char(20), a3:char7, ...)\n",
      " and where attributes 3..n all have the same structure\n",
      " stored as char strings, but with no trailing '\\0'\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 4/55\n",
      "\n",
      "content:\n",
      " Relations have a number of parameters:\n",
      " page size  (fixed, defined by PAGESIZE)\n",
      " tuple size  (fixed, determined by #attributes n)  ⇒  c\n",
      " #tuples  =  r  (dynamic, determined by #inserts)\n",
      " #pages  =  b  (dynamic, determined by r and c)\n",
      " signature size  =  m  (fixed, determined by n and pF)\n",
      " bit-slice size  (dynamic**, determined by b)\n",
      " ** but not in this assignment (fixed at 4K bits ⇒ no more than 4k pages)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 5/55\n",
      "\n",
      "content:\n",
      " Formulae for determining signature sizes:\n",
      " #bits / attribute  =  k  =  1/loge2 . loge ( 1/pF )\n",
      " #bits in tuple sig  =  mt  =  ( 1/loge 2 )2 . n . loge ( 1/pF )\n",
      " #bits in page sig  =  mp  =  ( 1/loge 2 )2 . n . c . loge ( 1/pF )\n",
      " #bits in bit-slice  =  b ** (except that we fix it to 4K bits)\n",
      " Implementation: round sig/slice sizes up to multiple of 8  (bits/byte)\n",
      " i.e. if  mt  =  12, allocate 16 bits (2 bytes) for signature\n",
      " Above values are computed when relation is created, and stored in params file\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 6/55\n",
      "\n",
      "content:\n",
      " ./gendata #Tuples #Attrs [StartID] [Seed]\n",
      " generates  #Tuples, each with  #Attrs  attributes\n",
      " default starting ID is 1000000; can change with  StartID\n",
      " can change seed for random # generator (default is 0)\n",
      " write tuples to standard output as comma-separated fields\n",
      " Example:\n",
      " $ ./gendata  5  4\n",
      " 1000000,lrfkQyuQFjKXyQVNRTyS,a3-00,a4-00\n",
      " 1000001,FrzrmzlYGFvEulQfpDBH,a3-01,a4-01\n",
      " 1000002,lqDqrrCRwDnXeuOQqekl,a3-02,a4-02\n",
      " 1000003,AITGDPHCSPIjtHbsFyfv,a3-03,a4-03\n",
      " 1000004,lADzPBfudkKlrwqAOzMi,a3-04,a4-04\n",
      " $\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 7/55\n",
      "\n",
      "content:\n",
      " ./create RelName #Attrs 1/pF\n",
      " creates a relation called  RelName\n",
      " initially with zero tuples; grows via  insert\n",
      " all tuples added at the end of the last page\n",
      " all pages are full, except the last; no deletions\n",
      " ./stats RelName\n",
      " displays information about relation  RelName\n",
      " ./dump RelName\n",
      " displays parameters; can be used to recreate  RelName\n",
      " displays tuples from database, one per line\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 8/55\n",
      "\n",
      "content:\n",
      " ./insert RelName\n",
      " reads tuples, on per line, from standard input\n",
      " insert each tuple into  RelName\n",
      " all tuples added at the end of the last page\n",
      " if last page is full, add a new page at end\n",
      " all pages are full, except the last; no deletions\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 9/55\n",
      "\n",
      "content:\n",
      " ./query RelName PMR-query SigType\n",
      " displays all tuples that match  PMR-query\n",
      " queries have the form  x1,x1,...xn\n",
      " where each xi is either a value or ?\n",
      " queries can return 0 or more tuples\n",
      " Example queries, assuming  n=3\n",
      " ?,?,? ... matches all tuples\n",
      " 1000000,?,? ... matches tuples with attr1 = 1000000\n",
      " ?,abcde,? ... matches tuples with attr2 = \"abcde\"\n",
      " ?,abcde,a3-01 ... matches tuples with attr2 = \"abcde\" and attr2 = \"a3-01\"\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 10/55\n",
      "\n",
      "content:\n",
      " ./query RelName PMR-query SigType\n",
      " displays matching tuples, one-per-line\n",
      " SigType determines what kind of signatures are used\n",
      " SigType = t ... use tuple-level signatures\n",
      " SigType = p ... use page-level signatures\n",
      " SigType = b ... use bit-sliced signatures\n",
      " after displaying result tuples, should also display:\n",
      " number of signature/bit-slice pages read\n",
      " number of data pages read\n",
      " number of tuples checked for matching\n",
      " number of data pages read but with no matching tuples\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 11/55\n",
      "\n",
      "content:\n",
      " Relations are implemented as five files:\n",
      " Rel.info ... one page with relation parameters\n",
      " Rel.data ... pages containing tuples\n",
      " Rel.tsig ... pages containing tuple signatures\n",
      " Rel.psig ... pages containing page signatures\n",
      " Rel.bsig ... pages containing bit slices\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 13/55\n",
      "\n",
      "content:\n",
      " Contents of relation .info file\n",
      " Count  npages;   // number of data pages (dynamic)\n",
      " Count  ntups;    // total number of tuples (dynamic)\n",
      " Count  nattrs;   // number of attributes (fixed)\n",
      " Count  tupSize;  // # bytes in tuples (all same size)\n",
      " Count  tupPP;    // max tuples per page\n",
      " Count  tk;       // bits set per attribute\n",
      " Count  tm;       // width of tuple signature (bits)\n",
      " Count  tsigSize; // # bytes in tuple signature\n",
      " Count  tsigPP;   // max tuple signatures per page\n",
      " Count  pm;       // width of page signature (bits)\n",
      " Count  psigSize; // # bytes in page signature\n",
      " Count  psigPP;   // max tuple signatures per page\n",
      " Count  bm;       // width of bit-slice\n",
      " Count  bsigSize; // # bytes in bit-slice\n",
      " Count  bsigPP;   // max bit-slices per page\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 14/55\n",
      "\n",
      "content:\n",
      " Page contents (data file):\n",
      " There are up to ntups tuples in each page\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 15/55\n",
      "\n",
      "content:\n",
      " Page contents (tuple-level signature file):\n",
      " There are up to tsigPP signatures per page\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 16/55\n",
      "\n",
      "content:\n",
      " Page contents (page-level signature file):\n",
      " There are up to psigPP signatures per page\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 18/55\n",
      "\n",
      "content:\n",
      " Query-time data structure (minimal):\n",
      " struct QueryRep {\n",
      "     // static info\n",
      "     Reln    rel;       // need to remember Relation info\n",
      "     char   *qstring;   // query string\n",
      "     //dynamic info\n",
      "     Bits    pages;     // list of pages to examine\n",
      "     PageID  curpage;   // current page in scan\n",
      "     Count   curtup;    // current tuple within page\n",
      "     // statistics info\n",
      "     ... you can put here whatever you need\n",
      "     ... to produce the required statistics\n",
      " };\n",
      " This is effectively the iteration structure described previously.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 19/55\n",
      "\n",
      "content:\n",
      " hash.c\n",
      " hash function from PostgreSQL\n",
      " produces a 32 bit integer given a string\n",
      " util.c\n",
      " definition of fatal() error handler\n",
      " defs.h\n",
      " global definitions (e.g. PAGESIZE, Count)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 20/55\n",
      "\n",
      "content:\n",
      " Abstract Data Types ... interface provided, you implement\n",
      " bits.h ... operations on long bit-strings (Bits)\n",
      " tsig.h ... operations on tuple signatures\n",
      " psig.h ... operations on page signatures\n",
      " bsig.h ... operations on bit-slices\n",
      " page.h ... operations on pages (Page)\n",
      " tuple.h ... operations on tuples (Tuple)\n",
      " reln.h ... operations on relations (Reln)\n",
      " query.h ... operations for queries (Query)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Assignment 2 21/55\n",
      "\n",
      "content:\n",
      " Style of implementing ADTs\n",
      " interface defines type as a pointer to a struct\n",
      " implementation defines struct details\n",
      " Example:\n",
      " In Bits.h\n",
      " typedef struct _BitsRep *Bits;\n",
      " In Bits.c\n",
      " struct _BitsRep {\n",
      "     Count  nbits;         // how many bits\n",
      "     Count  nbytes;        // how many bytes in array\n",
      "     Byte   bitstring[1];  // array of bytes to hold bits\n",
      "                           // actual array size is nbytes\n",
      " };\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Transactions: the story so far 22/55\n",
      "\n",
      "content:\n",
      " Transactions should obey ACID properties\n",
      " Isolation can be compromised by uncontrolled concurrency\n",
      " Serializable schedules avoid potential anomalies\n",
      " less safe (more concurrent) isolation levels exist\n",
      " read uncommitted, read committed, repeatable read\n",
      " Styles of concurrency control\n",
      " locking   (two-phase, deadlock)\n",
      " optimistic concurrency control   (try, then fix problems)\n",
      " multi-version concurrency control   (less locking needed)\n",
      " Implementing Atomicity/Durability\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Atomicity/Durability 24/55\n",
      "\n",
      "content:\n",
      " Reminder:\n",
      " Transactions are atomic\n",
      " if a tx commits, all of its changes persist in DB\n",
      " if a tx aborts, none of its changes occur in DB\n",
      " Transaction effects are durable\n",
      " if a tx commits, its effects persist (even in the event of subsequent (catastrophic) system failures)\n",
      " Implementation of atomicity/durability is intertwined.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Durability 25/55\n",
      "\n",
      "content:\n",
      " What kinds of \"system failures\" do we need to deal with?\n",
      " single-bit inversion during transfer mem-to-disk\n",
      " decay of storage medium on disk (some data changed)\n",
      " failure of entire disk device (data no longer accessible)\n",
      " failure of DBMS processes (e.g. postgres crashes)\n",
      " operating system crash; power failure to computer room\n",
      " complete destruction of computer system running DBMS\n",
      " The last requires off-site backup; all others should be locally recoverable.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Durability 26/55\n",
      "\n",
      "content:\n",
      " Consider following scenario:\n",
      " Desired behaviour after system restart:\n",
      " all effects of T1, T2 persist\n",
      " as if T3, T4 were aborted (no effects remain)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Durability 27/55\n",
      "\n",
      "content:\n",
      " Durabilty begins with a stable disk storage subsystem\n",
      " i.e. effects of putPage() and getPage() are consistent\n",
      " We can prevent/minimise loss/corruption of data due to:\n",
      " mem/disk transfer corruption: parity checking\n",
      " sector failure: mark \"bad\" blocks\n",
      " disk failure: RAID (levels 4,5,6)\n",
      " destruction of computer system: off-site backups\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Dealing with Transactions 28/55\n",
      "\n",
      "content:\n",
      " The remaining \"failure modes\" that we need to consider:\n",
      " failure of DBMS processes or operating system\n",
      " failure of transactions (ABORT)\n",
      " Standard technique for managing these:\n",
      " keep a log of changes made to database\n",
      " use this log to restore state in case of failures\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Execution of Transactions 30/55\n",
      "\n",
      "content:\n",
      " Transactions deal with three address spaces:\n",
      " stored data on the disk   (representing DB state)\n",
      " data in memory buffers   (where held for sharing)\n",
      " data in their own local variables   (where manipulated)\n",
      " Each of these may hold a different \"version\" of a DB object.\n",
      " PostgreSQL processes make heavy use of shared buffer pool\n",
      " ⇒ transactions do not deal with much local data.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Execution of Transactions 31/55\n",
      "\n",
      "content:\n",
      " Operations available for data transfer:\n",
      " INPUT(X) ... read page containing X into a buffer\n",
      " READ(X,v) ... copy value of X from buffer to local var v\n",
      " WRITE(X,v) ... copy value of local var v to X in buffer\n",
      " OUTPUT(X) ... write buffer containing X to disk\n",
      " READ/WRITE are issued by transaction.\n",
      " INPUT/OUTPUT are issued by buffer manager (and log manager).\n",
      " INPUT/OUTPUT correspond to getPage()/putPage() mentioned above\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Execution of Transactions 32/55\n",
      "\n",
      "content:\n",
      " Example of transaction execution:\n",
      " -- implements A = A*2; B = B+1;\n",
      " BEGIN\n",
      " READ(A,v); v = v*2; WRITE(A,v);\n",
      " READ(B,v); v = v+1; WRITE(B,v);\n",
      " COMMIT\n",
      " READ accesses the buffer manager and may cause INPUT.\n",
      " COMMIT needs to ensure that buffer contents go to disk.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Execution of Transactions 33/55\n",
      "\n",
      "content:\n",
      " States as the transaction executes:\n",
      " t   Action        v  Buf(A)  Buf(B)  Disk(A)  Disk(B)\n",
      " -----------------------------------------------------\n",
      " (0) BEGIN         .      .       .        8        5\n",
      " (1) READ(A,v)     8      8       .        8        5\n",
      " (2) v = v*2      16      8       .        8        5\n",
      " (3) WRITE(A,v)   16     16       .        8        5\n",
      " (4) READ(B,v)     5     16       5        8        5\n",
      " (5) v = v+1       6     16       5        8        5\n",
      " (6) WRITE(B,v)    6     16       6        8        5\n",
      " (7) OUTPUT(A)     6     16       6       16        5\n",
      " (8) OUTPUT(B)     6     16       6       16        6\n",
      " After tx completes, we must have either Disk(A)=8, Disk(B)=5   or   Disk(A)=16, Disk(B)=6\n",
      " If system crashes before (8), may need to undo disk changes.\n",
      " If system crashes after (8), may need to redo disk changes.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Transactions and Buffer Pool 34/55\n",
      "\n",
      "content:\n",
      " Two issues arise w.r.t. buffers:\n",
      " forcing ... OUTPUT buffer on each WRITE\n",
      " ensures durability; disk always consistent with buffer pool\n",
      " poor performance; defeats purpose of having buffer pool\n",
      " stealing ... replace buffers of uncommitted tx's\n",
      " if we don't, poor throughput (tx's blocked on buffers)\n",
      " if we do, seems to cause atomicity problems?\n",
      " Ideally, we want stealing and not forcing.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Transactions and Buffer Pool 35/55\n",
      "\n",
      "content:\n",
      " Handling stealing:\n",
      " transaction T loads page P and makes changes\n",
      " T2 needs a buffer, and P is the \"victim\"\n",
      " P is output to disk (it's dirty) and replaced\n",
      " if T aborts, some of its changes are already \"committed\"\n",
      " must log values changed in P at \"steal-time\"\n",
      " use these to UNDO changes in case of failure of T\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Transactions and Buffer Pool 36/55\n",
      "\n",
      "content:\n",
      " Handling no forcing:\n",
      " transaction T makes changes & commits, then system crashes\n",
      " but what if modified page P has not yet been output?\n",
      " must log changed values in P as soon as they change\n",
      " use these to support REDO to restore changes\n",
      " Above scenario may be a problem, even if we are forcing\n",
      " e.g. system crashes immediately after requesting a WRITE()\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Logging 37/55\n",
      "\n",
      "content:\n",
      " Three \"styles\" of logging\n",
      " undo ... removes changes by any uncommitted tx's\n",
      " redo ... repeats changes by any committed tx's\n",
      " undo/redo ... combines aspects of both\n",
      " All approaches require:\n",
      " a sequential file of log records\n",
      " each log record describes a change to a data item\n",
      " log records are written first\n",
      " actual changes to data are written later\n",
      " Known as write-ahead logging    (PostgreSQL uses WAL)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Undo Logging 38/55\n",
      "\n",
      "content:\n",
      " Simple form of logging which ensures atomicity.\n",
      " Log file consists of a sequence of small records:\n",
      " <START T> ... transaction T begins\n",
      " <COMMIT T> ... transaction T completes successfully\n",
      " <ABORT T> ... transaction T fails (no changes)\n",
      " <T,X,v> ... transaction T changed value of X from v\n",
      " Notes:\n",
      " we refer to <T,X,v> generically as <UPDATE> log records\n",
      " update log entry created for each WRITE (not OUTPUT)\n",
      " update log entry contains old value (new value is not recorded)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Undo Logging 39/55\n",
      "\n",
      "content:\n",
      " Data must be written to disk in the following order:\n",
      " 1. <START> transaction log record\n",
      " 2. <UPDATE> log records indicating changes\n",
      " 3. the changed data elements themselves\n",
      " 4. <COMMIT> log record\n",
      " Note: sufficient to have <T,X,v> output before X, for each X\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Undo Logging 40/55\n",
      "\n",
      "content:\n",
      " For the example transaction, we would get:\n",
      " t    Action        v  B(A)  B(B)  D(A)  D(B)  Log\n",
      " --------------------------------------------------------\n",
      " (0)  BEGIN         .    .     .     8     5   <START T>\n",
      " (1)  READ(A,v)     8    8     .     8     5\n",
      " (2)  v = v*2      16    8     .     8     5\n",
      " (3)  WRITE(A,v)   16   16     .     8     5   <T,A,8>\n",
      " (4)  READ(B,v)     5   16     5     8     5\n",
      " (5)  v = v+1       6   16     5     8     5\n",
      " (6)  WRITE(B,v)    6   16     6     8     5   <T,B,5>\n",
      " (7)  FlushLog\n",
      " (8)  StartCommit\n",
      " (9)  OUTPUT(A)     6   16     6    16     5\n",
      " (10) OUTPUT(B)     6   16     6    16     6\n",
      " (11) EndCommit                                <COMMIT T>\n",
      " (12) FlushLog\n",
      " Note that T is not regarded as committed until (12) completes.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Undo Logging 41/55\n",
      "\n",
      "content:\n",
      " Simplified view of recovery using UNDO logging:\n",
      " scan backwards through log\n",
      " if <COMMIT T>, mark T as committed\n",
      " if <T,X,v> and T not committed, set X to v on disk\n",
      " if <START T> and T not committed, put <ABORT T> in log\n",
      " Assumes we scan entire log; use checkpoints to limit scan.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Undo Logging 42/55\n",
      "\n",
      "content:\n",
      " Algorithmic view of recovery using UNDO logging:\n",
      " committedTrans = abortedTrans = startedTrans = {}\n",
      " for each log record from most recent to oldest {\n",
      "     switch (log record) {\n",
      "     <COMMIT T> : add T to committedTrans\n",
      "     <ABORT T>  : add T to abortedTrans\n",
      "     <START T>  : add T to startedTrans\n",
      "     <T,X,v>    : if (T in committedTrans)\n",
      "                      // don't undo committed changes\n",
      "                  else  // roll-back changes\n",
      "                      { WRITE(X,v); OUTPUT(X) }\n",
      " }   }\n",
      " for each T in startedTrans {\n",
      "     if (T in committedTrans) ignore\n",
      "     else if (T in abortedTrans) ignore\n",
      "     else write <ABORT T> to log\n",
      " }\n",
      " flush log\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Checkpointing 43/55\n",
      "\n",
      "content:\n",
      " Simple view of recovery implies reading entire log file.\n",
      " Since log file grows without bound, this is infeasible.\n",
      " Eventually we can delete \"old\" section of log.\n",
      " i.e. where all prior transactions have committed\n",
      " This point is called a checkpoint.\n",
      " all of log prior to checkpoint can be ignored for recovery\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Checkpointing 44/55\n",
      "\n",
      "content:\n",
      " Problem: many concurrent/overlapping transactions.\n",
      " How to know that all have finished?\n",
      " 1. periodically, write log record <CHKPT (T1,..,Tk)> (contains references to all active transactions ⇒ active tx table)\n",
      " 2. continue normal processing (e.g. new tx's can start)\n",
      " 3. when all of T1,..,Tk have completed, write log record <ENDCHKPT> and flush log\n",
      " Note: tx manager maintains chkpt and active tx information\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Checkpointing 45/55\n",
      "\n",
      "content:\n",
      " Recovery: scan backwards through log file processing as before.\n",
      " Determining where to stop depends on ...\n",
      " whether we meet <ENDCHKPT> or <CHKPT...> first\n",
      " If we encounter <ENDCHKPT> first:\n",
      " we know that all incomplete tx's come after prev <CHKPT...>\n",
      " thus, can stop backward scan when we reach <CHKPT...>\n",
      " If we encounter <CHKPT (T1,..,Tk)> first:\n",
      " crash occurred during the checkpoint period\n",
      " any of T1,..,Tk that committed before crash are ok\n",
      " for uncommitted tx's, need to continue backward scan\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Redo Logging 46/55\n",
      "\n",
      "content:\n",
      " Problem with UNDO logging:\n",
      " all changed data must be output to disk before committing\n",
      " conflicts with optimal use of the buffer pool\n",
      " Alternative approach is redo logging:\n",
      " allow changes to remain only in buffers after commit\n",
      " write records to indicate what changes are \"pending\"\n",
      " after a crash, can apply changes during recovery\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Redo Logging 47/55\n",
      "\n",
      "content:\n",
      " Requirement for redo logging: write-ahead rule.\n",
      " Data must be written to disk as follows:\n",
      " 1. start transaction log record\n",
      " 2. update log records indicating changes\n",
      " 3. then commit log record (OUTPUT)\n",
      " 4. then OUTPUT changed data elements themselves\n",
      " Note that update log records now contain <T,X,v'>, where v' is the new value for X.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Redo Logging 48/55\n",
      "\n",
      "content:\n",
      " For the example transaction, we would get:\n",
      " t    Action        v  B(A)  B(B)  D(A)  D(B)  Log\n",
      " --------------------------------------------------------\n",
      " (0)  BEGIN         .    .     .     8     5   <START T>\n",
      " (1)  READ(A,v)     8    8     .     8     5\n",
      " (2)  v = v*2      16    8     .     8     5\n",
      " (3)  WRITE(A,v)   16   16     .     8     5   <T,A,16>\n",
      " (4)  READ(B,v)     5   16     5     8     5\n",
      " (5)  v = v+1       6   16     5     8     5\n",
      " (6)  WRITE(B,v)    6   16     6     8     5   <T,B,6>\n",
      " (7)  COMMIT                                   <COMMIT T>\n",
      " (8)  FlushLog\n",
      " (9)  OUTPUT(A)     6   16     6    16     5\n",
      " (10) OUTPUT(B)     6   16     6    16     6\n",
      " Note that T is regarded as committed as soon as (8) completes.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Redo Logging 49/55\n",
      "\n",
      "content:\n",
      " Simplified view of recovery using REDO logging:\n",
      " identify all committed tx's   (backwards scan)\n",
      " scan forwards through log\n",
      " if <T,X,v> and T is committed, set X to v on disk\n",
      " if <START T> and T not committed, put <ABORT T> in log\n",
      " Assumes we scan entire log; use checkpoints to limit scan.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Undo/Redo Logging 50/55\n",
      "\n",
      "content:\n",
      " UNDO logging and REDO logging are incompatible in\n",
      " order of outputting <COMMIT T> and changed data\n",
      " how data in buffers is handled during checkpoints\n",
      " Undo/Redo logging combines aspects of both\n",
      " requires new kind of update log record <T,X,v,v'> gives both old and new values for X\n",
      " removes incompatibilities between output orders\n",
      " As for previous cases, requires write-ahead of log records.\n",
      " Undo/redo loging is common in practice; Aries algorithm.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Undo/Redo Logging 51/55\n",
      "\n",
      "content:\n",
      " For the example transaction, we might get:\n",
      " t    Action        v  B(A)  B(B)  D(A)  D(B)  Log\n",
      " --------------------------------------------------------\n",
      " (0)  BEGIN         .    .     .     8     5   <START T>\n",
      " (1)  READ(A,v)     8    8     .     8     5\n",
      " (2)  v = v*2      16    8     .     8     5\n",
      " (3)  WRITE(A,v)   16   16     .     8     5   <T,A,8,16>\n",
      " (4)  READ(B,v)     5   16     5     8     5\n",
      " (5)  v = v+1       6   16     5     8     5\n",
      " (6)  WRITE(B,v)    6   16     6     8     5   <T,B,5,6>\n",
      " (7)  FlushLog\n",
      " (8)  StartCommit\n",
      " (9)  OUTPUT(A)     6   16     6    16     5\n",
      " (10)                                          <COMMIT T>\n",
      " (11) OUTPUT(B)     6   16     6    16     6\n",
      " Note that T is regarded as committed as soon as (10) completes.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Undo/Redo Logging 52/55\n",
      "\n",
      "content:\n",
      " Simplified view of recovery using UNDO/REDO logging:\n",
      " scan log to determine committed/uncommitted txs\n",
      " for each uncommitted tx T add <ABORT T> to log\n",
      " scan backwards through log\n",
      " if <T,X,v,w> and T is not committed, set X to v on disk\n",
      " scan forwards through log\n",
      " if <T,X,v,w> and T is committed, set X to w on disk\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Undo/Redo Logging 53/55\n",
      "\n",
      "content:\n",
      " The above description simplifies details of undo/redo logging.\n",
      " Aries is a complete algorithm for undo/redo logging.\n",
      " Differences to what we have described:\n",
      " log records contain a sequence numnber (LSN) LSNs used in tx and buffer managers, and stored in data pages\n",
      " additional log record to mark <END> (of commit or abort)\n",
      " <CHKPT> contains only a timestamp\n",
      " <ENDCHKPT..> contains tx and dirty page info\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Recovery in PostgreSQL 54/55\n",
      "\n",
      "content:\n",
      " PostgreSQL uses write-ahead undo/redo style logging.\n",
      " It also uses multi-version concurrency control, which\n",
      " tags each record with a tx and update timestamp\n",
      " MVCC simplifies some aspects of undo/redo, e.g.\n",
      " some info required by logging is already held in each tuple\n",
      " no need to undo effects of aborted tx's; use old version\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Recovery in PostgreSQL 55/55\n",
      "\n",
      "content:\n",
      " Transaction/logging code is distributed throughout backend.\n",
      " Core transaction code is in src/backend/access/transam.\n",
      " Transaction/logging data is written to files in PGDATA/pg_xlog\n",
      " a number of very large files containing log records\n",
      " old files are removed once all txs noted there are completed\n",
      " new files added when existing files reach their capacity (16MB)\n",
      " number of tx log files varies depending on tx activity\n",
      " Produced: 20 Sep 2018\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: header\n",
      "\n",
      "content:\n",
      " Week 10 Lectures\n",
      " Week 10 Lectures\n",
      " Beyond RDBMSs\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Future of Database 2/52\n",
      "\n",
      "content:\n",
      " Core \"database\" goals:\n",
      " deal with very large amounts of data (terabytes, petabyes, ...)\n",
      " very-high-level languages (deal with big data in uniform ways)\n",
      " query execution   (if evaluation too slow ⇒ useless)\n",
      " At the moment (and for the last 20 years) RDBMSs dominate ...\n",
      " simple/clean data model, backed up by theory\n",
      " high-level language for accessing data\n",
      " 40 years development work on RDBMS engine technology\n",
      " RDBMSs work well in domains with uniform, structured data.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Future of Database 3/52\n",
      "\n",
      "content:\n",
      " Limitations/pitfalls of RDBMSs:\n",
      " NULL is ambiguous: unknown, not applicable, not supplied\n",
      " \"limited\" support for constraints/integrity and rules\n",
      " no support for uncertainty (data represents the state-of-the-world)\n",
      " data model too simple (e.g. no direct support for complex objects)\n",
      " query model too rigid (e.g. no approximate matching)\n",
      " continually changing data sources not well-handled\n",
      " data must be \"molded\" to fit a single rigid schema\n",
      " database systems must be manually \"tuned\"\n",
      " do not scale well to some data sets (e.g. Google, Telco's)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Future of Database 4/52\n",
      "\n",
      "content:\n",
      " How to overcome (some of) these limitations?\n",
      " Extend the relational model ...\n",
      " add new data types and query ops for new applications deal with uncertainty/inaccuracy/approximation in data\n",
      " Replace the relational model ...\n",
      " object-oriented DBMS ... OO programming with persistent objects\n",
      " XML DBMS ... all data stored as XML documents, new query model\n",
      " application-effective data model (e.g. (key,value) pairs)\n",
      " Performance ...\n",
      " new query algorithms/data-structures for new types of queries\n",
      " DBMSs that \"tune\" themselves\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Future of Database 5/52\n",
      "\n",
      "content:\n",
      " An overview of the possibilities:\n",
      " \"classical\" RDBMS   (e.g. PostgreSQL, Oracle, SQLite)\n",
      " parallel DBMS   (e.g. XPRS)\n",
      " distributed DBMS   (e.g. Cohera)\n",
      " deductive databases   (e.g. Datalog)\n",
      " temporal databases   (e.g. MariaDB)\n",
      " column stores   (e.g. C-Store?)\n",
      " object-oriented DBMS   (e.g. ObjectStore)\n",
      " key-value stores   (e.g. Redis, DynamoDB)\n",
      " wide column stores   (e.g. Cassandra, Scylla, HBase)\n",
      " graph databases   (e.g. Neo4J, Datastax)\n",
      " document stores   (e.g. MongoDB, Couchbase)\n",
      " search engines   (e.g. Google, Solr)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Big Data 7/52\n",
      "\n",
      "content:\n",
      " Some modern applications have massive data sets (e.g. Google)\n",
      " far too large to store on a single machine/RDBMS\n",
      " query demands far too high even if could store in DBMS\n",
      " Approach to dealing with such data\n",
      " distribute data over large collection of nodes  (also, redundancy)\n",
      " provide computational mechanisms for distributing computation\n",
      " Often this data does not need full relational selection\n",
      " represent data via (key,value) pairs\n",
      " unique keys can be used for addressing data\n",
      " values can be large objects (e.g. web pages, images, ...)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Big Data 8/52\n",
      "\n",
      "content:\n",
      " Popular computational approach to Big Data: map/reduce\n",
      " suitable for widely-distributed, very-large data\n",
      " allows parallel computation on such data to be easily specified\n",
      " distribute (map) parts of computation across network\n",
      " compute in parallel (possibly with further mapping)\n",
      " merge (reduce) multiple results for delivery to requestor\n",
      " Some Big Data proponents see no future need for SQL/relational ...\n",
      " depends on application (e.g. hard integrity vs eventual consistency)\n",
      " Humour: Parody of noSQL fans   (strong language warning)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Information Retrieval 9/52\n",
      "\n",
      "content:\n",
      " DBMSs generally do precise matching (although like/regexps)\n",
      " Information retrieval systems do approximate matching.\n",
      " E.g. documents containing these words (Google, etc.)\n",
      " Also introduces notion of \"quality\" of matching (e.g. tuple T1 is a better match than tuple T2)\n",
      " Quality also implies ranking of results.\n",
      " Much activity in incorporating IR ideas into DBMS context.\n",
      " Goal: support database exploration better.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Multimedia Data 10/52\n",
      "\n",
      "content:\n",
      " Data which does not fit the \"tabular model\":\n",
      " image, video, music, text, ... (and combinations of these)\n",
      " Research problems:\n",
      " how to specify queries on such data? (image1 ≅ image2)\n",
      " how to \"display\" results? (synchronize components)\n",
      " Solutions to the first problem typically:\n",
      " extend notions of \"matching\"/indexes for querying\n",
      " require sophisticated methods for capturing data features\n",
      " Sample query: find other songs like this one?\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Uncertainty 11/52\n",
      "\n",
      "content:\n",
      " Multimedia/IR introduces approximate matching.\n",
      " In some contexts, we have approximate/uncertain data.\n",
      " E.g. witness statements in a crime-fighting database\n",
      " http://www.youtube.com/watch?v=b2F-DItXtZs\n",
      " \"I think the getaway car was red ... or maybe orange ...\"\n",
      " \"I am 75% sure that John carried out the crime\"\n",
      " Work by Jennifer Widom at Stanford on the Trio system\n",
      " extends the relational model (ULDB)\n",
      " extends the query language (TriQL)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Stream Management Systems 12/52\n",
      "\n",
      "content:\n",
      " Makes one addition to the relational model\n",
      " stream = infinite sequence of tuples, arriving one-at-a-time\n",
      " Applications: news feeds, telecomms, monitoring web usage, ...\n",
      " RDBMSs: run a variety of queries on (relatively) fixed data StreamDBs: run fixed queries on changing data (stream)\n",
      " Approaches:\n",
      " window = relation formed from a stream via a rule\n",
      " stream data type = build new stream-specific operations\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Graph Data 13/52\n",
      "\n",
      "content:\n",
      " Uses graphs rather than tables as basic data structure tool.\n",
      " Applications: complex data representation, via \"flexible\" objects, e.g. XML\n",
      " Graph nature of data changes query model considerably.\n",
      " (e.g. Xquery language, high-level like SQL, but different operators, etc.)\n",
      " Implementing graphs in RDBMSs is often inefficient.\n",
      " Research problem: query processing for XML data.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Dispersed Databases 14/52\n",
      "\n",
      "content:\n",
      " Characteristics of dispersed databases:\n",
      " very large numbers of small processing nodes\n",
      " data is distributed/shared among nodes\n",
      " Applications: environmental monitoring devices, \"intelligent dust\", ...\n",
      " Research issues:\n",
      " query/search strategies (how to organise query processing)\n",
      " distribution of data (trade-off between centralised and diffused)\n",
      " Less extreme versions of this already exist:\n",
      " grid and cloud computing\n",
      " database management for mobile devices\n",
      " Parallelism in Databases\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Parallel DBMSs 16/52\n",
      "\n",
      "content:\n",
      " The discussion so far has revolved around systems\n",
      " with a single or small number of processors\n",
      " accessing a single memory space\n",
      " getting data from one or more disk devices\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Parallel DBMSs 17/52\n",
      "\n",
      "content:\n",
      " Why parallelism? ... Throughput!\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Parallel DBMSs 18/52\n",
      "\n",
      "content:\n",
      " DBMSs lend are a success story in application of parallelism\n",
      " can process many data elements (tuples) at the same time\n",
      " can create pipelines of query evaluation steps\n",
      " don't require special hardware\n",
      " can hide paralleism within the query evaluator\n",
      " application programmers don't need to change habits\n",
      " Compare this with effort to do parallel programming.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Parallel Architectures 19/52\n",
      "\n",
      "content:\n",
      " Types:   shared memory,   shared disk,   shared nothing\n",
      " Example shared-nothing architecture:\n",
      " Typically same room/LAN   (data transfer cost ~ 100's of μsecs .. msecs)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Distributed Architectures 20/52\n",
      "\n",
      "content:\n",
      " Distributed architectures are ...\n",
      " effectively shared-nothing, on a global-scale network\n",
      " Typically on the Internet   (data transfer cost ~ secs)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Parallel Databases (PDBs) 21/52\n",
      "\n",
      "content:\n",
      " Parallel databases provide various forms of parallelism ...\n",
      " process parallelism can speed up query evaluation\n",
      " processor parallelism can assist in speeding up memory ops\n",
      " processor parallelism introduces cache coherence issues\n",
      " disk parallelism can assist in overcoming latency\n",
      " disk parallelism can be used to improve fault-tolerance (RAID)\n",
      " one limiting factor is congestion on communication bus\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Parallel Databases (PDBs) 22/52\n",
      "\n",
      "content:\n",
      " Types of parallelism\n",
      " pipeline parallelism\n",
      " multi-step process,   each processor handles one step\n",
      " run in parallel and pipeline result from one to another\n",
      " partition parallelism\n",
      " many processors running in parallel\n",
      " each performs same task on a subset of the data\n",
      " results from processors need to be merged\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Data Storage in PDBs 23/52\n",
      "\n",
      "content:\n",
      " Consider each table as a collection of pages ...\n",
      " Page addressing on single processor/disk: (Table, File, Page)\n",
      " Table maps to a set of files (e.g. named by tableID)\n",
      " File distinguishes primary/overflow files\n",
      " PageNum maps to an offset in a specific file\n",
      " If multiple nodes, then addressing depends how data distributed\n",
      " partitioned: (Node, Table, File, Page)\n",
      " replicated: ({Nodes}, Table, File, Page)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Data Storage in PDBs 24/52\n",
      "\n",
      "content:\n",
      " Assume that each table/relation consists of pages in a file\n",
      " Can distribute data across multiple storage devices\n",
      " duplicate all pages from a relation  (replication)\n",
      " store some pages on one store, some on others  (partitioning)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Data Storage in PDBs 26/52\n",
      "\n",
      "content:\n",
      " Assume that partitioning is based on one attribute\n",
      " Data-partitioning strategies for one table on n nodes:\n",
      " round-robin,   hash-based,   range-based\n",
      " Round-robin partitioning\n",
      " cycle through nodes, new tuple added on \"next\" node\n",
      " e.g. i th tuple is placed on (i mod n)th node\n",
      " balances load on nodes;   no help for querying\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Data Storage in PDBs 27/52\n",
      "\n",
      "content:\n",
      " Hash partitioning\n",
      " use hash value to determine which node and page\n",
      " e.g. i = hash(tuple) so tuple is placed on i th node\n",
      " helpful for equality-based queries ?   not really\n",
      " Range partitioning\n",
      " ranges of attr values are assigned to processors\n",
      " e.g. values 1-10 on node0,  11-20 on node1, ...,  99-100 noden-1\n",
      " potentially helpful for range-based queries\n",
      " In both cases, data skew may lead to unbalanced load\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Parallelism in DB Operations 28/52\n",
      "\n",
      "content:\n",
      " Different types of parallelism in DBMS operations\n",
      " intra-operator parallelism\n",
      " get all machines working to compute a given operation\n",
      " (scan, sort, join)\n",
      " inter-operator parallelism\n",
      " each operator runs concurrently on a different processor (exploits pipelining)\n",
      " Inter-query parallelism\n",
      " different queries run on different processors\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Parallelism in DB Operations 29/52\n",
      "\n",
      "content:\n",
      " Parallel scanning\n",
      " scan partitions in parallel and merge results\n",
      " selection may allow us to ignore some partitions (e.g. for range and hash partitioning)\n",
      " can build indexes on each partition\n",
      " Effectiveness depends on query type vs partitioning type\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Parallelism in DB Operations 30/52\n",
      "\n",
      "content:\n",
      " Parallel sorting\n",
      " scan in parallel, range-partition during scan\n",
      " pipeline into local sort on each processor\n",
      " \"merge\" sorted partitions in order\n",
      " Potential problem:\n",
      " data skew because of unfortunate choice of partition points\n",
      " resolve by initial data sampling to determine partitions\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Parallelism in DB Operations 32/52\n",
      "\n",
      "content:\n",
      " Parallel nested loop join\n",
      " each outer tuple needs to examine each inner tuple\n",
      " but only if it could potentially join\n",
      " range/hash partitioning reduce partitions to consider\n",
      " Parallel sort-merge join\n",
      " as noted above, parallel sort gives range partitioning\n",
      " merging partitioned tables has no parallelism (but is fast)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Parallelism in DB Operations 35/52\n",
      "\n",
      "content:\n",
      " Parallel hash join\n",
      " distribute partitions to different processors\n",
      " partition 0 of R goes to same site as partition 0 of S\n",
      " join phase can be done in parallel on each processor\n",
      " then results need to be merged\n",
      " very effective for equijoin\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Parallelism in DB Operations 36/52\n",
      "\n",
      "content:\n",
      " Parallel hash join:\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: PostgreSQL and Parallelism 37/52\n",
      "\n",
      "content:\n",
      " PostgreSQL assumes\n",
      " shared memory space accessable to all back-ends\n",
      " files for one table are located on one disk\n",
      " PostgreSQL allows\n",
      " data to be distributed across multiple disk devices\n",
      " So could run on ...\n",
      " shared-memory, shared-disk architectures\n",
      " hierarchical architectures with distributed virtual memory\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... PostgreSQL and Parallelism 38/52\n",
      "\n",
      "content:\n",
      " PostgreSQL can provide\n",
      " multiple servers running on separate nodes\n",
      " application #1: high availability\n",
      " \"standby\" server takes over if primary server fails\n",
      " application #2: load balancing\n",
      " several servers can be used to provide same data\n",
      " direct queries to least loaded server\n",
      " Both need data synchronisation between servers\n",
      " PostgreSQL uses notion of master and slave servers.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... PostgreSQL and Parallelism 39/52\n",
      "\n",
      "content:\n",
      " High availability ...\n",
      " updates occur on master, recorded in tx log\n",
      " tx logs shipped/streamed from master to slave(s)\n",
      " slave uses tx logs to maintain current state\n",
      " configuration controls frequency of log shipping\n",
      " bringing slave up-to-date is \"fast\" (~1-2secs)\n",
      " Note: small window for data loss (committed tx log records not sent)\n",
      " Load balancing ...\n",
      " not provided built-in to PostgreSQL, 3rd-party tools exist\n",
      " Distributed Databases\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Distributed Databases 41/52\n",
      "\n",
      "content:\n",
      " A distributed database (DDB) is\n",
      " collection of multiple logically-related databases\n",
      " distributed over a network (generally a WAN)\n",
      " A distributed database management system (DDBMS) is\n",
      " software that manages a distributed database\n",
      " providing access that hides complexity of distribution\n",
      " A DDBMS may involve\n",
      " instances of a single DBMS  (e.g. ≥1 PostgreSQL servers)\n",
      " a layer over multiple different DBMSs  (e.g. Oracle+PostgreSQL+DB2)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Distributed Databases 44/52\n",
      "\n",
      "content:\n",
      " Two kinds of distributed databases\n",
      " parallel database on a distributed architecture\n",
      " single schema, homogeneous DBMSs\n",
      " independent databases on a distributed architecture\n",
      " independent schemas, heterogeneous DBMSs\n",
      " The latter are also called federated databases\n",
      " Ultimately, the distributed database (DDB) provides\n",
      " global schema, with mappings from constituent schemas\n",
      " giving the impression of a single database\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Distributed Databases 45/52\n",
      "\n",
      "content:\n",
      " Advantages of distributed databases\n",
      " allow information from multiple DBs to be merged\n",
      " provide for replication of some data  (resilience)\n",
      " allow for possible parallel query evaluation\n",
      " Disadavtanges of distributed databases\n",
      " cost of mapping between different schemas  (federated)\n",
      " communication costs  (write-to-network vs write-to-disk)\n",
      " maintaining ACID properties in distributed transactions\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Distributed Databases 46/52\n",
      "\n",
      "content:\n",
      " Application examples:\n",
      " bank with multiple branches\n",
      " local branch-related data (e.g. accounts) stored in branch\n",
      " corporate data (e.g. HR) stored on central server(s)\n",
      " central register of credit-worthiness for customers\n",
      " chain of department stores\n",
      " store-related data (e.g. sales, inventory) stored in store\n",
      " corporate data (e.g. HR, customers) stored on central server(s)\n",
      " sales data sent to data warehouse for analysis\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Distributed Databases 47/52\n",
      "\n",
      "content:\n",
      " In both examples\n",
      " some data is conceptually a single table at corporate level\n",
      " but does not physically exist as a table in one location\n",
      " E.g. account(acct_id, branch, customer, balance)\n",
      " each branch maintains its own data (for its accounts)\n",
      " set of tuples, all with same branch\n",
      " bank also needs a view of all accounts\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Data Distribution 48/52\n",
      "\n",
      "content:\n",
      " Partitioning/distributing data\n",
      " where to place (parts of) tables\n",
      " determined by usage of data  (locality, used together)\n",
      " affects communication cost ⇒ query evaluation cost\n",
      " how to partition data within tables\n",
      " no partitioning ... whole table stored on ≥1 DBMS\n",
      " horizontal partitioning ... subsets of rows\n",
      " vertical partitioning ... subsets of columns\n",
      " Problem: maintaining consistency\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Data Distribution 49/52\n",
      "\n",
      "content:\n",
      " Consider table R decomposed into R1, R2, ..., Rn\n",
      " Fragmentation can be done in multiple ways, but need to ensure ...\n",
      " Completeness\n",
      " decompostion is complete iff each t∈R is in some Ri\n",
      " Reconstruction\n",
      " original R can be produced by some relational operation\n",
      " Disjoint\n",
      " if item t ∈ Ri, then t ∉ Rk, k ≠ i   (assuming no replication)\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Query Processing 50/52\n",
      "\n",
      "content:\n",
      " Query processing typically involves shipping data\n",
      " e.g. reconstructing table from distributed partitions\n",
      " e.g. join on tables stored on separate sites\n",
      " Aim: minimise shipping cost   (since it is a networking cost)\n",
      " Shipping cost becomes the \"disk access cost\" of DQOpt\n",
      " Larger space of possibilities than standard QOpt, but same principle\n",
      " consider possible execution plans, choose cheapest\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: ... Query Processing 51/52\n",
      "\n",
      "content:\n",
      " Distributed query processing\n",
      " may require query ops to be executed on different nodes\n",
      " node provides only source of some data\n",
      " some nodes may have limited set of operations\n",
      " needs to merge data received from different nodes\n",
      " may require data transformation (to fit schemas together)\n",
      " Query optimisation in such contexts is complex.\n",
      "\n",
      "********************\n",
      "********************\n",
      "topic: Transaction Processing 52/52\n",
      "\n",
      "content:\n",
      " Distribution of data complicates tx processing ...\n",
      " potential for multiple copies of data to become inconsistent\n",
      " commit or abort must occur consistently on all nodes\n",
      " Distributed tx processing handled by two-phase commit\n",
      " initiating site has transaction coordinator Ci ...\n",
      " waits for all other sites executing tx T to \"complete\"\n",
      " sends <prepare T> message to all other sites\n",
      " waits for <ready T> response from all other sites\n",
      " if not received (timeout), or <abort T> received, flag abort\n",
      " if all other sites respond <ready T>, flag commit\n",
      " write <commit T> or <abort T> to log\n",
      " send <commit T> or <abort T> to all other sites\n",
      " non-initiating sites write log entries before responding\n",
      " Produced: 4 Oct 2018\n",
      "\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def pprint(data):\n",
    "    for ele in data:\n",
    "        print('*'*20)\n",
    "        print('topic:' ,ele['topic'])\n",
    "        \n",
    "        print('content:\\n' ,ele['content'])\n",
    "        print('*'*20)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for i in range(9,11):\n",
    "        try:\n",
    "            if i < 10:\n",
    "                week = 'week0'\n",
    "            else:\n",
    "                week = 'week'\n",
    "            file_adr = './data_preprocessed/'+ week + str(i)+'.json'\n",
    "            with open(file_adr,'r') as f:\n",
    "                data = json.loads(f.read())\n",
    "            pprint(data)\n",
    "        except:\n",
    "            print('week',i,'was not processed...')\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!open ./data_preprocessed/week10.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_preprocessed/week09.json\n",
      "./data_preprocessed/week10.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for i in range(9,11):\n",
    "        if i < 10:\n",
    "            week = 'week0'\n",
    "        else:\n",
    "            week = 'week'\n",
    "        file_adr = './data_preprocessed/'+ week + str(i)+'.json'\n",
    "        print(file_adr)\n",
    "        with open(file_adr,'r') as f:\n",
    "            data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
